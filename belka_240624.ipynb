{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. binds=1과 동일한 숫자로 랜덤추출\n",
    "다음은 8:2로 추출해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./queried_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def query_and_save_parquet(train_path, output_path):\n",
    "    # DuckDB 연결\n",
    "    con = duckdb.connect()\n",
    "\n",
    "    # 데이터 쿼리 실행\n",
    "    df = con.execute(f\"\"\"\n",
    "    (SELECT * FROM parquet_scan('{train_path}') WHERE binds = 1 ORDER BY RANDOM() LIMIT 1589906)\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    # 데이터 쿼리 실행\n",
    "    # df = con.execute(f\"\"\"\n",
    "    # (SELECT * FROM parquet_scan('{train_path}') WHERE binds = 0 ORDER BY RANDOM() LIMIT 1589906)\n",
    "    # \"\"\").fetchdf()\n",
    "\n",
    "#                         \n",
    "    \n",
    "    # DataFrame을 Parquet 파일로 저장\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, output_path)\n",
    "    print(f\"Data saved to {output_path}\")\n",
    "    \n",
    "    # 메모리 해제\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "# 파일 경로 및 설정\n",
    "train_path = './train.parquet'\n",
    "output_parquet_path = './queried_data.parquet'\n",
    "\n",
    "# 데이터 쿼리 및 저장 실행\n",
    "query_and_save_parquet(train_path, output_parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. bind=0을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./0_queried_data_chunk_1.parquet, fetched: 100000, remaining: 1589906\n",
      "Data saved to ./0_queried_data_chunk_2.parquet, fetched: 100000, remaining: 1489906\n",
      "Data saved to ./0_queried_data_chunk_3.parquet, fetched: 100000, remaining: 1389906\n",
      "Data saved to ./0_queried_data_chunk_4.parquet, fetched: 100000, remaining: 1289906\n",
      "Data saved to ./0_queried_data_chunk_5.parquet, fetched: 100000, remaining: 1189906\n",
      "Data saved to ./0_queried_data_chunk_6.parquet, fetched: 100000, remaining: 1089906\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Query interrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 데이터 쿼리 및 청크 저장 실행\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mquery_and_save_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 청크 파일 병합 실행\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# total_chunks = (total_records + chunk_size - 1) // chunk_size\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# merge_chunks(output_prefix, final_output_path, total_chunks)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# print(\"Data querying and merging completed.\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mquery_and_save_chunks\u001b[1;34m(train_path, output_prefix, total_records, chunk_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m fetch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(chunk_size, remaining_records)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 데이터 쿼리\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;43mSELECT * FROM parquet_scan(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtrain_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) WHERE binds = 0 ORDER BY RANDOM() LIMIT \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfetch_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m OFFSET \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moffset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;43m\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchdf()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Query interrupted"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def query_and_save_chunks(train_path, output_prefix, total_records=1589906, chunk_size=100000):\n",
    "    # DuckDB 연결\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    remaining_records = total_records\n",
    "    offset = 0\n",
    "    chunk_id = 1\n",
    "    \n",
    "    while remaining_records > 0:\n",
    "        fetch_size = min(chunk_size, remaining_records)\n",
    "        \n",
    "        # 데이터 쿼리\n",
    "        df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{train_path}') WHERE binds = 0 ORDER BY RANDOM() LIMIT {fetch_size} OFFSET {offset}\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        if df.empty:\n",
    "            break\n",
    "        \n",
    "        # 청크 파일로 저장\n",
    "        chunk_path = f\"{output_prefix}_chunk_{chunk_id}.parquet\"\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, chunk_path)\n",
    "        print(f\"Data saved to {chunk_path}, fetched: {fetch_size}, remaining: {remaining_records}\")\n",
    "        \n",
    "        # 메모리 해제\n",
    "        del df, table\n",
    "        gc.collect()\n",
    "        \n",
    "        offset += fetch_size\n",
    "        remaining_records -= fetch_size\n",
    "        chunk_id += 1\n",
    "\n",
    "# 파일 경로 및 설정\n",
    "train_path = './train.parquet'\n",
    "output_prefix = './0_queried_data'\n",
    "final_output_path = './final_queried_data.parquet'\n",
    "total_records = 1589906\n",
    "chunk_size = 100000\n",
    "\n",
    "# 데이터 쿼리 및 청크 저장 실행\n",
    "query_and_save_chunks(train_path, output_prefix, total_records, chunk_size)\n",
    "\n",
    "# 청크 파일 병합 실행\n",
    "# total_chunks = (total_records + chunk_size - 1) // chunk_size\n",
    "# merge_chunks(output_prefix, final_output_path, total_chunks)\n",
    "\n",
    "# print(\"Data querying and merging completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binds=0을 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed and merged.\n",
      "Chunk 1 processed and merged.\n",
      "Chunk 2 processed and merged.\n",
      "Chunk 3 processed and merged.\n",
      "Chunk 4 processed and merged.\n",
      "Chunk 5 processed and merged.\n",
      "Chunk 6 processed and merged.\n",
      "Chunk 7 processed and merged.\n",
      "Chunk 8 processed and merged.\n",
      "Chunk 9 processed and merged.\n",
      "Chunk 10 processed and merged.\n",
      "Chunk 11 processed and merged.\n",
      "Chunk 12 processed and merged.\n",
      "Chunk 13 processed and merged.\n",
      "Chunk 14 processed and merged.\n",
      "Chunk 15 processed and merged.\n",
      "Final data saved to ./final_queried_data2.parquet\n",
      "Data querying and merging completed.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def merge_chunks(output_prefix, final_output_path, total_chunks):\n",
    "    schema = None\n",
    "    unique_data = pd.DataFrame()\n",
    "    \n",
    "    for chunk_id in range(total_chunks):\n",
    "        chunk_path = f\"{output_prefix}_chunk_{chunk_id}.parquet\"\n",
    "        table = pq.read_table(chunk_path)\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # 중복 제거 및 데이터 합치기\n",
    "        unique_data = pd.concat([unique_data, df]).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        # 메모리 해제 및 청크 파일 삭제\n",
    "        del table, df\n",
    "        os.remove(chunk_path)\n",
    "        gc.collect()\n",
    "        print(f\"Chunk {chunk_id} processed and merged.\")\n",
    "    \n",
    "    # 최종 파일로 저장\n",
    "    final_table = pa.Table.from_pandas(unique_data)\n",
    "    pq.write_table(final_table, final_output_path)\n",
    "    print(f\"Final data saved to {final_output_path}\")\n",
    "    \n",
    "    # 메모리 해제\n",
    "    del unique_data, final_table\n",
    "    gc.collect()\n",
    "\n",
    "# 파일 경로 및 설정\n",
    "train_path = './train.parquet'\n",
    "output_prefix = './0_queried_data'\n",
    "final_output_path = './final_queried_data3.parquet'\n",
    "total_records = 1600000#1589906\n",
    "chunk_size = 100000\n",
    "\n",
    "# 청크 파일 병합 실행\n",
    "total_chunks = (total_records + chunk_size - 1) // chunk_size\n",
    "merge_chunks(output_prefix, final_output_path, total_chunks)\n",
    "\n",
    "print(\"Data querying and merging completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binds=0과 binds=1을 합치고 순서를 다 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from ./final_queried_data2.parquet and ./merged_shuffled_data.parquet merged and shuffled, saved to ./merged_shuffled_data2.parquet\n",
      "Data merging and shuffling completed.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "def merge_and_shuffle_parquets(input_path1, input_path2, output_path):\n",
    "    # 첫 번째 Parquet 파일 읽기\n",
    "    table1 = pq.read_table(input_path1)\n",
    "    df1 = table1.to_pandas()\n",
    "    \n",
    "    # 두 번째 Parquet 파일 읽기\n",
    "    table2 = pq.read_table(input_path2)\n",
    "    df2 = table2.to_pandas()\n",
    "    \n",
    "    # 데이터 프레임 병합\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "    # 데이터 프레임 셔플\n",
    "    shuffled_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # 결과를 Parquet 파일로 저장\n",
    "    shuffled_table = pa.Table.from_pandas(shuffled_df)\n",
    "    pq.write_table(shuffled_table, output_path)\n",
    "    \n",
    "    print(f\"Data from {input_path1} and {input_path2} merged and shuffled, saved to {output_path}\")\n",
    "    \n",
    "    # 메모리 해제\n",
    "    del df1, df2, combined_df, shuffled_df, table1, table2, shuffled_table\n",
    "    gc.collect()\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_path1 = './final_queried_data2.parquet'\n",
    "input_path2 = './merged_shuffled_data.parquet'#'./queried_data.parquet'\n",
    "output_path = './merged_shuffled_data2.parquet'\n",
    "\n",
    "# 데이터 병합 및 셔플 실행\n",
    "merge_and_shuffle_parquets(input_path1, input_path2, output_path)\n",
    "\n",
    "print(\"Data merging and shuffling completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 섞였는지 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc0</th>\n",
       "      <th>enc1</th>\n",
       "      <th>enc2</th>\n",
       "      <th>enc3</th>\n",
       "      <th>enc4</th>\n",
       "      <th>enc5</th>\n",
       "      <th>enc6</th>\n",
       "      <th>enc7</th>\n",
       "      <th>enc8</th>\n",
       "      <th>enc9</th>\n",
       "      <th>...</th>\n",
       "      <th>enc133</th>\n",
       "      <th>enc134</th>\n",
       "      <th>enc135</th>\n",
       "      <th>enc136</th>\n",
       "      <th>enc137</th>\n",
       "      <th>enc138</th>\n",
       "      <th>enc139</th>\n",
       "      <th>enc140</th>\n",
       "      <th>enc141</th>\n",
       "      <th>bind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    enc0  enc1  enc2  enc3  enc4  enc5  enc6  enc7  enc8  enc9  ...  enc133  \\\n",
       "0      8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "1      8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "2      8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "3      8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "4      8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "..   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...     ...   \n",
       "95     8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "96     8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "97     8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "98     8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "99     8    22     8     8    28    12    27    12    12    12  ...       0   \n",
       "\n",
       "    enc134  enc135  enc136  enc137  enc138  enc139  enc140  enc141  bind  \n",
       "0        0       0       0       0       0       0       0       0     0  \n",
       "1        0       0       0       0       0       0       0       0     0  \n",
       "2        0       0       0       0       0       0       0       0     0  \n",
       "3        0       0       0       0       0       0       0       0     0  \n",
       "4        0       0       0       0       0       0       0       0     0  \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...   ...  \n",
       "95       0       0       0       0       0       0       0       0     0  \n",
       "96       0       0       0       0       0       0       0       0     0  \n",
       "97       0       0       0       0       0       0       0       0     0  \n",
       "98       0       0       0       0       0       0       0       0     0  \n",
       "99       0       0       0       0       0       0       0       0     0  \n",
       "\n",
       "[100 rows x 143 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "\n",
    "# df = con.query(f\"\"\"\n",
    "# SELECT * FROM parquet_scan('./merged_shuffled_data.parquet') LIMIT 1000       \n",
    "# \"\"\").df()\n",
    "\n",
    "df = con.query(f\"\"\"\n",
    "SELECT * FROM parquet_scan('./train_enc_BRD4.parquet') LIMIT 100       \n",
    "\"\"\").df()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇대 몇인지 비율 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binds_1_count</th>\n",
       "      <th>binds_0_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>ratio_binds_1</th>\n",
       "      <th>ratio_binds_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1589906.0</td>\n",
       "      <td>3191877.0</td>\n",
       "      <td>4781783</td>\n",
       "      <td>0.332492</td>\n",
       "      <td>0.667508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   binds_1_count  binds_0_count  total_count  ratio_binds_1  ratio_binds_0\n",
       "0      1589906.0      3191877.0      4781783       0.332492       0.667508"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 전체 데이터 중에서 binds=1 및 binds=0의 갯수를 계산\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN binds = 1 THEN 1 ELSE 0 END) AS binds_1_count,\n",
    "    SUM(CASE WHEN binds = 0 THEN 1 ELSE 0 END) AS binds_0_count,\n",
    "    COUNT(*) AS total_count\n",
    "FROM parquet_scan('./merged_shuffled_data.parquet')\n",
    "\"\"\"\n",
    "\n",
    "df = con.execute(query).df()\n",
    "\n",
    "# 전체 행의 수를 사용하여 비율 계산\n",
    "df['ratio_binds_1'] = df['binds_1_count'] / df['total_count']\n",
    "df['ratio_binds_0'] = df['binds_0_count'] / df['total_count']\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ecfp 적용 (id와 protein_name 검토할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 146batch [2:11:32, 54.06s/batch]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECFP generation and saving completed.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ECFP 생성 함수\n",
    "def generate_ecfp(smiles, radius=2, bits=1024):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    if molecule is None:\n",
    "        return [0] * bits\n",
    "    return list(AllChem.GetMorganFingerprintAsBitVect(molecule, radius, nBits=bits))\n",
    "\n",
    "def process_batch(df):\n",
    "    # 각 SMILES 열에 대해 ECFP 생성\n",
    "    df['molecule_ecfp'] = df['molecule_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock1_ecfp'] = df['buildingblock1_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock2_ecfp'] = df['buildingblock2_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock3_ecfp'] = df['buildingblock3_smiles'].apply(generate_ecfp)\n",
    "    \n",
    "    # 필요한 열만 포함된 DataFrame 반환\n",
    "    return df[['id', 'protein_name', 'molecule_ecfp', 'buildingblock1_ecfp', 'buildingblock2_ecfp', 'buildingblock3_ecfp', 'binds']]\n",
    "\n",
    "def preprocess_and_save_ecfp(input_path, output_path, batch_size=32768):\n",
    "    reader = pq.ParquetFile(input_path)\n",
    "    \n",
    "    # 적절한 스키마로 Parquet writer 초기화\n",
    "    schema = pa.schema([\n",
    "        ('id', pa.int32()),\n",
    "        ('molecule_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock1_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock2_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock3_ecfp', pa.list_(pa.int32())),\n",
    "        ('protein_name', pa.string()),\n",
    "        ('binds', pa.int32())  # test할 때는 제외\n",
    "    ])\n",
    "    \n",
    "    with pq.ParquetWriter(output_path, schema) as writer:\n",
    "        total_batches = reader.metadata.num_row_groups\n",
    "        \n",
    "        with tqdm(total=total_batches, desc=\"Processing\", unit=\"batch\", leave=True) as pbar:\n",
    "            for batch in reader.iter_batches(batch_size=batch_size):\n",
    "                df_batch = batch.to_pandas()\n",
    "                \n",
    "                processed_batch = process_batch(df_batch)\n",
    "                \n",
    "                # 처리된 DataFrame을 Arrow Table로 변환하여 파일에 작성\n",
    "                table = pa.Table.from_pandas(processed_batch, schema=schema)\n",
    "                writer.write_table(table)\n",
    "                \n",
    "                # 진행 상황 업데이트\n",
    "                pbar.update(1)\n",
    "\n",
    "                # 주기적으로 가비지 컬렉션 호출\n",
    "                gc.collect()\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_parquet_path = './merged_shuffled_data.parquet'\n",
    "output_parquet_path = './processed_merged_queried_data.parquet'\n",
    "# 테스트 파일 경로\n",
    "# input_parquet_path = './test.parquet'\n",
    "# output_parquet_path = './test_processed_data.parquet'\n",
    "\n",
    "# 데이터 전처리 및 저장 실행\n",
    "preprocess_and_save_ecfp(input_parquet_path, output_parquet_path)\n",
    "\n",
    "print(\"ECFP generation and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "1521460\n",
      "Processing BRD4 batch starting at offset 0...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Initialized new LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 0...\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 4s 25ms/step - loss: 0.2905 - recall: 0.6957\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 2s 24ms/step - loss: 0.1754 - recall: 0.8800\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 2s 24ms/step - loss: 0.1559 - recall: 0.8936\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.1358 - recall: 0.9064\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.1118 - recall: 0.9223\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.83430\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.89162\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.90364\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.89759\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.903639\n",
      "Average Precision by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.891622\n",
      "Average f1-score by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu   0.89759\n",
      "Processing BRD4 batch starting at offset 100000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 100000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1316 - recall_1: 0.9361\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0954 - recall_1: 0.9699\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0708 - recall_1: 0.9474\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0400 - recall_1: 0.9737\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0221 - recall_1: 0.9887\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.85209\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.87671\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.95522\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.91429\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.929431\n",
      "Average Precision by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.884167\n",
      "Average f1-score by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.905938\n",
      "Processing BRD4 batch starting at offset 200000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 200000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3139 - recall_2: 0.8970\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1553 - recall_2: 0.8841\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1092 - recall_2: 0.9142\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0949 - recall_2: 0.9485\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0871 - recall_2: 0.9399\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.84133\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.91379\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.88333\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.89831\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.914065\n",
      "Average Precision by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.894043\n",
      "Average f1-score by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.903394\n",
      "Processing BRD4 batch starting at offset 300000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 300000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1788 - recall_3: 0.8622\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1213 - recall_3: 0.8933\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0843 - recall_3: 0.9511\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0531 - recall_3: 0.9822\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0299 - recall_3: 0.9822\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.81009\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.92982\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.80303\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.86179\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.886307\n",
      "Average Precision by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.902988\n",
      "Average f1-score by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.892992\n",
      "Processing BRD4 batch starting at offset 400000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 400000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2205 - recall_4: 0.8835\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1439 - recall_4: 0.9759\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0843 - recall_4: 0.9759\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0649 - recall_4: 0.9639\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0530 - recall_4: 0.9839\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.80108\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.91525\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.80597\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.85714\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.870239\n",
      "Average Precision by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.905441\n",
      "Average f1-score by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.885823\n",
      "Processing BRD4 batch starting at offset 500000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 500000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2271 - recall_5: 0.8171\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1731 - recall_5: 0.9228\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1345 - recall_5: 0.9512\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1010 - recall_5: 0.9553\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0793 - recall_5: 0.9675\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.71707\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.87805\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.75000\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.80899\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.850199\n",
      "Average Precision by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.900876\n",
      "Average f1-score by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.873017\n",
      "Processing BRD4 batch starting at offset 600000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 600000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1817 - recall_6: 0.8327\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1629 - recall_6: 0.9510\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1156 - recall_6: 0.9510\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0874 - recall_6: 0.9306\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0726 - recall_6: 0.9306\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.76824\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.87500\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.81667\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.84483\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.845409\n",
      "Average Precision by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.897179\n",
      "Average f1-score by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu   0.86899\n",
      "Processing BRD4 batch starting at offset 700000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 700000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2567 - recall_7: 0.7738\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1757 - recall_7: 0.8869\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1508 - recall_7: 0.9231\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1256 - recall_7: 0.9321\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1018 - recall_7: 0.9412\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.88427\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.94828\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.90164\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.92437\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.852438\n",
      "Average Precision by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.903566\n",
      "Average f1-score by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.875912\n",
      "Processing BRD4 batch starting at offset 800000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 800000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2130 - recall_8: 0.8474\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1697 - recall_8: 0.9036\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1337 - recall_8: 0.9116\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1062 - recall_8: 0.9036\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0774 - recall_8: 0.9237\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.73579\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.83636\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.82143\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.82883\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.848992\n",
      "Average Precision by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.896099\n",
      "Average f1-score by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.870681\n",
      "Processing BRD4 batch starting at offset 900000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 900000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2244 - recall_9: 0.8458\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1651 - recall_9: 0.9130\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1290 - recall_9: 0.9486\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1006 - recall_9: 0.9565\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0863 - recall_9: 0.9526\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.82529\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.90476\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.86364\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.88372\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.850457\n",
      "Average Precision by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.896966\n",
      "Average f1-score by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.871985\n",
      "Processing BRD4 batch starting at offset 1000000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 1000000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1960 - recall_10: 0.8327\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1723 - recall_10: 0.9402\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1281 - recall_10: 0.9482\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0988 - recall_10: 0.9442\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0794 - recall_10: 0.9203\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.91730\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.96429\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.93103\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.94737\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857782\n",
      "Average Precision by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.903086\n",
      "Average f1-score by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.878838\n",
      "Processing BRD4 batch starting at offset 1100000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 1100000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1682 - recall_11: 0.8582\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1344 - recall_11: 0.9349\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1043 - recall_11: 0.9464\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0717 - recall_11: 0.9579\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0563 - recall_11: 0.9502\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.83927\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.90000\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.90000\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.90000\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  Recall\n",
      "0    BRD4          elu         relu  0.8613\n",
      "Average Precision by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.902829\n",
      "Average f1-score by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880601\n",
      "Processing BRD4 batch starting at offset 1200000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 1200000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1617 - recall_12: 0.8704\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1296 - recall_12: 0.9433\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1007 - recall_12: 0.9433\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0754 - recall_12: 0.9474\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0653 - recall_12: 0.9433\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.77115\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.87097\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.81818\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.84375\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857983\n",
      "Average Precision by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.900378\n",
      "Average f1-score by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.877767\n",
      "Processing BRD4 batch starting at offset 1300000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 1300000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1867 - recall_13: 0.8208\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1405 - recall_13: 0.9083\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1113 - recall_13: 0.9333\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0848 - recall_13: 0.9333\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0650 - recall_13: 0.9375\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.85569\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.93333\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.87500\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.90323\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.859199\n",
      "Average Precision by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.902732\n",
      "Average f1-score by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.879585\n",
      "Processing BRD4 batch starting at offset 1400000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 1400000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2167 - recall_14: 0.8526\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1565 - recall_14: 0.8606\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1217 - recall_14: 0.8884\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0961 - recall_14: 0.9283\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0764 - recall_14: 0.9482\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.87081\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.96226\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.86441\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.91071\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.859546\n",
      "Average Precision by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906701\n",
      "Average f1-score by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu   0.88166\n",
      "Processing BRD4 batch starting at offset 1500000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for BRD4 with elu and relu\n",
      "Iteration 1 for BRD4 with LSTM batch 1500000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1222 - recall_15: 0.8978\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0971 - recall_15: 0.8889\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0648 - recall_15: 0.9689\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0473 - recall_15: 0.9822\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0330 - recall_15: 0.9867\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "BRD4 - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.80288\n",
      "BRD4 - LSTM - Iteration 1 - Precision: 0.89706\n",
      "BRD4 - LSTM - Iteration 1 - Recall: 0.82432\n",
      "BRD4 - LSTM - Iteration 1 - F1 Score: 0.85915\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1500000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "Average Precision by Activation Function Combinations (Batch 1500000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "Average f1-score by Activation Function Combinations (Batch 1500000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1474768\n",
      "Processing HSA batch starting at offset 0...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Initialized new LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 0...\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 5s 32ms/step - loss: 0.3776 - recall_16: 0.4801\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.2542 - recall_16: 0.7463\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.2309 - recall_16: 0.7663\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.2059 - recall_16: 0.7904\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 2s 31ms/step - loss: 0.1783 - recall_16: 0.8196\n",
      "625/625 [==============================] - 2s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.69735\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.81339\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.78419\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.79852\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.784186\n",
      "Average Precision by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.813394\n",
      "Average f1-score by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.798523\n",
      "Processing HSA batch starting at offset 100000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 100000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3382 - recall_17: 0.7324\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2346 - recall_17: 0.6714\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1930 - recall_17: 0.7465\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1495 - recall_17: 0.8028\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1106 - recall_17: 0.8967\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.62235\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.76364\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.70000\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.73043\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.742093\n",
      "Average Precision by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.788515\n",
      "Average f1-score by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.764479\n",
      "Processing HSA batch starting at offset 200000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 200000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2847 - recall_18: 0.7432\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2133 - recall_18: 0.7838\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1688 - recall_18: 0.8108\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1268 - recall_18: 0.8874\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0934 - recall_18: 0.9054\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.73058\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.84615\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.80000\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.82243\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.761395\n",
      "Average Precision by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.807728\n",
      "Average f1-score by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.783796\n",
      "Processing HSA batch starting at offset 300000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 300000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3493 - recall_19: 0.7051\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2425 - recall_19: 0.8157\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1993 - recall_19: 0.8479\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1614 - recall_19: 0.8479\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1367 - recall_19: 0.8387\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.75334\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.90196\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.75410\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.82143\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.759571\n",
      "Average Precision by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.831286\n",
      "Average f1-score by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.793204\n",
      "Processing HSA batch starting at offset 400000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 400000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2952 - recall_20: 0.6275\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2333 - recall_20: 0.8431\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1827 - recall_20: 0.8824\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1421 - recall_20: 0.8627\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1078 - recall_20: 0.8824\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.64627\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.77778\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.73684\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.75676\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.755025\n",
      "Average Precision by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.820584\n",
      "Average f1-score by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.785915\n",
      "Processing HSA batch starting at offset 500000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 500000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3077 - recall_21: 0.6910\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2102 - recall_21: 0.8670\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1767 - recall_21: 0.8884\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1380 - recall_21: 0.8884\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1101 - recall_21: 0.8970\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.61293\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.82051\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.64000\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.71910\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.735854\n",
      "Average Precision by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.820573\n",
      "Average f1-score by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.774779\n",
      "Processing HSA batch starting at offset 600000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 600000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3099 - recall_22: 0.7273\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2341 - recall_22: 0.8554\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1975 - recall_22: 0.8678\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1583 - recall_22: 0.8554\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1292 - recall_22: 0.8595\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.73732\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.89583\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.74138\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.81132\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.736644\n",
      "Average Precision by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.831324\n",
      "Average f1-score by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.779999\n",
      "Processing HSA batch starting at offset 700000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 700000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2783 - recall_23: 0.7123\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2282 - recall_23: 0.8265\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1798 - recall_23: 0.8174\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1501 - recall_23: 0.7900\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1167 - recall_23: 0.8356\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.65765\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.83333\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.67797\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.74766\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.729309\n",
      "Average Precision by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.831575\n",
      "Average f1-score by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.775957\n",
      "Processing HSA batch starting at offset 800000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 800000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3413 - recall_24: 0.6916\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2565 - recall_24: 0.7710\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2153 - recall_24: 0.7850\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1878 - recall_24: 0.7897\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1563 - recall_24: 0.8271\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.69818\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.89362\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.66667\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.76364\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722349\n",
      "Average Precision by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.838469\n",
      "Average f1-score by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.774588\n",
      "Processing HSA batch starting at offset 900000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 900000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2859 - recall_25: 0.6980\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2500 - recall_25: 0.8275\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2089 - recall_25: 0.8588\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1687 - recall_25: 0.8627\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1350 - recall_25: 0.8627\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.59772\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.74510\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.69091\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.71698\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.719205\n",
      "Average Precision by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.829132\n",
      "Average f1-score by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.768828\n",
      "Processing HSA batch starting at offset 1000000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 1000000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2978 - recall_26: 0.7467\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2188 - recall_26: 0.7860\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1929 - recall_26: 0.8122\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1609 - recall_26: 0.8210\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1292 - recall_26: 0.8559\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.77697\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.92500\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.78723\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.85057\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.725389\n",
      "Average Precision by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.837847\n",
      "Average f1-score by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.776259\n",
      "Processing HSA batch starting at offset 1100000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 1100000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2853 - recall_27: 0.6844\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2380 - recall_27: 0.8133\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1951 - recall_27: 0.8267\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1660 - recall_27: 0.8400\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1266 - recall_27: 0.8844\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.70113\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.90000\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.69231\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.78261\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722632\n",
      "Average Precision by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.843026\n",
      "Average f1-score by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.776788\n",
      "Processing HSA batch starting at offset 1200000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 1200000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2986 - recall_28: 0.7277\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2169 - recall_28: 0.7700\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1839 - recall_28: 0.7934\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1629 - recall_28: 0.7981\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1322 - recall_28: 0.8404\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.69034\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.90476\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.65517\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.76000\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.717443\n",
      "Average Precision by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.847775\n",
      "Average f1-score by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.775497\n",
      "Processing HSA batch starting at offset 1300000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 1300000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2324 - recall_29: 0.7358\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1880 - recall_29: 0.8443\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1523 - recall_29: 0.8443\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1175 - recall_29: 0.8821\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0928 - recall_29: 0.9009\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.74096\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.82609\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.82609\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.82609\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.725203\n",
      "Average Precision by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846226\n",
      "Average f1-score by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.779110\n",
      "Processing HSA batch starting at offset 1400000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for HSA with relu and elu\n",
      "Iteration 1 for HSA with LSTM batch 1400000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.4314 - recall_30: 0.6810\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3002 - recall_30: 0.7672\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2378 - recall_30: 0.7974\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2182 - recall_30: 0.8060\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2047 - recall_30: 0.8060\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "HSA - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.66514\n",
      "HSA - LSTM - Iteration 1 - Precision: 0.85714\n",
      "HSA - LSTM - Iteration 1 - Recall: 0.67925\n",
      "HSA - LSTM - Iteration 1 - F1 Score: 0.75789\n",
      "Memory cleared.\n",
      "Model saved: HSA_lstm_model_relu_elu_relu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "Average Precision by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "Average f1-score by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "1785555\n",
      "Processing sEH batch starting at offset 0...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Initialized new LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 0...\n",
      "Epoch 1/5\n",
      "79/79 [==============================] - 5s 32ms/step - loss: 0.2161 - recall_31: 0.8450\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.1089 - recall_31: 0.9579\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.0905 - recall_31: 0.9688\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.0720 - recall_31: 0.9760\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 3s 32ms/step - loss: 0.0571 - recall_31: 0.9815\n",
      "625/625 [==============================] - 2s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.94221\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.95849\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.97050\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.96446\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.970501\n",
      "Average Precision by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.958485\n",
      "Average f1-score by Activation Function Combinations (Batch 0):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.964456\n",
      "Processing sEH batch starting at offset 100000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 100000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0872 - recall_32: 0.9631\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0540 - recall_32: 0.9877\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0388 - recall_32: 0.9908\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0227 - recall_32: 0.9938\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0113 - recall_32: 1.0000\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.85637\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.88000\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.95652\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.91667\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.963512\n",
      "Average Precision by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.919243\n",
      "Average f1-score by Activation Function Combinations (Batch 100000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.940561\n",
      "Processing sEH batch starting at offset 200000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 200000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1665 - recall_33: 0.9741\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0763 - recall_33: 0.9770\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0534 - recall_33: 0.9856\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0432 - recall_33: 0.9943\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0319 - recall_33: 1.0000\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.94711\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.96203\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.97436\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.96815\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.967127\n",
      "Average Precision by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.933503\n",
      "Average f1-score by Activation Function Combinations (Batch 200000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.949758\n",
      "Processing sEH batch starting at offset 300000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 300000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1195 - recall_34: 0.9444\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0984 - recall_34: 0.9854\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0586 - recall_34: 0.9795\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0486 - recall_34: 0.9766\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0373 - recall_34: 0.9854\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.96218\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.98810\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.95402\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.97076\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.963851\n",
      "Average Precision by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.947151\n",
      "Average f1-score by Activation Function Combinations (Batch 300000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.955009\n",
      "Processing sEH batch starting at offset 400000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 400000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1112 - recall_35: 0.9544\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0766 - recall_35: 0.9757\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0533 - recall_35: 0.9878\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0359 - recall_35: 0.9939\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0217 - recall_35: 0.9970\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.92291\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.93750\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.97403\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.95541\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.965886\n",
      "Average Precision by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.945221\n",
      "Average f1-score by Activation Function Combinations (Batch 400000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.955090\n",
      "Processing sEH batch starting at offset 500000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 500000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1256 - recall_36: 0.9525\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0792 - recall_36: 0.9881\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0537 - recall_36: 0.9911\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0468 - recall_36: 0.9852\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0335 - recall_36: 0.9941\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.86561\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.92105\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.89744\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.90909\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.954478\n",
      "Average Precision by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.941193\n",
      "Average f1-score by Activation Function Combinations (Batch 500000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.947423\n",
      "Processing sEH batch starting at offset 600000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 600000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1217 - recall_37: 0.9358\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0954 - recall_37: 0.9908\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0653 - recall_37: 0.9908\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0499 - recall_37: 0.9908\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0377 - recall_37: 0.9939\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.92932\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.96512\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.93258\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94857\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.951350\n",
      "Average Precision by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.944611\n",
      "Average f1-score by Activation Function Combinations (Batch 600000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.947587\n",
      "Processing sEH batch starting at offset 700000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 700000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1227 - recall_38: 0.9450\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0943 - recall_38: 0.9786\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0693 - recall_38: 0.9939\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0564 - recall_38: 0.9908\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0407 - recall_38: 0.9969\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.86242\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.90667\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.91892\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.91275\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.947296\n",
      "Average Precision by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.939868\n",
      "Average f1-score by Activation Function Combinations (Batch 700000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.943233\n",
      "Processing sEH batch starting at offset 800000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 800000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1102 - recall_39: 0.9633\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0903 - recall_39: 0.9847\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0667 - recall_39: 0.9847\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0507 - recall_39: 0.9786\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0408 - recall_39: 0.9878\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.92043\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.94382\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.95455\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94915\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.948102\n",
      "Average Precision by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.940307\n",
      "Average f1-score by Activation Function Combinations (Batch 800000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.943891\n",
      "Processing sEH batch starting at offset 900000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 900000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1291 - recall_40: 0.9323\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1048 - recall_40: 0.9815\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0764 - recall_40: 0.9908\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0582 - recall_40: 0.9846\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0461 - recall_40: 0.9846\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.93538\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.95402\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.96512\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.95954\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.949803\n",
      "Average Precision by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.941678\n",
      "Average f1-score by Activation Function Combinations (Batch 900000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.945455\n",
      "Processing sEH batch starting at offset 1000000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1000000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1329 - recall_41: 0.9335\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1115 - recall_41: 0.9827\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0813 - recall_41: 0.9855\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0607 - recall_41: 0.9827\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0466 - recall_41: 0.9913\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.92608\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.97260\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.92208\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94667\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.947283\n",
      "Average Precision by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.944490\n",
      "Average f1-score by Activation Function Combinations (Batch 1000000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.945565\n",
      "Processing sEH batch starting at offset 1100000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1100000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1563 - recall_42: 0.9542\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1179 - recall_42: 0.9828\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0890 - recall_42: 0.9885\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0735 - recall_42: 0.9857\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0637 - recall_42: 0.9828\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.93225\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.98507\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.91667\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94964\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.944731\n",
      "Average Precision by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.947872\n",
      "Average f1-score by Activation Function Combinations (Batch 1100000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.945905\n",
      "Processing sEH batch starting at offset 1200000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1200000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0991 - recall_43: 0.9335\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0692 - recall_43: 0.9789\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0446 - recall_43: 0.9879\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0253 - recall_43: 0.9970\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0201 - recall_43: 0.9940\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.94193\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.96296\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.96296\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.96296\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.946134\n",
      "Average Precision by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.949033\n",
      "Average f1-score by Activation Function Combinations (Batch 1200000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.947217\n",
      "Processing sEH batch starting at offset 1300000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1300000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1831 - recall_44: 0.9335\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1206 - recall_44: 0.9740\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0815 - recall_44: 0.9827\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0633 - recall_44: 0.9884\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0559 - recall_44: 0.9913\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.91705\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.94186\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.95294\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94737\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.946620\n",
      "Average Precision by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.948520\n",
      "Average f1-score by Activation Function Combinations (Batch 1300000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.947228\n",
      "Processing sEH batch starting at offset 1400000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1400000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1254 - recall_45: 0.9682\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0979 - recall_45: 0.9740\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0716 - recall_45: 0.9913\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0531 - recall_45: 0.9913\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0342 - recall_45: 0.9884\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.91275\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.94253\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.94253\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94253\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.946347\n",
      "Average Precision by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.948121\n",
      "Average f1-score by Activation Function Combinations (Batch 1400000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.946915\n",
      "Processing sEH batch starting at offset 1500000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1500000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1125 - recall_46: 0.9251\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0770 - recall_46: 0.9837\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0522 - recall_46: 0.9935\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0328 - recall_46: 0.9870\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0259 - recall_46: 0.9902\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.92369\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.97368\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.91358\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.94268\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1500000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.944299\n",
      "Average Precision by Activation Function Combinations (Batch 1500000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.949719\n",
      "Average f1-score by Activation Function Combinations (Batch 1500000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.946650\n",
      "Processing sEH batch starting at offset 1600000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1600000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1326 - recall_47: 0.9247\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1028 - recall_47: 0.9729\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0816 - recall_47: 0.9819\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0623 - recall_47: 0.9880\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0506 - recall_47: 0.9910\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.95213\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.98592\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.94595\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.96552\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1600000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.944396\n",
      "Average Precision by Activation Function Combinations (Batch 1600000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.951848\n",
      "Average f1-score by Activation Function Combinations (Batch 1600000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.947760\n",
      "Processing sEH batch starting at offset 1700000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with LSTM...\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded existing LSTM model for sEH with elu and relu\n",
      "Iteration 1 for sEH with LSTM batch 1700000...\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1164 - recall_48: 0.9560\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0853 - recall_48: 0.9619\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0626 - recall_48: 0.9736\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0383 - recall_48: 0.9912\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0243 - recall_48: 0.9971\n",
      "7/7 [==============================] - 0s 3ms/step\n",
      "sEH - LSTM - Iteration 1 - Mean Average Precision (mAP): 0.90179\n",
      "sEH - LSTM - Iteration 1 - Precision: 0.97143\n",
      "sEH - LSTM - Iteration 1 - Recall: 0.88312\n",
      "sEH - LSTM - Iteration 1 - F1 Score: 0.92517\n",
      "Memory cleared.\n",
      "Model saved: sEH_lstm_model_elu_relu_elu_iteration_1.weights.h5\n",
      "Average Recall by Activation Function Combinations (Batch 1700000):\n",
      "  Protein Activation 1 Activation 2    Recall\n",
      "0    BRD4          elu         relu  0.857345\n",
      "1     HSA         relu          elu  0.722140\n",
      "2     sEH          elu         relu  0.940992\n",
      "Average Precision by Activation Function Combinations (Batch 1700000):\n",
      "  Protein Activation 1 Activation 2  Precision\n",
      "0    BRD4          elu         relu   0.906098\n",
      "1     HSA         relu          elu   0.846954\n",
      "2     sEH          elu         relu   0.952936\n",
      "Average f1-score by Activation Function Combinations (Batch 1700000):\n",
      "  Protein Activation 1 Activation 2  f1-score\n",
      "0    BRD4          elu         relu  0.880254\n",
      "1     HSA         relu          elu  0.777696\n",
      "2     sEH          elu         relu  0.946505\n"
     ]
    }
   ],
   "source": [
    "# import duckdb\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import gc\n",
    "# import joblib\n",
    "# import cupy as cp\n",
    "# from tqdm import tqdm\n",
    "# import psutil\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.losses import BinaryCrossentropy\n",
    "# from tensorflow.keras.regularizers import L1, L2, L1L2\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Check if TensorFlow is using the GPU\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# if physical_devices:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#         print(\"TensorFlow GPU memory growth enabled\")\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     print(\"TensorFlow GPU not available\")\n",
    "\n",
    "# # 데이터 로드 및 전처리\n",
    "# input_parquet_path = 'processed_merged_queried_data.parquet'\n",
    "\n",
    "# con = duckdb.connect()\n",
    "\n",
    "# # GPU 설정을 위한 XGBClassifier 옵션\n",
    "# xgb_params = {\n",
    "#     'n_estimators': 100,\n",
    "#     'device': 'cuda',\n",
    "#     'eta': 0.1,\n",
    "#     'max_depth': 14,\n",
    "#     'updater': 'grow_gpu_hist',\n",
    "#     'refresh_leaf': 1,\n",
    "#     'process_type': 'default',\n",
    "#     'use_label_encoder': False,\n",
    "#     'objective': 'binary:logistic', #'rank:map', \n",
    "#     'eval_metric': 'error'#'auc' #'map'\n",
    "# }\n",
    "\n",
    "# # RandomForestClassifier 옵션\n",
    "# rf_params = {\n",
    "#     'n_estimators': 100,\n",
    "#     'n_jobs': -1\n",
    "# }\n",
    "\n",
    "# # 메모리 정리 함수\n",
    "# def clear_memory():\n",
    "#     gc.collect()\n",
    "#     print(\"Memory cleared.\")\n",
    "\n",
    "# # 모델 학습 및 평가 함수 정의\n",
    "# def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name, iteration):\n",
    "#     X_train_gpu = cp.array(X_train)\n",
    "#     X_test_gpu = cp.array(X_test)\n",
    "#     model.fit(X_train_gpu, y_train)\n",
    "#     y_pred_proba = model.predict_proba(X_test_gpu)[:, 1]\n",
    "#     y_pred = model.predict(X_test_gpu)\n",
    "#     y_pred_proba_np = cp.asnumpy(y_pred_proba)\n",
    "#     y_pred_np = cp.asnumpy(y_pred)\n",
    "\n",
    "#     map_score = average_precision_score(y_test, y_pred_proba_np)\n",
    "#     accuracy = accuracy_score(y_test, y_pred_np)\n",
    "#     precision = precision_score(y_test, y_pred_np)\n",
    "#     recall = recall_score(y_test, y_pred_np)\n",
    "#     f1 = f1_score(y_test, y_pred_np)\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Precision: {precision:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Recall: {recall:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "#     # Delete GPU arrays to free up memory\n",
    "#     del X_train_gpu, X_test_gpu, y_pred_proba, y_pred\n",
    "#     cp._default_memory_pool.free_all_blocks()\n",
    "#     clear_memory()\n",
    "#     return model\n",
    "\n",
    "# def train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, model_name, iteration):\n",
    "#     # CuPy 배열을 NumPy 배열로 변환\n",
    "#     X_train_np = cp.asnumpy(X_train)\n",
    "#     X_test_np = cp.asnumpy(X_test)\n",
    "\n",
    "#     model.fit(X_train_np, y_train)\n",
    "#     y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "#     y_pred = model.predict(X_test_np)\n",
    "#     y_pred_proba_np = cp.asnumpy(y_pred_proba)\n",
    "#     y_pred_np = cp.asnumpy(y_pred)\n",
    "\n",
    "#     map_score = average_precision_score(y_test, y_pred_proba_np)\n",
    "#     accuracy = accuracy_score(y_test, y_pred_np)\n",
    "#     precision = precision_score(y_test, y_pred_np)\n",
    "#     recall = recall_score(y_test, y_pred_np)\n",
    "#     f1 = f1_score(y_test, y_pred_np)\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Precision: {precision:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Recall: {recall:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "#     # Delete GPU arrays to free up memory\n",
    "#     del X_train_np, X_test_np, y_pred_proba, y_pred\n",
    "#     cp._default_memory_pool.free_all_blocks()\n",
    "#     clear_memory()\n",
    "#     return model\n",
    "\n",
    "# # LSTM 모델 학습 및 평가 함수 정의 (TensorFlow)\n",
    "# def train_and_evaluate_lstm_tf(model, X_train, X_test, y_train, y_test, model_name, iteration, batch_size, num_epochs):\n",
    "#     # TensorFlow 텐서로 변환\n",
    "#     X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "#     X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "#     y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "#     y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "#     # 모델 컴파일\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(),\n",
    "#                   metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "#     # 학습\n",
    "#     model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "#     # 예측\n",
    "#     y_pred_proba = model.predict(X_test_tensor)\n",
    "#     y_pred = (y_pred_proba >= 0.5).astype(int)  # 정밀도 수정함!!!!!!!!!!!! 기본이 0.5\n",
    "\n",
    "#     map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "#     accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "#     precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "#     recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "#     f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Precision: {precision:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Recall: {recall:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "#     clear_memory()\n",
    "#     return model, precision, recall, f1, map_score\n",
    "\n",
    "# protein_names = ['BRD4', 'HSA', 'sEH']\n",
    "# # protein_names = ['HSA']  # 'BRD4', 'HSA', 'sEH'\n",
    "# # 각 protein에 대한 최적의 조합을 설정\n",
    "# best_combinations = {\n",
    "#     'BRD4': [('elu', 'relu')],  # 확정\n",
    "#     'HSA': [('relu', 'elu')],\n",
    "# #    'HSA': [('elu', 'relu')],\n",
    "#     # 'HSA': [('elu','tanh')]\n",
    "#     'sEH': [('elu', 'relu')]  # 확정\n",
    "# }\n",
    "\n",
    "# # 사용할 모델 종류 지정: 'lstm'\n",
    "# model_types = ['lstm']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "# #model_types = ['randomforest']  # 여기서 원하는 모델 종류를 리스트로 선택하세요 # randomforest\n",
    "# #model_types = ['xgboost']  # 여기서 원하는 모델 종류를 리스트로 선택하세요 # randomforest\n",
    "\n",
    "# num_iterations = 1\n",
    "\n",
    "# # Precision 기록을 위한 리스트\n",
    "# precision_records = []\n",
    "# total_records = []\n",
    "\n",
    "# # RAM usage monitoring\n",
    "# def monitor_memory(threshold=0.80):\n",
    "#     memory_info = psutil.virtual_memory()\n",
    "#     return memory_info.percent / 100 >= threshold\n",
    "\n",
    "# for protein in protein_names:\n",
    "#     # 전체 행 수 계산\n",
    "#     total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "#     print(total_rows)\n",
    "#     # 배치 크기 설정\n",
    "#     batch_size = 100000  # 적절한 배치 크기로 설정\n",
    "\n",
    "#     # 데이터 배치로 읽어와서 처리\n",
    "#     for offset in range(0, total_rows, batch_size):\n",
    "#         filtered_df = con.execute(f\"\"\"\n",
    "#         SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "#         WHERE protein_name = '{protein}'\n",
    "#         LIMIT {batch_size} OFFSET {offset}\n",
    "#         \"\"\").df()\n",
    "\n",
    "#         print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "\n",
    "#         X = np.concatenate([\n",
    "#             np.array(filtered_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "#             np.array(filtered_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "#             np.array(filtered_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "#             np.array(filtered_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "#         ], axis=1)\n",
    "#         y = filtered_df['binds'].tolist()\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#         print(f\"Processing {protein} train test split done...\")\n",
    "\n",
    "#         for model_type in model_types:\n",
    "#             print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "#             if model_type == 'lstm':\n",
    "#                 for activation_1, activation_2 in best_combinations[protein]:\n",
    "#                     input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "#                     hidden_dim = 128\n",
    "#                     output_dim = 1\n",
    "#                     num_layers = 2\n",
    "#                     num_epochs = 5\n",
    "#                     batch_size = 1024  # 2048\n",
    "\n",
    "#                     model = Sequential([\n",
    "# #                        Input(shape=(input_dim)),\n",
    "#                         Input(shape=(None, input_dim)),  # 여기를 수정했습니다.\n",
    "#                         LSTM(hidden_dim*8, return_sequences=True, activation=activation_1, dropout=0.05),\n",
    "#                         LSTM(hidden_dim*4, return_sequences=True, activation=activation_2, dropout=0.05),\n",
    "#                         LSTM(hidden_dim*1, return_sequences=False, activation=activation_1, dropout=0.05),\n",
    "#                         Dense(output_dim, activation='sigmoid')\n",
    "#                     ])\n",
    "\n",
    "#             # if model_type == 'lstm':\n",
    "#             #     for activation_1, activation_2 in best_combinations[protein]:\n",
    "#             #         input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "#             #         hidden_dim = 128\n",
    "#             #         output_dim = 1\n",
    "#             #         num_layers = 2\n",
    "#             #         num_epochs = 1\n",
    "#             #         batch_size = 1024  # 2048\n",
    "\n",
    "#             #         model = Sequential([\n",
    "#             #             Input(shape=(1, input_dim)),\n",
    "#             #             LSTM(hidden_dim, return_sequences=True, activation=activation_1, dropout=0.1),\n",
    "#             #             LSTM(hidden_dim, return_sequences=False, activation=activation_2, dropout=0.1),\n",
    "#             #             Dense(output_dim, activation='sigmoid')\n",
    "#             #         ])\n",
    "#                     model_filename = f\"{protein}_lstm_model_{activation_1}_{activation_2}_{activation_1}_iteration_1.weights.h5\"\n",
    "\n",
    "#                     # LSTM 모델 로드 또는 초기화\n",
    "#                     try:\n",
    "#                         model.load_weights(model_filename)\n",
    "#                         print(f\"Loaded existing LSTM model for {protein} with {activation_1} and {activation_2}\")\n",
    "#                     except FileNotFoundError:\n",
    "#                         print(f\"Initialized new LSTM model for {protein} with {activation_1} and {activation_2}\")\n",
    "\n",
    "#                     for i in range(num_iterations):\n",
    "#                         print(f\"Iteration {i + 1} for {protein} with LSTM batch {offset}...\")\n",
    "\n",
    "#                         X_train_lstm = np.expand_dims(X_train, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "#                         X_test_lstm = np.expand_dims(X_test, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "#                         model, precision, recall, f1, mAP = train_and_evaluate_lstm_tf(model, X_train_lstm, X_test_lstm, y_train, y_test, f\"{protein} - LSTM\", i + 1, batch_size, num_epochs)\n",
    "\n",
    "#                         # Precision 기록\n",
    "#                         precision_records.append((protein, activation_1, activation_2, precision, recall, f1, mAP, offset))\n",
    "\n",
    "#                         if monitor_memory():\n",
    "#                             print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "#                             clear_memory()\n",
    "\n",
    "#                         # 모델 저장\n",
    "#                         model.save_weights(model_filename)\n",
    "#                         print(f\"Model saved: {model_filename}\")\n",
    "#                 total_records.extend(precision_records)\n",
    "#                 precision_records = []\n",
    "\n",
    "#             else:\n",
    "#                 if model_type == 'xgboost':\n",
    "#                     model = XGBClassifier(**xgb_params)  # 초기 XGB 모델 생성\n",
    "#                     model_filename = f\"{protein}_xgb_model_iteration_1.pkl\"\n",
    "#                 elif model_type == 'randomforest':\n",
    "#                     model = RandomForestClassifier(**rf_params)  # 초기 RF 모델 생성\n",
    "#                     model_filename = f\"{protein}_rf_model_iteration_1.pkl\"\n",
    "\n",
    "#                 # 모델 로드 또는 초기화\n",
    "#                 try:\n",
    "#                     model = joblib.load(model_filename)\n",
    "#                     print(f\"Loaded existing {model_type.upper()} model for {protein}\")\n",
    "#                 except FileNotFoundError:\n",
    "#                     print(f\"Initialized new {model_type.upper()} model for {protein}\")\n",
    "\n",
    "#                 for i in range(num_iterations):\n",
    "#                     print(f\"Iteration {i + 1} for {protein} with {model_type.upper()} batch {offset}...\")\n",
    "\n",
    "#                     if model_type == 'xgboost':\n",
    "#                         model = train_and_evaluate_model(model, X_train, X_test, y_train, y_test, f\"{protein} - {model_type.upper()}\", i + 1)\n",
    "#                     elif model_type == 'randomforest':\n",
    "#                         model = train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, f\"{protein} - {model_type.upper()}\", i + 1)\n",
    "\n",
    "#                     if monitor_memory():\n",
    "#                         print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "#                         clear_memory()\n",
    "\n",
    "#                     # 모델 저장\n",
    "#                     joblib.dump(model, model_filename)\n",
    "#                     print(f\"Model saved: {model_filename}\")\n",
    "\n",
    "#             # 각 best_combinations마다 average precision과 average recall 계산 및 출력 (LSTM 모델에 대해서만)\n",
    "#             if model_type == 'lstm':\n",
    "#                 precision_df = pd.DataFrame(total_records, columns=['Protein', 'Activation 1', 'Activation 2', 'Precision', 'Recall', 'f1-score', 'mAP', 'batch_num'])\n",
    "#                 avg_precision = precision_df.groupby(['Protein', 'Activation 1', 'Activation 2'])['Precision'].mean().reset_index()\n",
    "#                 avg_recall = precision_df.groupby(['Protein', 'Activation 1', 'Activation 2'])['Recall'].mean().reset_index()\n",
    "#                 avg_f1 = precision_df.groupby(['Protein', 'Activation 1', 'Activation 2'])['f1-score'].mean().reset_index()\n",
    "\n",
    "#                 print(f\"Average Recall by Activation Function Combinations (Batch {offset}):\")\n",
    "#                 print(avg_recall)\n",
    "#                 print(f\"Average Precision by Activation Function Combinations (Batch {offset}):\")\n",
    "#                 print(avg_precision)\n",
    "#                 print(f\"Average f1-score by Activation Function Combinations (Batch {offset}):\")\n",
    "#                 print(avg_f1)\n",
    "# con.close()\n",
    "\n",
    "# # 모든 기록을 데이터프레임으로 변환하여 저장\n",
    "# results_df = pd.DataFrame(total_records, columns=['Protein', 'Activation 1', 'Activation 2', 'Precision', 'Recall', 'f1-score', 'mAP', 'batch_num'])\n",
    "# results_df.to_csv('total_precision_records.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catboost 추가한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "1521460\n",
      "Processing BRD4 batch starting at offset 0...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Initialized new CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 0...\n",
      "0:\tlearn: 0.6219602\ttotal: 38.5ms\tremaining: 38.5s\n",
      "100:\tlearn: 0.1965713\ttotal: 3.6s\tremaining: 32.1s\n",
      "200:\tlearn: 0.1571676\ttotal: 8.29s\tremaining: 33s\n",
      "300:\tlearn: 0.1361743\ttotal: 12.8s\tremaining: 29.8s\n",
      "400:\tlearn: 0.1216822\ttotal: 17.5s\tremaining: 26.1s\n",
      "500:\tlearn: 0.1103377\ttotal: 22.1s\tremaining: 22s\n",
      "600:\tlearn: 0.1012215\ttotal: 25.2s\tremaining: 16.7s\n",
      "700:\tlearn: 0.0934380\ttotal: 26.9s\tremaining: 11.5s\n",
      "800:\tlearn: 0.0863934\ttotal: 28.9s\tremaining: 7.19s\n",
      "900:\tlearn: 0.0804095\ttotal: 31.2s\tremaining: 3.42s\n",
      "999:\tlearn: 0.0752043\ttotal: 33.4s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96000\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91305\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88629\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89947\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 100000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 100000...\n",
      "0:\tlearn: 0.6231834\ttotal: 23.8ms\tremaining: 23.8s\n",
      "100:\tlearn: 0.1963012\ttotal: 2.35s\tremaining: 20.9s\n",
      "200:\tlearn: 0.1566491\ttotal: 4.64s\tremaining: 18.5s\n",
      "300:\tlearn: 0.1347509\ttotal: 6.99s\tremaining: 16.2s\n",
      "400:\tlearn: 0.1200521\ttotal: 9.25s\tremaining: 13.8s\n",
      "500:\tlearn: 0.1086321\ttotal: 11.5s\tremaining: 11.5s\n",
      "600:\tlearn: 0.0993091\ttotal: 13.8s\tremaining: 9.18s\n",
      "700:\tlearn: 0.0917989\ttotal: 16.1s\tremaining: 6.87s\n",
      "800:\tlearn: 0.0851603\ttotal: 18.4s\tremaining: 4.57s\n",
      "900:\tlearn: 0.0789764\ttotal: 20.7s\tremaining: 2.28s\n",
      "999:\tlearn: 0.0736276\ttotal: 23s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96040\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91439\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88029\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89701\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 200000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 200000...\n",
      "0:\tlearn: 0.6236991\ttotal: 28.3ms\tremaining: 28.2s\n",
      "100:\tlearn: 0.1996941\ttotal: 2.49s\tremaining: 22.1s\n",
      "200:\tlearn: 0.1593767\ttotal: 4.73s\tremaining: 18.8s\n",
      "300:\tlearn: 0.1381503\ttotal: 6.98s\tremaining: 16.2s\n",
      "400:\tlearn: 0.1234455\ttotal: 9.22s\tremaining: 13.8s\n",
      "500:\tlearn: 0.1120921\ttotal: 11.5s\tremaining: 11.4s\n",
      "600:\tlearn: 0.1023771\ttotal: 13.7s\tremaining: 9.07s\n",
      "700:\tlearn: 0.0944503\ttotal: 15.9s\tremaining: 6.77s\n",
      "800:\tlearn: 0.0877516\ttotal: 18s\tremaining: 4.47s\n",
      "900:\tlearn: 0.0817995\ttotal: 20s\tremaining: 2.2s\n",
      "999:\tlearn: 0.0762775\ttotal: 22s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.95999\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90976\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.87830\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89376\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 300000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 300000...\n",
      "0:\tlearn: 0.6220533\ttotal: 23.3ms\tremaining: 23.2s\n",
      "100:\tlearn: 0.1975861\ttotal: 2.36s\tremaining: 21s\n",
      "200:\tlearn: 0.1579803\ttotal: 4.66s\tremaining: 18.5s\n",
      "300:\tlearn: 0.1370575\ttotal: 6.91s\tremaining: 16s\n",
      "400:\tlearn: 0.1214314\ttotal: 9.16s\tremaining: 13.7s\n",
      "500:\tlearn: 0.1100177\ttotal: 11.4s\tremaining: 11.4s\n",
      "600:\tlearn: 0.1010484\ttotal: 13.7s\tremaining: 9.09s\n",
      "700:\tlearn: 0.0931667\ttotal: 15.9s\tremaining: 6.8s\n",
      "800:\tlearn: 0.0862024\ttotal: 18.2s\tremaining: 4.51s\n",
      "900:\tlearn: 0.0806895\ttotal: 20.3s\tremaining: 2.23s\n",
      "999:\tlearn: 0.0750469\ttotal: 22.3s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96358\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91832\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88552\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.90162\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 400000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 400000...\n",
      "0:\tlearn: 0.6198663\ttotal: 24.4ms\tremaining: 24.4s\n",
      "100:\tlearn: 0.1998179\ttotal: 2.37s\tremaining: 21.1s\n",
      "200:\tlearn: 0.1580168\ttotal: 4.73s\tremaining: 18.8s\n",
      "300:\tlearn: 0.1367151\ttotal: 7s\tremaining: 16.3s\n",
      "400:\tlearn: 0.1218416\ttotal: 9.25s\tremaining: 13.8s\n",
      "500:\tlearn: 0.1105106\ttotal: 11.5s\tremaining: 11.4s\n",
      "600:\tlearn: 0.1005909\ttotal: 13.8s\tremaining: 9.14s\n",
      "700:\tlearn: 0.0925138\ttotal: 16s\tremaining: 6.8s\n",
      "800:\tlearn: 0.0856969\ttotal: 18.2s\tremaining: 4.52s\n",
      "900:\tlearn: 0.0795109\ttotal: 20.4s\tremaining: 2.25s\n",
      "999:\tlearn: 0.0740146\ttotal: 22.6s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96435\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91176\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88998\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.90074\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 500000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 500000...\n",
      "0:\tlearn: 0.6225026\ttotal: 25.7ms\tremaining: 25.6s\n",
      "100:\tlearn: 0.1989381\ttotal: 2.37s\tremaining: 21.1s\n",
      "200:\tlearn: 0.1584407\ttotal: 4.72s\tremaining: 18.8s\n",
      "300:\tlearn: 0.1369890\ttotal: 7.04s\tremaining: 16.3s\n",
      "400:\tlearn: 0.1216103\ttotal: 9.3s\tremaining: 13.9s\n",
      "500:\tlearn: 0.1102410\ttotal: 11.6s\tremaining: 11.6s\n",
      "600:\tlearn: 0.1009168\ttotal: 13.9s\tremaining: 9.21s\n",
      "700:\tlearn: 0.0925664\ttotal: 16.1s\tremaining: 6.87s\n",
      "800:\tlearn: 0.0853743\ttotal: 18.4s\tremaining: 4.56s\n",
      "900:\tlearn: 0.0791584\ttotal: 20.6s\tremaining: 2.26s\n",
      "999:\tlearn: 0.0736564\ttotal: 22.8s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.95947\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90841\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.87952\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89373\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 600000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 600000...\n",
      "0:\tlearn: 0.6237162\ttotal: 23.4ms\tremaining: 23.4s\n",
      "100:\tlearn: 0.2013709\ttotal: 2.17s\tremaining: 19.3s\n",
      "200:\tlearn: 0.1614538\ttotal: 4.3s\tremaining: 17.1s\n",
      "300:\tlearn: 0.1395187\ttotal: 6.42s\tremaining: 14.9s\n",
      "400:\tlearn: 0.1249975\ttotal: 8.56s\tremaining: 12.8s\n",
      "500:\tlearn: 0.1132939\ttotal: 10.8s\tremaining: 10.8s\n",
      "600:\tlearn: 0.1039493\ttotal: 13s\tremaining: 8.66s\n",
      "700:\tlearn: 0.0958619\ttotal: 15.4s\tremaining: 6.55s\n",
      "800:\tlearn: 0.0889733\ttotal: 17.7s\tremaining: 4.39s\n",
      "900:\tlearn: 0.0822870\ttotal: 20s\tremaining: 2.2s\n",
      "999:\tlearn: 0.0770238\ttotal: 22.3s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96252\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91441\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88145\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89763\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 700000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 700000...\n",
      "0:\tlearn: 0.6233342\ttotal: 156ms\tremaining: 2m 36s\n",
      "100:\tlearn: 0.1995812\ttotal: 2.04s\tremaining: 18.2s\n",
      "200:\tlearn: 0.1584676\ttotal: 3.79s\tremaining: 15.1s\n",
      "300:\tlearn: 0.1372756\ttotal: 5.49s\tremaining: 12.7s\n",
      "400:\tlearn: 0.1226890\ttotal: 7.16s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1114358\ttotal: 8.79s\tremaining: 8.76s\n",
      "600:\tlearn: 0.1022034\ttotal: 10.5s\tremaining: 6.95s\n",
      "700:\tlearn: 0.0940007\ttotal: 12.1s\tremaining: 5.18s\n",
      "800:\tlearn: 0.0871655\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.0804865\ttotal: 15.5s\tremaining: 1.7s\n",
      "999:\tlearn: 0.0753820\ttotal: 17.1s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96166\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91435\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.87802\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89582\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 800000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 800000...\n",
      "0:\tlearn: 0.6226862\ttotal: 151ms\tremaining: 2m 31s\n",
      "100:\tlearn: 0.1999987\ttotal: 2.12s\tremaining: 18.9s\n",
      "200:\tlearn: 0.1600490\ttotal: 3.82s\tremaining: 15.2s\n",
      "300:\tlearn: 0.1383818\ttotal: 5.5s\tremaining: 12.8s\n",
      "400:\tlearn: 0.1234362\ttotal: 7.16s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1115717\ttotal: 8.81s\tremaining: 8.78s\n",
      "600:\tlearn: 0.1019406\ttotal: 10.4s\tremaining: 6.94s\n",
      "700:\tlearn: 0.0940447\ttotal: 12.1s\tremaining: 5.15s\n",
      "800:\tlearn: 0.0871313\ttotal: 13.7s\tremaining: 3.4s\n",
      "900:\tlearn: 0.0807510\ttotal: 15.3s\tremaining: 1.69s\n",
      "999:\tlearn: 0.0750608\ttotal: 17s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96058\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91061\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88123\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89568\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 900000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 900000...\n",
      "0:\tlearn: 0.6212723\ttotal: 165ms\tremaining: 2m 45s\n",
      "100:\tlearn: 0.1974097\ttotal: 2.06s\tremaining: 18.3s\n",
      "200:\tlearn: 0.1583052\ttotal: 3.73s\tremaining: 14.8s\n",
      "300:\tlearn: 0.1372591\ttotal: 5.37s\tremaining: 12.5s\n",
      "400:\tlearn: 0.1219467\ttotal: 7.01s\tremaining: 10.5s\n",
      "500:\tlearn: 0.1102974\ttotal: 8.61s\tremaining: 8.58s\n",
      "600:\tlearn: 0.1007354\ttotal: 10.2s\tremaining: 6.79s\n",
      "700:\tlearn: 0.0928330\ttotal: 11.8s\tremaining: 5.04s\n",
      "800:\tlearn: 0.0861294\ttotal: 13.4s\tremaining: 3.34s\n",
      "900:\tlearn: 0.0801150\ttotal: 15s\tremaining: 1.65s\n",
      "999:\tlearn: 0.0745521\ttotal: 16.6s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96028\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91378\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.87987\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89651\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 1000000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 1000000...\n",
      "0:\tlearn: 0.6214503\ttotal: 137ms\tremaining: 2m 16s\n",
      "100:\tlearn: 0.1987202\ttotal: 2s\tremaining: 17.8s\n",
      "200:\tlearn: 0.1577455\ttotal: 3.75s\tremaining: 14.9s\n",
      "300:\tlearn: 0.1357802\ttotal: 5.49s\tremaining: 12.7s\n",
      "400:\tlearn: 0.1206355\ttotal: 7.2s\tremaining: 10.8s\n",
      "500:\tlearn: 0.1092261\ttotal: 8.87s\tremaining: 8.83s\n",
      "600:\tlearn: 0.0997997\ttotal: 10.5s\tremaining: 6.98s\n",
      "700:\tlearn: 0.0921543\ttotal: 12.2s\tremaining: 5.2s\n",
      "800:\tlearn: 0.0857189\ttotal: 13.8s\tremaining: 3.42s\n",
      "900:\tlearn: 0.0796388\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.0744815\ttotal: 17.1s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.96132\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.91625\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88227\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89894\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 1100000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 1100000...\n",
      "0:\tlearn: 0.6241022\ttotal: 169ms\tremaining: 2m 48s\n",
      "100:\tlearn: 0.1971425\ttotal: 2.04s\tremaining: 18.2s\n",
      "200:\tlearn: 0.1568359\ttotal: 3.76s\tremaining: 14.9s\n",
      "300:\tlearn: 0.1352976\ttotal: 5.43s\tremaining: 12.6s\n",
      "400:\tlearn: 0.1207063\ttotal: 7.08s\tremaining: 10.6s\n",
      "500:\tlearn: 0.1091292\ttotal: 8.72s\tremaining: 8.69s\n",
      "600:\tlearn: 0.1001455\ttotal: 10.3s\tremaining: 6.87s\n",
      "700:\tlearn: 0.0923145\ttotal: 12s\tremaining: 5.1s\n",
      "800:\tlearn: 0.0856397\ttotal: 13.6s\tremaining: 3.37s\n",
      "900:\tlearn: 0.0796555\ttotal: 15.2s\tremaining: 1.67s\n",
      "999:\tlearn: 0.0738139\ttotal: 16.8s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.95567\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90432\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.87536\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.88960\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 1200000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 1200000...\n",
      "0:\tlearn: 0.6213267\ttotal: 173ms\tremaining: 2m 53s\n",
      "100:\tlearn: 0.1983100\ttotal: 1.88s\tremaining: 16.7s\n",
      "200:\tlearn: 0.1567290\ttotal: 3.58s\tremaining: 14.2s\n",
      "300:\tlearn: 0.1360864\ttotal: 5.25s\tremaining: 12.2s\n",
      "400:\tlearn: 0.1211314\ttotal: 6.89s\tremaining: 10.3s\n",
      "500:\tlearn: 0.1097801\ttotal: 8.54s\tremaining: 8.5s\n",
      "600:\tlearn: 0.1006062\ttotal: 10.1s\tremaining: 6.74s\n",
      "700:\tlearn: 0.0927720\ttotal: 11.8s\tremaining: 5.02s\n",
      "800:\tlearn: 0.0857357\ttotal: 13.4s\tremaining: 3.33s\n",
      "900:\tlearn: 0.0797263\ttotal: 15s\tremaining: 1.65s\n",
      "999:\tlearn: 0.0743128\ttotal: 16.6s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.95777\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90887\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.87784\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89309\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 1300000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 1300000...\n",
      "0:\tlearn: 0.6230754\ttotal: 146ms\tremaining: 2m 26s\n",
      "100:\tlearn: 0.1970629\ttotal: 2.1s\tremaining: 18.7s\n",
      "200:\tlearn: 0.1564105\ttotal: 3.87s\tremaining: 15.4s\n",
      "300:\tlearn: 0.1349867\ttotal: 5.61s\tremaining: 13s\n",
      "400:\tlearn: 0.1199479\ttotal: 7.36s\tremaining: 11s\n",
      "500:\tlearn: 0.1088234\ttotal: 9.06s\tremaining: 9.03s\n",
      "600:\tlearn: 0.0999043\ttotal: 10.8s\tremaining: 7.14s\n",
      "700:\tlearn: 0.0920397\ttotal: 12.4s\tremaining: 5.3s\n",
      "800:\tlearn: 0.0850744\ttotal: 14.1s\tremaining: 3.51s\n",
      "900:\tlearn: 0.0790137\ttotal: 15.8s\tremaining: 1.73s\n",
      "999:\tlearn: 0.0738420\ttotal: 17.4s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.95648\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90639\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88114\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89359\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 1400000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 1400000...\n",
      "0:\tlearn: 0.6225575\ttotal: 149ms\tremaining: 2m 29s\n",
      "100:\tlearn: 0.1994853\ttotal: 2.04s\tremaining: 18.1s\n",
      "200:\tlearn: 0.1566992\ttotal: 3.76s\tremaining: 15s\n",
      "300:\tlearn: 0.1340863\ttotal: 5.49s\tremaining: 12.8s\n",
      "400:\tlearn: 0.1196258\ttotal: 7.16s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1084545\ttotal: 8.81s\tremaining: 8.78s\n",
      "600:\tlearn: 0.0987445\ttotal: 10.5s\tremaining: 6.96s\n",
      "700:\tlearn: 0.0905512\ttotal: 12.1s\tremaining: 5.17s\n",
      "800:\tlearn: 0.0839144\ttotal: 13.8s\tremaining: 3.42s\n",
      "900:\tlearn: 0.0782607\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.0730974\ttotal: 17s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.95841\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90779\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.88630\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.89691\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "Processing BRD4 batch starting at offset 1500000...\n",
      "Processing BRD4 train test split done...\n",
      "Processing BRD4 with CATBOOST...\n",
      "Loaded existing CATBOOST model for BRD4\n",
      "Iteration 1 for BRD4 with CATBOOST batch 1500000...\n",
      "0:\tlearn: 0.6255010\ttotal: 261ms\tremaining: 4m 20s\n",
      "100:\tlearn: 0.1973057\ttotal: 24.6s\tremaining: 3m 39s\n",
      "200:\tlearn: 0.1544806\ttotal: 48.5s\tremaining: 3m 12s\n",
      "300:\tlearn: 0.1269293\ttotal: 1m 11s\tremaining: 2m 46s\n",
      "400:\tlearn: 0.1071685\ttotal: 1m 35s\tremaining: 2m 22s\n",
      "500:\tlearn: 0.0912255\ttotal: 1m 59s\tremaining: 1m 58s\n",
      "600:\tlearn: 0.0793633\ttotal: 2m 23s\tremaining: 1m 35s\n",
      "700:\tlearn: 0.0688708\ttotal: 2m 47s\tremaining: 1m 11s\n",
      "800:\tlearn: 0.0606108\ttotal: 3m 11s\tremaining: 47.6s\n",
      "900:\tlearn: 0.0535749\ttotal: 3m 35s\tremaining: 23.7s\n",
      "999:\tlearn: 0.0475882\ttotal: 3m 59s\tremaining: 0us\n",
      "BRD4 - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.94884\n",
      "BRD4 - CATBOOST - Iteration 1 - Precision: 0.90040\n",
      "BRD4 - CATBOOST - Iteration 1 - Recall: 0.85769\n",
      "BRD4 - CATBOOST - Iteration 1 - F1 Score: 0.87853\n",
      "Memory cleared.\n",
      "Model saved: BRD4_catboost_model_iteration_1.pkl\n",
      "1474768\n",
      "Processing HSA batch starting at offset 0...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Initialized new CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 0...\n",
      "0:\tlearn: 0.6250973\ttotal: 141ms\tremaining: 2m 21s\n",
      "100:\tlearn: 0.2577954\ttotal: 2s\tremaining: 17.8s\n",
      "200:\tlearn: 0.2217281\ttotal: 3.74s\tremaining: 14.9s\n",
      "300:\tlearn: 0.1981754\ttotal: 5.44s\tremaining: 12.6s\n",
      "400:\tlearn: 0.1811750\ttotal: 7.16s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1666959\ttotal: 8.82s\tremaining: 8.78s\n",
      "600:\tlearn: 0.1548963\ttotal: 10.5s\tremaining: 6.96s\n",
      "700:\tlearn: 0.1444940\ttotal: 12.1s\tremaining: 5.18s\n",
      "800:\tlearn: 0.1348281\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.1259747\ttotal: 15.5s\tremaining: 1.7s\n",
      "999:\tlearn: 0.1180955\ttotal: 17.1s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.88919\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86323\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.74628\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80051\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 100000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 100000...\n",
      "0:\tlearn: 0.6261337\ttotal: 152ms\tremaining: 2m 31s\n",
      "100:\tlearn: 0.2590125\ttotal: 2.03s\tremaining: 18s\n",
      "200:\tlearn: 0.2220189\ttotal: 3.75s\tremaining: 14.9s\n",
      "300:\tlearn: 0.1981458\ttotal: 5.45s\tremaining: 12.7s\n",
      "400:\tlearn: 0.1813265\ttotal: 7.12s\tremaining: 10.6s\n",
      "500:\tlearn: 0.1675880\ttotal: 8.77s\tremaining: 8.73s\n",
      "600:\tlearn: 0.1558448\ttotal: 10.4s\tremaining: 6.91s\n",
      "700:\tlearn: 0.1449246\ttotal: 12.1s\tremaining: 5.15s\n",
      "800:\tlearn: 0.1349755\ttotal: 13.7s\tremaining: 3.41s\n",
      "900:\tlearn: 0.1269117\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.1189808\ttotal: 17s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89090\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.84702\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75959\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80092\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 200000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 200000...\n",
      "0:\tlearn: 0.6266454\ttotal: 168ms\tremaining: 2m 48s\n",
      "100:\tlearn: 0.2575109\ttotal: 2.08s\tremaining: 18.5s\n",
      "200:\tlearn: 0.2219694\ttotal: 3.83s\tremaining: 15.2s\n",
      "300:\tlearn: 0.1991786\ttotal: 5.56s\tremaining: 12.9s\n",
      "400:\tlearn: 0.1821672\ttotal: 7.26s\tremaining: 10.8s\n",
      "500:\tlearn: 0.1677259\ttotal: 8.93s\tremaining: 8.9s\n",
      "600:\tlearn: 0.1559020\ttotal: 10.6s\tremaining: 7.03s\n",
      "700:\tlearn: 0.1459333\ttotal: 12.3s\tremaining: 5.24s\n",
      "800:\tlearn: 0.1361295\ttotal: 14s\tremaining: 3.48s\n",
      "900:\tlearn: 0.1279457\ttotal: 15.6s\tremaining: 1.72s\n",
      "999:\tlearn: 0.1201868\ttotal: 17.3s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89859\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.85888\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75923\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80598\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 300000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 300000...\n",
      "0:\tlearn: 0.6270852\ttotal: 165ms\tremaining: 2m 44s\n",
      "100:\tlearn: 0.2600396\ttotal: 2.13s\tremaining: 19s\n",
      "200:\tlearn: 0.2243792\ttotal: 3.9s\tremaining: 15.5s\n",
      "300:\tlearn: 0.2010883\ttotal: 5.67s\tremaining: 13.2s\n",
      "400:\tlearn: 0.1830810\ttotal: 7.42s\tremaining: 11.1s\n",
      "500:\tlearn: 0.1691786\ttotal: 9.14s\tremaining: 9.11s\n",
      "600:\tlearn: 0.1561292\ttotal: 10.9s\tremaining: 7.21s\n",
      "700:\tlearn: 0.1448138\ttotal: 12.6s\tremaining: 5.37s\n",
      "800:\tlearn: 0.1354028\ttotal: 14.3s\tremaining: 3.55s\n",
      "900:\tlearn: 0.1269609\ttotal: 16s\tremaining: 1.76s\n",
      "999:\tlearn: 0.1194895\ttotal: 17.7s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.90417\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.87109\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.76496\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.81459\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 400000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 400000...\n",
      "0:\tlearn: 0.6291735\ttotal: 143ms\tremaining: 2m 22s\n",
      "100:\tlearn: 0.2631350\ttotal: 1.97s\tremaining: 17.6s\n",
      "200:\tlearn: 0.2255001\ttotal: 3.68s\tremaining: 14.6s\n",
      "300:\tlearn: 0.2020813\ttotal: 5.38s\tremaining: 12.5s\n",
      "400:\tlearn: 0.1853190\ttotal: 7.06s\tremaining: 10.5s\n",
      "500:\tlearn: 0.1708184\ttotal: 8.71s\tremaining: 8.67s\n",
      "600:\tlearn: 0.1587357\ttotal: 10.4s\tremaining: 6.88s\n",
      "700:\tlearn: 0.1477603\ttotal: 12s\tremaining: 5.13s\n",
      "800:\tlearn: 0.1386423\ttotal: 13.7s\tremaining: 3.4s\n",
      "900:\tlearn: 0.1298425\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.1218954\ttotal: 17s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89945\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86344\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75914\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80794\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 500000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 500000...\n",
      "0:\tlearn: 0.6242446\ttotal: 175ms\tremaining: 2m 54s\n",
      "100:\tlearn: 0.2563954\ttotal: 1.97s\tremaining: 17.6s\n",
      "200:\tlearn: 0.2201438\ttotal: 3.7s\tremaining: 14.7s\n",
      "300:\tlearn: 0.1976498\ttotal: 5.4s\tremaining: 12.6s\n",
      "400:\tlearn: 0.1801519\ttotal: 7.11s\tremaining: 10.6s\n",
      "500:\tlearn: 0.1660024\ttotal: 8.79s\tremaining: 8.76s\n",
      "600:\tlearn: 0.1534411\ttotal: 10.5s\tremaining: 6.96s\n",
      "700:\tlearn: 0.1429314\ttotal: 12.2s\tremaining: 5.19s\n",
      "800:\tlearn: 0.1339860\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.1253147\ttotal: 15.5s\tremaining: 1.7s\n",
      "999:\tlearn: 0.1177118\ttotal: 17.1s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.90100\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86987\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.76889\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.81627\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 600000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 600000...\n",
      "0:\tlearn: 0.6263542\ttotal: 165ms\tremaining: 2m 44s\n",
      "100:\tlearn: 0.2564020\ttotal: 2.07s\tremaining: 18.4s\n",
      "200:\tlearn: 0.2212396\ttotal: 3.76s\tremaining: 15s\n",
      "300:\tlearn: 0.1989809\ttotal: 5.46s\tremaining: 12.7s\n",
      "400:\tlearn: 0.1817391\ttotal: 7.13s\tremaining: 10.6s\n",
      "500:\tlearn: 0.1674109\ttotal: 8.78s\tremaining: 8.74s\n",
      "600:\tlearn: 0.1554542\ttotal: 10.4s\tremaining: 6.92s\n",
      "700:\tlearn: 0.1445115\ttotal: 12.1s\tremaining: 5.16s\n",
      "800:\tlearn: 0.1351253\ttotal: 13.7s\tremaining: 3.41s\n",
      "900:\tlearn: 0.1264863\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.1185413\ttotal: 17s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89750\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86610\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75022\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80401\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 700000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 700000...\n",
      "0:\tlearn: 0.6257902\ttotal: 165ms\tremaining: 2m 44s\n",
      "100:\tlearn: 0.2563966\ttotal: 2.04s\tremaining: 18.1s\n",
      "200:\tlearn: 0.2203949\ttotal: 3.79s\tremaining: 15.1s\n",
      "300:\tlearn: 0.1980417\ttotal: 5.53s\tremaining: 12.8s\n",
      "400:\tlearn: 0.1804458\ttotal: 7.26s\tremaining: 10.8s\n",
      "500:\tlearn: 0.1666755\ttotal: 8.96s\tremaining: 8.92s\n",
      "600:\tlearn: 0.1550660\ttotal: 10.7s\tremaining: 7.08s\n",
      "700:\tlearn: 0.1447155\ttotal: 12.4s\tremaining: 5.28s\n",
      "800:\tlearn: 0.1348448\ttotal: 14.1s\tremaining: 3.5s\n",
      "900:\tlearn: 0.1267107\ttotal: 15.8s\tremaining: 1.74s\n",
      "999:\tlearn: 0.1188918\ttotal: 17.5s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89444\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.85333\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75801\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80285\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 800000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 800000...\n",
      "0:\tlearn: 0.6254546\ttotal: 174ms\tremaining: 2m 53s\n",
      "100:\tlearn: 0.2571260\ttotal: 1.99s\tremaining: 17.7s\n",
      "200:\tlearn: 0.2209569\ttotal: 3.72s\tremaining: 14.8s\n",
      "300:\tlearn: 0.1985630\ttotal: 5.44s\tremaining: 12.6s\n",
      "400:\tlearn: 0.1806378\ttotal: 7.17s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1667315\ttotal: 8.87s\tremaining: 8.83s\n",
      "600:\tlearn: 0.1544338\ttotal: 10.6s\tremaining: 7s\n",
      "700:\tlearn: 0.1442905\ttotal: 12.2s\tremaining: 5.21s\n",
      "800:\tlearn: 0.1347984\ttotal: 13.9s\tremaining: 3.46s\n",
      "900:\tlearn: 0.1260886\ttotal: 15.6s\tremaining: 1.71s\n",
      "999:\tlearn: 0.1189804\ttotal: 17.2s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.90292\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86921\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.76574\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.81420\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 900000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 900000...\n",
      "0:\tlearn: 0.6252541\ttotal: 150ms\tremaining: 2m 29s\n",
      "100:\tlearn: 0.2570230\ttotal: 1.99s\tremaining: 17.7s\n",
      "200:\tlearn: 0.2215467\ttotal: 3.71s\tremaining: 14.8s\n",
      "300:\tlearn: 0.1989847\ttotal: 5.4s\tremaining: 12.5s\n",
      "400:\tlearn: 0.1823885\ttotal: 7.07s\tremaining: 10.6s\n",
      "500:\tlearn: 0.1678328\ttotal: 8.74s\tremaining: 8.71s\n",
      "600:\tlearn: 0.1557512\ttotal: 10.4s\tremaining: 6.9s\n",
      "700:\tlearn: 0.1446732\ttotal: 12.1s\tremaining: 5.15s\n",
      "800:\tlearn: 0.1350080\ttotal: 13.7s\tremaining: 3.41s\n",
      "900:\tlearn: 0.1266433\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.1190409\ttotal: 17s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.90116\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.87092\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75751\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.81026\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 1000000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 1000000...\n",
      "0:\tlearn: 0.6256977\ttotal: 171ms\tremaining: 2m 50s\n",
      "100:\tlearn: 0.2592595\ttotal: 1.97s\tremaining: 17.6s\n",
      "200:\tlearn: 0.2233583\ttotal: 3.74s\tremaining: 14.9s\n",
      "300:\tlearn: 0.2011811\ttotal: 5.46s\tremaining: 12.7s\n",
      "400:\tlearn: 0.1840669\ttotal: 7.15s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1691944\ttotal: 8.83s\tremaining: 8.8s\n",
      "600:\tlearn: 0.1570692\ttotal: 10.5s\tremaining: 6.98s\n",
      "700:\tlearn: 0.1461881\ttotal: 12.2s\tremaining: 5.2s\n",
      "800:\tlearn: 0.1362789\ttotal: 13.9s\tremaining: 3.45s\n",
      "900:\tlearn: 0.1281028\ttotal: 15.6s\tremaining: 1.71s\n",
      "999:\tlearn: 0.1207030\ttotal: 17.2s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89947\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86235\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.76591\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.81127\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 1100000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 1100000...\n",
      "0:\tlearn: 0.6264770\ttotal: 168ms\tremaining: 2m 48s\n",
      "100:\tlearn: 0.2581262\ttotal: 2.1s\tremaining: 18.7s\n",
      "200:\tlearn: 0.2211926\ttotal: 3.88s\tremaining: 15.4s\n",
      "300:\tlearn: 0.1988823\ttotal: 5.63s\tremaining: 13.1s\n",
      "400:\tlearn: 0.1813630\ttotal: 7.38s\tremaining: 11s\n",
      "500:\tlearn: 0.1672606\ttotal: 9.07s\tremaining: 9.04s\n",
      "600:\tlearn: 0.1548114\ttotal: 10.8s\tremaining: 7.16s\n",
      "700:\tlearn: 0.1443942\ttotal: 12.5s\tremaining: 5.32s\n",
      "800:\tlearn: 0.1352230\ttotal: 14.2s\tremaining: 3.52s\n",
      "900:\tlearn: 0.1269688\ttotal: 15.8s\tremaining: 1.74s\n",
      "999:\tlearn: 0.1189625\ttotal: 17.5s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89514\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86317\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75888\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80767\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 1200000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 1200000...\n",
      "0:\tlearn: 0.6274843\ttotal: 158ms\tremaining: 2m 37s\n",
      "100:\tlearn: 0.2587939\ttotal: 2s\tremaining: 17.8s\n",
      "200:\tlearn: 0.2221796\ttotal: 3.73s\tremaining: 14.8s\n",
      "300:\tlearn: 0.2002812\ttotal: 5.42s\tremaining: 12.6s\n",
      "400:\tlearn: 0.1828852\ttotal: 7.1s\tremaining: 10.6s\n",
      "500:\tlearn: 0.1686151\ttotal: 8.76s\tremaining: 8.72s\n",
      "600:\tlearn: 0.1564769\ttotal: 10.4s\tremaining: 6.92s\n",
      "700:\tlearn: 0.1456190\ttotal: 12.1s\tremaining: 5.16s\n",
      "800:\tlearn: 0.1365101\ttotal: 13.7s\tremaining: 3.41s\n",
      "900:\tlearn: 0.1274893\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.1197528\ttotal: 17s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.89723\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86091\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.75470\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.80431\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 1300000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 1300000...\n",
      "0:\tlearn: 0.6286332\ttotal: 174ms\tremaining: 2m 54s\n",
      "100:\tlearn: 0.2611492\ttotal: 2.06s\tremaining: 18.4s\n",
      "200:\tlearn: 0.2249778\ttotal: 3.9s\tremaining: 15.5s\n",
      "300:\tlearn: 0.2024968\ttotal: 5.68s\tremaining: 13.2s\n",
      "400:\tlearn: 0.1841106\ttotal: 7.44s\tremaining: 11.1s\n",
      "500:\tlearn: 0.1700052\ttotal: 9.19s\tremaining: 9.15s\n",
      "600:\tlearn: 0.1576271\ttotal: 10.9s\tremaining: 7.27s\n",
      "700:\tlearn: 0.1467440\ttotal: 12.7s\tremaining: 5.43s\n",
      "800:\tlearn: 0.1376723\ttotal: 14.5s\tremaining: 3.59s\n",
      "900:\tlearn: 0.1292584\ttotal: 16.2s\tremaining: 1.78s\n",
      "999:\tlearn: 0.1213889\ttotal: 17.9s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.90636\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.87203\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.77351\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.81982\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "Processing HSA batch starting at offset 1400000...\n",
      "Processing HSA train test split done...\n",
      "Processing HSA with CATBOOST...\n",
      "Loaded existing CATBOOST model for HSA\n",
      "Iteration 1 for HSA with CATBOOST batch 1400000...\n",
      "0:\tlearn: 0.6275819\ttotal: 157ms\tremaining: 2m 36s\n",
      "100:\tlearn: 0.2531294\ttotal: 2.06s\tremaining: 18.3s\n",
      "200:\tlearn: 0.2127802\ttotal: 3.78s\tremaining: 15s\n",
      "300:\tlearn: 0.1862368\ttotal: 5.51s\tremaining: 12.8s\n",
      "400:\tlearn: 0.1680675\ttotal: 7.19s\tremaining: 10.7s\n",
      "500:\tlearn: 0.1525260\ttotal: 8.86s\tremaining: 8.83s\n",
      "600:\tlearn: 0.1395511\ttotal: 10.5s\tremaining: 6.98s\n",
      "700:\tlearn: 0.1281678\ttotal: 12.2s\tremaining: 5.19s\n",
      "800:\tlearn: 0.1186804\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.1102469\ttotal: 15.4s\tremaining: 1.7s\n",
      "999:\tlearn: 0.1024975\ttotal: 17.1s\tremaining: 0us\n",
      "HSA - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.88909\n",
      "HSA - CATBOOST - Iteration 1 - Precision: 0.86213\n",
      "HSA - CATBOOST - Iteration 1 - Recall: 0.74420\n",
      "HSA - CATBOOST - Iteration 1 - F1 Score: 0.79883\n",
      "Memory cleared.\n",
      "Model saved: HSA_catboost_model_iteration_1.pkl\n",
      "1785555\n",
      "Processing sEH batch starting at offset 0...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Initialized new CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 0...\n",
      "0:\tlearn: 0.5805583\ttotal: 159ms\tremaining: 2m 38s\n",
      "100:\tlearn: 0.1194381\ttotal: 1.99s\tremaining: 17.7s\n",
      "200:\tlearn: 0.0884799\ttotal: 3.66s\tremaining: 14.6s\n",
      "300:\tlearn: 0.0739282\ttotal: 5.29s\tremaining: 12.3s\n",
      "400:\tlearn: 0.0643998\ttotal: 6.91s\tremaining: 10.3s\n",
      "500:\tlearn: 0.0572217\ttotal: 8.5s\tremaining: 8.46s\n",
      "600:\tlearn: 0.0517746\ttotal: 10.1s\tremaining: 6.68s\n",
      "700:\tlearn: 0.0468762\ttotal: 11.7s\tremaining: 4.98s\n",
      "800:\tlearn: 0.0425396\ttotal: 13.3s\tremaining: 3.29s\n",
      "900:\tlearn: 0.0387519\ttotal: 14.9s\tremaining: 1.63s\n",
      "999:\tlearn: 0.0351877\ttotal: 16.5s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99072\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96863\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96399\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96630\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 100000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 100000...\n",
      "0:\tlearn: 0.5813458\ttotal: 161ms\tremaining: 2m 41s\n",
      "100:\tlearn: 0.1190814\ttotal: 2.01s\tremaining: 17.9s\n",
      "200:\tlearn: 0.0891471\ttotal: 3.73s\tremaining: 14.8s\n",
      "300:\tlearn: 0.0751331\ttotal: 5.38s\tremaining: 12.5s\n",
      "400:\tlearn: 0.0657605\ttotal: 7.03s\tremaining: 10.5s\n",
      "500:\tlearn: 0.0584461\ttotal: 8.67s\tremaining: 8.63s\n",
      "600:\tlearn: 0.0524280\ttotal: 10.3s\tremaining: 6.84s\n",
      "700:\tlearn: 0.0475366\ttotal: 11.9s\tremaining: 5.08s\n",
      "800:\tlearn: 0.0432088\ttotal: 13.5s\tremaining: 3.36s\n",
      "900:\tlearn: 0.0396710\ttotal: 15.2s\tremaining: 1.67s\n",
      "999:\tlearn: 0.0363161\ttotal: 16.9s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99077\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96553\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96945\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96748\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 200000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 200000...\n",
      "0:\tlearn: 0.5804033\ttotal: 159ms\tremaining: 2m 39s\n",
      "100:\tlearn: 0.1184836\ttotal: 1.99s\tremaining: 17.7s\n",
      "200:\tlearn: 0.0879123\ttotal: 3.71s\tremaining: 14.7s\n",
      "300:\tlearn: 0.0731891\ttotal: 5.37s\tremaining: 12.5s\n",
      "400:\tlearn: 0.0637549\ttotal: 6.99s\tremaining: 10.4s\n",
      "500:\tlearn: 0.0562772\ttotal: 8.63s\tremaining: 8.6s\n",
      "600:\tlearn: 0.0501786\ttotal: 10.3s\tremaining: 6.81s\n",
      "700:\tlearn: 0.0454142\ttotal: 11.9s\tremaining: 5.08s\n",
      "800:\tlearn: 0.0413077\ttotal: 13.5s\tremaining: 3.36s\n",
      "900:\tlearn: 0.0376009\ttotal: 15.1s\tremaining: 1.66s\n",
      "999:\tlearn: 0.0343670\ttotal: 16.7s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98843\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96434\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96121\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96277\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 300000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 300000...\n",
      "0:\tlearn: 0.5788497\ttotal: 165ms\tremaining: 2m 44s\n",
      "100:\tlearn: 0.1182437\ttotal: 2.14s\tremaining: 19.1s\n",
      "200:\tlearn: 0.0862400\ttotal: 3.9s\tremaining: 15.5s\n",
      "300:\tlearn: 0.0723811\ttotal: 5.63s\tremaining: 13.1s\n",
      "400:\tlearn: 0.0630616\ttotal: 7.31s\tremaining: 10.9s\n",
      "500:\tlearn: 0.0560263\ttotal: 8.95s\tremaining: 8.91s\n",
      "600:\tlearn: 0.0503307\ttotal: 10.6s\tremaining: 7.03s\n",
      "700:\tlearn: 0.0454606\ttotal: 12.3s\tremaining: 5.23s\n",
      "800:\tlearn: 0.0412811\ttotal: 14s\tremaining: 3.47s\n",
      "900:\tlearn: 0.0376860\ttotal: 15.6s\tremaining: 1.72s\n",
      "999:\tlearn: 0.0343066\ttotal: 17.3s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99019\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96150\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96475\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96312\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 400000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 400000...\n",
      "0:\tlearn: 0.5786034\ttotal: 171ms\tremaining: 2m 50s\n",
      "100:\tlearn: 0.1174961\ttotal: 2.09s\tremaining: 18.6s\n",
      "200:\tlearn: 0.0873428\ttotal: 3.83s\tremaining: 15.2s\n",
      "300:\tlearn: 0.0725633\ttotal: 5.55s\tremaining: 12.9s\n",
      "400:\tlearn: 0.0630182\ttotal: 7.21s\tremaining: 10.8s\n",
      "500:\tlearn: 0.0559205\ttotal: 8.85s\tremaining: 8.82s\n",
      "600:\tlearn: 0.0500950\ttotal: 10.5s\tremaining: 6.96s\n",
      "700:\tlearn: 0.0452962\ttotal: 12.1s\tremaining: 5.17s\n",
      "800:\tlearn: 0.0413131\ttotal: 13.7s\tremaining: 3.41s\n",
      "900:\tlearn: 0.0377022\ttotal: 15.4s\tremaining: 1.69s\n",
      "999:\tlearn: 0.0343789\ttotal: 17s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98987\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96603\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96639\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96621\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 500000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 500000...\n",
      "0:\tlearn: 0.5777868\ttotal: 153ms\tremaining: 2m 33s\n",
      "100:\tlearn: 0.1196716\ttotal: 2.03s\tremaining: 18.1s\n",
      "200:\tlearn: 0.0878267\ttotal: 3.74s\tremaining: 14.9s\n",
      "300:\tlearn: 0.0735542\ttotal: 5.39s\tremaining: 12.5s\n",
      "400:\tlearn: 0.0639210\ttotal: 7.03s\tremaining: 10.5s\n",
      "500:\tlearn: 0.0563989\ttotal: 8.67s\tremaining: 8.64s\n",
      "600:\tlearn: 0.0505426\ttotal: 10.3s\tremaining: 6.83s\n",
      "700:\tlearn: 0.0453539\ttotal: 11.9s\tremaining: 5.08s\n",
      "800:\tlearn: 0.0410774\ttotal: 13.5s\tremaining: 3.36s\n",
      "900:\tlearn: 0.0373567\ttotal: 15.2s\tremaining: 1.67s\n",
      "999:\tlearn: 0.0342595\ttotal: 16.8s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99180\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96694\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96504\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96599\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 600000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 600000...\n",
      "0:\tlearn: 0.5784914\ttotal: 143ms\tremaining: 2m 23s\n",
      "100:\tlearn: 0.1158384\ttotal: 2.09s\tremaining: 18.6s\n",
      "200:\tlearn: 0.0859218\ttotal: 3.83s\tremaining: 15.2s\n",
      "300:\tlearn: 0.0709427\ttotal: 5.51s\tremaining: 12.8s\n",
      "400:\tlearn: 0.0615735\ttotal: 7.17s\tremaining: 10.7s\n",
      "500:\tlearn: 0.0544235\ttotal: 8.81s\tremaining: 8.78s\n",
      "600:\tlearn: 0.0487851\ttotal: 10.5s\tremaining: 6.95s\n",
      "700:\tlearn: 0.0439553\ttotal: 12.1s\tremaining: 5.17s\n",
      "800:\tlearn: 0.0400066\ttotal: 13.8s\tremaining: 3.42s\n",
      "900:\tlearn: 0.0362271\ttotal: 15.4s\tremaining: 1.7s\n",
      "999:\tlearn: 0.0331744\ttotal: 17.1s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99024\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96578\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96351\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96465\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 700000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 700000...\n",
      "0:\tlearn: 0.5771333\ttotal: 152ms\tremaining: 2m 32s\n",
      "100:\tlearn: 0.1176952\ttotal: 2.1s\tremaining: 18.7s\n",
      "200:\tlearn: 0.0865314\ttotal: 3.9s\tremaining: 15.5s\n",
      "300:\tlearn: 0.0721778\ttotal: 5.6s\tremaining: 13s\n",
      "400:\tlearn: 0.0625357\ttotal: 7.28s\tremaining: 10.9s\n",
      "500:\tlearn: 0.0557061\ttotal: 8.96s\tremaining: 8.92s\n",
      "600:\tlearn: 0.0500395\ttotal: 10.6s\tremaining: 7.06s\n",
      "700:\tlearn: 0.0454517\ttotal: 12.3s\tremaining: 5.25s\n",
      "800:\tlearn: 0.0412071\ttotal: 14s\tremaining: 3.47s\n",
      "900:\tlearn: 0.0374233\ttotal: 15.7s\tremaining: 1.72s\n",
      "999:\tlearn: 0.0341623\ttotal: 17.3s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98759\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96297\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96572\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96434\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 800000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 800000...\n",
      "0:\tlearn: 0.5771418\ttotal: 177ms\tremaining: 2m 56s\n",
      "100:\tlearn: 0.1163830\ttotal: 2.04s\tremaining: 18.2s\n",
      "200:\tlearn: 0.0862169\ttotal: 3.8s\tremaining: 15.1s\n",
      "300:\tlearn: 0.0723731\ttotal: 5.44s\tremaining: 12.6s\n",
      "400:\tlearn: 0.0624317\ttotal: 7.09s\tremaining: 10.6s\n",
      "500:\tlearn: 0.0554943\ttotal: 8.72s\tremaining: 8.69s\n",
      "600:\tlearn: 0.0497703\ttotal: 10.3s\tremaining: 6.87s\n",
      "700:\tlearn: 0.0450741\ttotal: 12s\tremaining: 5.11s\n",
      "800:\tlearn: 0.0407939\ttotal: 13.6s\tremaining: 3.38s\n",
      "900:\tlearn: 0.0372968\ttotal: 15.2s\tremaining: 1.67s\n",
      "999:\tlearn: 0.0341196\ttotal: 16.8s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98899\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96328\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96718\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96523\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 900000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 900000...\n",
      "0:\tlearn: 0.5783886\ttotal: 148ms\tremaining: 2m 27s\n",
      "100:\tlearn: 0.1169147\ttotal: 1.98s\tremaining: 17.6s\n",
      "200:\tlearn: 0.0868807\ttotal: 3.66s\tremaining: 14.6s\n",
      "300:\tlearn: 0.0723112\ttotal: 5.3s\tremaining: 12.3s\n",
      "400:\tlearn: 0.0628422\ttotal: 6.94s\tremaining: 10.4s\n",
      "500:\tlearn: 0.0558075\ttotal: 8.54s\tremaining: 8.51s\n",
      "600:\tlearn: 0.0499535\ttotal: 10.1s\tremaining: 6.74s\n",
      "700:\tlearn: 0.0454233\ttotal: 11.8s\tremaining: 5.02s\n",
      "800:\tlearn: 0.0413976\ttotal: 13.3s\tremaining: 3.31s\n",
      "900:\tlearn: 0.0377892\ttotal: 15s\tremaining: 1.64s\n",
      "999:\tlearn: 0.0346429\ttotal: 16.6s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98983\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96265\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96524\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96394\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1000000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1000000...\n",
      "0:\tlearn: 0.5777641\ttotal: 163ms\tremaining: 2m 43s\n",
      "100:\tlearn: 0.1145828\ttotal: 1.99s\tremaining: 17.7s\n",
      "200:\tlearn: 0.0846441\ttotal: 3.73s\tremaining: 14.8s\n",
      "300:\tlearn: 0.0707492\ttotal: 5.42s\tremaining: 12.6s\n",
      "400:\tlearn: 0.0613138\ttotal: 7.11s\tremaining: 10.6s\n",
      "500:\tlearn: 0.0542319\ttotal: 8.82s\tremaining: 8.78s\n",
      "600:\tlearn: 0.0482019\ttotal: 10.5s\tremaining: 6.96s\n",
      "700:\tlearn: 0.0434978\ttotal: 12.2s\tremaining: 5.18s\n",
      "800:\tlearn: 0.0394717\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.0361000\ttotal: 15.4s\tremaining: 1.7s\n",
      "999:\tlearn: 0.0330784\ttotal: 17.1s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99100\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96860\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96126\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96492\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1100000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1100000...\n",
      "0:\tlearn: 0.5794342\ttotal: 171ms\tremaining: 2m 50s\n",
      "100:\tlearn: 0.1172357\ttotal: 2.12s\tremaining: 18.9s\n",
      "200:\tlearn: 0.0874157\ttotal: 3.87s\tremaining: 15.4s\n",
      "300:\tlearn: 0.0731129\ttotal: 5.59s\tremaining: 13s\n",
      "400:\tlearn: 0.0637770\ttotal: 7.25s\tremaining: 10.8s\n",
      "500:\tlearn: 0.0562459\ttotal: 8.92s\tremaining: 8.88s\n",
      "600:\tlearn: 0.0504562\ttotal: 10.6s\tremaining: 7.02s\n",
      "700:\tlearn: 0.0456078\ttotal: 12.2s\tremaining: 5.21s\n",
      "800:\tlearn: 0.0414773\ttotal: 13.9s\tremaining: 3.44s\n",
      "900:\tlearn: 0.0379683\ttotal: 15.5s\tremaining: 1.7s\n",
      "999:\tlearn: 0.0348135\ttotal: 17.1s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99009\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96775\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96396\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96585\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1200000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1200000...\n",
      "0:\tlearn: 0.5797537\ttotal: 149ms\tremaining: 2m 28s\n",
      "100:\tlearn: 0.1195558\ttotal: 2.04s\tremaining: 18.2s\n",
      "200:\tlearn: 0.0893110\ttotal: 3.75s\tremaining: 14.9s\n",
      "300:\tlearn: 0.0745054\ttotal: 5.41s\tremaining: 12.6s\n",
      "400:\tlearn: 0.0651843\ttotal: 7.04s\tremaining: 10.5s\n",
      "500:\tlearn: 0.0577339\ttotal: 8.65s\tremaining: 8.62s\n",
      "600:\tlearn: 0.0520669\ttotal: 10.3s\tremaining: 6.82s\n",
      "700:\tlearn: 0.0472307\ttotal: 11.9s\tremaining: 5.07s\n",
      "800:\tlearn: 0.0429007\ttotal: 13.5s\tremaining: 3.35s\n",
      "900:\tlearn: 0.0392360\ttotal: 15.1s\tremaining: 1.66s\n",
      "999:\tlearn: 0.0359924\ttotal: 16.7s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98929\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96596\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.95927\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96261\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1300000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1300000...\n",
      "0:\tlearn: 0.5808547\ttotal: 166ms\tremaining: 2m 45s\n",
      "100:\tlearn: 0.1200297\ttotal: 2.13s\tremaining: 18.9s\n",
      "200:\tlearn: 0.0887378\ttotal: 3.84s\tremaining: 15.3s\n",
      "300:\tlearn: 0.0736237\ttotal: 5.56s\tremaining: 12.9s\n",
      "400:\tlearn: 0.0639466\ttotal: 7.21s\tremaining: 10.8s\n",
      "500:\tlearn: 0.0566948\ttotal: 8.85s\tremaining: 8.82s\n",
      "600:\tlearn: 0.0505804\ttotal: 10.5s\tremaining: 6.98s\n",
      "700:\tlearn: 0.0455370\ttotal: 12.2s\tremaining: 5.18s\n",
      "800:\tlearn: 0.0413936\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.0375597\ttotal: 15.5s\tremaining: 1.7s\n",
      "999:\tlearn: 0.0344483\ttotal: 17.2s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98990\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96420\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96491\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96456\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1400000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1400000...\n",
      "0:\tlearn: 0.5794353\ttotal: 176ms\tremaining: 2m 56s\n",
      "100:\tlearn: 0.1163848\ttotal: 2.13s\tremaining: 19s\n",
      "200:\tlearn: 0.0870362\ttotal: 3.87s\tremaining: 15.4s\n",
      "300:\tlearn: 0.0724147\ttotal: 5.57s\tremaining: 12.9s\n",
      "400:\tlearn: 0.0632811\ttotal: 7.22s\tremaining: 10.8s\n",
      "500:\tlearn: 0.0563665\ttotal: 8.87s\tremaining: 8.83s\n",
      "600:\tlearn: 0.0505921\ttotal: 10.5s\tremaining: 6.99s\n",
      "700:\tlearn: 0.0455715\ttotal: 12.2s\tremaining: 5.19s\n",
      "800:\tlearn: 0.0413711\ttotal: 13.8s\tremaining: 3.43s\n",
      "900:\tlearn: 0.0377704\ttotal: 15.4s\tremaining: 1.7s\n",
      "999:\tlearn: 0.0346724\ttotal: 17.1s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98830\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96290\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96419\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96354\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1500000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1500000...\n",
      "0:\tlearn: 0.5800843\ttotal: 156ms\tremaining: 2m 36s\n",
      "100:\tlearn: 0.1183931\ttotal: 2.06s\tremaining: 18.4s\n",
      "200:\tlearn: 0.0894373\ttotal: 3.81s\tremaining: 15.2s\n",
      "300:\tlearn: 0.0745103\ttotal: 5.55s\tremaining: 12.9s\n",
      "400:\tlearn: 0.0651472\ttotal: 7.21s\tremaining: 10.8s\n",
      "500:\tlearn: 0.0577489\ttotal: 8.87s\tremaining: 8.84s\n",
      "600:\tlearn: 0.0518914\ttotal: 10.6s\tremaining: 7s\n",
      "700:\tlearn: 0.0469687\ttotal: 12.2s\tremaining: 5.2s\n",
      "800:\tlearn: 0.0424236\ttotal: 13.8s\tremaining: 3.44s\n",
      "900:\tlearn: 0.0384712\ttotal: 15.5s\tremaining: 1.71s\n",
      "999:\tlearn: 0.0352978\ttotal: 17.2s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99116\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96598\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.96550\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96574\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1600000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1600000...\n",
      "0:\tlearn: 0.5797915\ttotal: 172ms\tremaining: 2m 51s\n",
      "100:\tlearn: 0.1187450\ttotal: 2.19s\tremaining: 19.5s\n",
      "200:\tlearn: 0.0882493\ttotal: 3.94s\tremaining: 15.7s\n",
      "300:\tlearn: 0.0736446\ttotal: 5.68s\tremaining: 13.2s\n",
      "400:\tlearn: 0.0638533\ttotal: 7.32s\tremaining: 10.9s\n",
      "500:\tlearn: 0.0566066\ttotal: 8.99s\tremaining: 8.95s\n",
      "600:\tlearn: 0.0507471\ttotal: 10.7s\tremaining: 7.09s\n",
      "700:\tlearn: 0.0458563\ttotal: 12.3s\tremaining: 5.26s\n",
      "800:\tlearn: 0.0414394\ttotal: 14s\tremaining: 3.48s\n",
      "900:\tlearn: 0.0377476\ttotal: 15.6s\tremaining: 1.72s\n",
      "999:\tlearn: 0.0345546\ttotal: 17.3s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.99033\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96320\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.95986\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96153\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n",
      "Processing sEH batch starting at offset 1700000...\n",
      "Processing sEH train test split done...\n",
      "Processing sEH with CATBOOST...\n",
      "Loaded existing CATBOOST model for sEH\n",
      "Iteration 1 for sEH with CATBOOST batch 1700000...\n",
      "0:\tlearn: 0.5803613\ttotal: 135ms\tremaining: 2m 15s\n",
      "100:\tlearn: 0.1198969\ttotal: 2s\tremaining: 17.8s\n",
      "200:\tlearn: 0.0875374\ttotal: 3.68s\tremaining: 14.6s\n",
      "300:\tlearn: 0.0726113\ttotal: 5.3s\tremaining: 12.3s\n",
      "400:\tlearn: 0.0625687\ttotal: 6.9s\tremaining: 10.3s\n",
      "500:\tlearn: 0.0554092\ttotal: 8.5s\tremaining: 8.47s\n",
      "600:\tlearn: 0.0493175\ttotal: 10.1s\tremaining: 6.71s\n",
      "700:\tlearn: 0.0442717\ttotal: 11.7s\tremaining: 4.99s\n",
      "800:\tlearn: 0.0397466\ttotal: 13.3s\tremaining: 3.31s\n",
      "900:\tlearn: 0.0361181\ttotal: 14.9s\tremaining: 1.64s\n",
      "999:\tlearn: 0.0326099\ttotal: 16.5s\tremaining: 0us\n",
      "sEH - CATBOOST - Iteration 1 - Mean Average Precision (mAP): 0.98673\n",
      "sEH - CATBOOST - Iteration 1 - Precision: 0.96227\n",
      "sEH - CATBOOST - Iteration 1 - Recall: 0.95991\n",
      "sEH - CATBOOST - Iteration 1 - F1 Score: 0.96109\n",
      "Memory cleared.\n",
      "Model saved: sEH_catboost_model_iteration_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# import duckdb\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import cupy as cp\n",
    "# import gc\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.metrics import average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # GPU 설정 확인 및 메모리 사용량 조절\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# if physical_devices:\n",
    "#     try:\n",
    "#         tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#         print(\"TensorFlow GPU memory growth enabled\")\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "# else:\n",
    "#     print(\"TensorFlow GPU not available\")\n",
    "\n",
    "# # 데이터 로드 및 전처리\n",
    "# input_parquet_path = 'processed_merged_queried_data.parquet'\n",
    "\n",
    "# con = duckdb.connect()\n",
    "\n",
    "# # GPU 설정을 위한 XGBClassifier 옵션\n",
    "# xgb_params = {\n",
    "#     'n_estimators': 100,\n",
    "#     'device': 'cuda',\n",
    "#     'eta': 0.1,\n",
    "#     'max_depth': 14,\n",
    "#     'updater': 'grow_gpu_hist',\n",
    "#     'refresh_leaf': 1,\n",
    "#     'process_type': 'default',\n",
    "#     'use_label_encoder': False,\n",
    "#     'objective': 'binary:logistic', #'rank:map', \n",
    "#     'eval_metric': 'error'#'auc' #'map'\n",
    "# }\n",
    "\n",
    "# # RandomForestClassifier 옵션\n",
    "# rf_params = {\n",
    "#     'n_estimators': 100,\n",
    "#     'n_jobs': -1\n",
    "# }\n",
    "\n",
    "# # CatBoostClassifier 옵션\n",
    "# catboost_params = {\n",
    "#     'iterations': 1000,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'depth': 10,\n",
    "#     'task_type': 'GPU',\n",
    "#     'verbose': 100\n",
    "# }\n",
    "\n",
    "# # 메모리 정리 함수\n",
    "# def clear_memory():\n",
    "#     gc.collect()\n",
    "#     print(\"Memory cleared.\")\n",
    "\n",
    "# # 모델 학습 및 평가 함수 정의\n",
    "# def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name, iteration, model_type):\n",
    "#     if model_type == 'catboost':\n",
    "#         X_train = cp.asnumpy(X_train)\n",
    "#         X_test = cp.asnumpy(X_test)\n",
    "#     else:\n",
    "#         X_train = cp.asnumpy(X_train)\n",
    "#         X_test = cp.asnumpy(X_test)\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "#     y_pred = model.predict(X_test)\n",
    "    \n",
    "#     map_score = average_precision_score(y_test, y_pred_proba)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Precision: {precision:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Recall: {recall:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "#     clear_memory()\n",
    "#     return model\n",
    "\n",
    "# def train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, model_name, iteration):\n",
    "#     # CuPy 배열을 NumPy 배열로 변환\n",
    "#     X_train_np = cp.asnumpy(X_train)\n",
    "#     X_test_np = cp.asnumpy(X_test)\n",
    "\n",
    "#     model.fit(X_train_np, y_train)\n",
    "#     y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "#     y_pred = model.predict(X_test_np)\n",
    "#     y_pred_proba_np = cp.asnumpy(y_pred_proba)\n",
    "#     y_pred_np = cp.asnumpy(y_pred)\n",
    "\n",
    "#     map_score = average_precision_score(y_test, y_pred_proba_np)\n",
    "#     accuracy = accuracy_score(y_test, y_pred_np)\n",
    "#     precision = precision_score(y_test, y_pred_np)\n",
    "#     recall = recall_score(y_test, y_pred_np)\n",
    "#     f1 = f1_score(y_test, y_pred_np)\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Precision: {precision:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Recall: {recall:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "#     # Delete GPU arrays to free up memory\n",
    "#     del X_train_np, X_test_np, y_pred_proba, y_pred\n",
    "#     cp._default_memory_pool.free_all_blocks()\n",
    "#     clear_memory()\n",
    "#     return model\n",
    "\n",
    "# # LSTM 모델 학습 및 평가 함수 정의 (TensorFlow)\n",
    "# def train_and_evaluate_lstm_tf(model, X_train, X_test, y_train, y_test, model_name, iteration, batch_size, num_epochs):\n",
    "#     # TensorFlow 텐서로 변환\n",
    "#     X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "#     X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "#     y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "#     y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "#     # 모델 컴파일\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(),\n",
    "#                   metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "#     # 학습\n",
    "#     model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "#     # 예측\n",
    "#     y_pred_proba = model.predict(X_test_tensor)\n",
    "#     y_pred = (y_pred_proba >= 0.5).astype(int)  # 정밀도 수정함!!!!!!!!!!!! 기본이 0.5\n",
    "\n",
    "#     map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "#     accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "#     precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "#     recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "#     f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Precision: {precision:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - Recall: {recall:.5f}\")\n",
    "#     print(f\"{model_name} - Iteration {iteration} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "#     clear_memory()\n",
    "#     return model, precision, recall, f1, map_score\n",
    "\n",
    "# def get_model(model_type):\n",
    "#     if model_type == 'random_forest':\n",
    "#         return RandomForestClassifier(**rf_params)\n",
    "#     elif model_type == 'xgboost':\n",
    "#         return XGBClassifier(**xgb_params)\n",
    "#     elif model_type == 'catboost':\n",
    "#         return CatBoostClassifier(**catboost_params)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "# protein_names = ['BRD4', 'HSA', 'sEH']\n",
    "# # protein_names = ['HSA']  # 'BRD4', 'HSA', 'sEH'\n",
    "# # 각 protein에 대한 최적의 조합을 설정\n",
    "# best_combinations = {\n",
    "#     'BRD4': [('elu', 'relu')],  # 확정\n",
    "#     'HSA': [('relu', 'elu')],\n",
    "# #    'HSA': [('elu', 'relu')],\n",
    "#     # 'HSA': [('elu','tanh')]\n",
    "#     'sEH': [('elu', 'relu')]  # 확정\n",
    "# }\n",
    "\n",
    "# # 사용할 모델 종류 지정: 'lstm', 'randomforest', 'xgboost', 'catboost'\n",
    "# #model_types = ['lstm', 'randomforest', 'xgboost', 'catboost']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "# model_types = ['catboost']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "# num_iterations = 1\n",
    "\n",
    "# # Precision 기록을 위한 리스트\n",
    "# precision_records = []\n",
    "# total_records = []\n",
    "\n",
    "# # RAM usage monitoring\n",
    "# def monitor_memory(threshold=0.80):\n",
    "#     memory_info = psutil.virtual_memory()\n",
    "#     return memory_info.percent / 100 >= threshold\n",
    "\n",
    "# for protein in protein_names:\n",
    "#     # 전체 행 수 계산\n",
    "#     total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "#     print(total_rows)\n",
    "#     # 배치 크기 설정\n",
    "#     batch_size = 100000  # 적절한 배치 크기로 설정\n",
    "\n",
    "#     # 데이터 배치로 읽어와서 처리\n",
    "#     for offset in range(0, total_rows, batch_size):\n",
    "#         filtered_df = con.execute(f\"\"\"\n",
    "#         SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "#         WHERE protein_name = '{protein}'\n",
    "#         LIMIT {batch_size} OFFSET {offset}\n",
    "#         \"\"\").df()\n",
    "\n",
    "#         print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "\n",
    "#         X = np.concatenate([\n",
    "#             np.array(filtered_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "#             np.array(filtered_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "#             np.array(filtered_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "#             np.array(filtered_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "#         ], axis=1)\n",
    "#         y = filtered_df['binds'].tolist()\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#         print(f\"Processing {protein} train test split done...\")\n",
    "\n",
    "#         for model_type in model_types:\n",
    "#             print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "#             if model_type == 'lstm':\n",
    "#                 for activation_1, activation_2 in best_combinations[protein]:\n",
    "#                     input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "#                     hidden_dim = 128\n",
    "#                     output_dim = 1\n",
    "#                     num_layers = 2\n",
    "#                     num_epochs = 5\n",
    "#                     batch_size = 1024  # 2048\n",
    "\n",
    "#                     model = Sequential([\n",
    "#                         Input(shape=(None, input_dim)),  # 여기를 수정했습니다.\n",
    "#                         LSTM(hidden_dim*8, return_sequences=True, activation=activation_1, dropout=0.05),\n",
    "#                         LSTM(hidden_dim*4, return_sequences=True, activation=activation_2, dropout=0.05),\n",
    "#                         LSTM(hidden_dim*1, return_sequences=False, activation=activation_1, dropout=0.05),\n",
    "#                         Dense(output_dim, activation='sigmoid')\n",
    "#                     ])\n",
    "\n",
    "#                     model_filename = f\"{protein}_lstm_model_{activation_1}_{activation_2}_{activation_1}_iteration_1.weights.h5\"\n",
    "\n",
    "#                     # LSTM 모델 로드 또는 초기화\n",
    "#                     try:\n",
    "#                         model.load_weights(model_filename)\n",
    "#                         print(f\"Loaded existing LSTM model for {protein} with {activation_1} and {activation_2}\")\n",
    "#                     except FileNotFoundError:\n",
    "#                         print(f\"Initialized new LSTM model for {protein} with {activation_1} and {activation_2}\")\n",
    "\n",
    "#                     for i in range(num_iterations):\n",
    "#                         print(f\"Iteration {i + 1} for {protein} with LSTM batch {offset}...\")\n",
    "\n",
    "#                         X_train_lstm = np.expand_dims(X_train, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "#                         X_test_lstm = np.expand_dims(X_test, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "#                         model, precision, recall, f1, mAP = train_and_evaluate_lstm_tf(model, X_train_lstm, X_test_lstm, y_train, y_test, f\"{protein} - LSTM\", i + 1, batch_size, num_epochs)\n",
    "\n",
    "#                         # Precision 기록\n",
    "#                         precision_records.append((protein, activation_1, activation_2, precision, recall, f1, mAP, offset))\n",
    "\n",
    "#                         if monitor_memory():\n",
    "#                             print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "#                             clear_memory()\n",
    "\n",
    "#                         # 모델 저장\n",
    "#                         model.save_weights(model_filename)\n",
    "#                         print(f\"Model saved: {model_filename}\")\n",
    "#                 total_records.extend(precision_records)\n",
    "#                 precision_records = []\n",
    "\n",
    "#             else:\n",
    "#                 model = get_model(model_type)\n",
    "#                 model_filename = f\"{protein}_{model_type}_model_iteration_1.pkl\"\n",
    "\n",
    "#                 # 모델 로드 또는 초기화\n",
    "#                 try:\n",
    "#                     model = joblib.load(model_filename)\n",
    "#                     print(f\"Loaded existing {model_type.upper()} model for {protein}\")\n",
    "#                 except FileNotFoundError:\n",
    "#                     print(f\"Initialized new {model_type.upper()} model for {protein}\")\n",
    "\n",
    "#                 for i in range(num_iterations):\n",
    "#                     print(f\"Iteration {i + 1} for {protein} with {model_type.upper()} batch {offset}...\")\n",
    "\n",
    "#                     if model_type in ['xgboost', 'catboost']:\n",
    "#                         model = train_and_evaluate_model(model, X_train, X_test, y_train, y_test, f\"{protein} - {model_type.upper()}\", i + 1, model_type)\n",
    "#                     elif model_type == 'randomforest':\n",
    "#                         model = train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, f\"{protein} - {model_type.upper()}\", i + 1)\n",
    "\n",
    "#                     if monitor_memory():\n",
    "#                         print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "#                         clear_memory()\n",
    "\n",
    "#                     # 모델 저장\n",
    "#                     joblib.dump(model, model_filename)\n",
    "#                     print(f\"Model saved: {model_filename}\")\n",
    "\n",
    "#             # 각 best_combinations마다 average precision과 average recall 계산 및 출력 (LSTM 모델에 대해서만)\n",
    "#             if model_type == 'lstm':\n",
    "#                 precision_df = pd.DataFrame(total_records, columns=['Protein', 'Activation 1', 'Activation 2', 'Precision', 'Recall', 'f1-score', 'mAP', 'batch_num'])\n",
    "#                 avg_precision = precision_df.groupby(['Protein', 'Activation 1', 'Activation 2'])['Precision'].mean().reset_index()\n",
    "#                 avg_recall = precision_df.groupby(['Protein', 'Activation 1', 'Activation 2'])['Recall'].mean().reset_index()\n",
    "#                 avg_f1 = precision_df.groupby(['Protein', 'Activation 1', 'Activation 2'])['f1-score'].mean().reset_index()\n",
    "\n",
    "#                 print(f\"Average Recall by Activation Function Combinations (Batch {offset}):\")\n",
    "#                 print(avg_recall)\n",
    "#                 print(f\"Average Precision by Activation Function Combinations (Batch {offset}):\")\n",
    "#                 print(avg_precision)\n",
    "#                 print(f\"Average f1-score by Activation Function Combinations (Batch {offset}):\")\n",
    "#                 print(avg_f1)\n",
    "# con.close()\n",
    "\n",
    "# # 모든 기록을 데이터프레임으로 변환하여 저장\n",
    "# results_df = pd.DataFrame(total_records, columns=['Protein', 'Activation 1', 'Activation 2', 'Precision', 'Recall', 'f1-score', 'mAP', 'batch_num'])\n",
    "# results_df.to_csv('total_precision_records.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test 이걸로 하면 됨 240629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for BRD4\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for HSA\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Loaded CATBOOST model for sEH\n",
      "Memory cleared.\n",
      "Test results saved to 'test_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# GPU 설정 확인 및 메모리 사용량 조절\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"TensorFlow GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"TensorFlow GPU not available\")\n",
    "\n",
    "# 데이터베이스 연결\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# 테스트 데이터 경로 설정\n",
    "test_parquet_path = 'test_processed_data.parquet'\n",
    "\n",
    "# 각 protein에 대한 최적의 조합을 설정\n",
    "best_combinations = {\n",
    "    'BRD4': [('elu', 'relu')],\n",
    "    'HSA': [('relu', 'elu')],\n",
    "    'sEH': [('elu', 'relu')]\n",
    "}\n",
    "\n",
    "# 사용할 모델 종류 지정\n",
    "#model_types = ['lstm', 'xgboost', 'randomforest', 'catboost']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "model_types = ['catboost']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "batch_size = 20000  # 적절한 배치 크기로 설정\n",
    "\n",
    "# 메모리 정리 함수\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "for protein in best_combinations.keys():\n",
    "    # 총 행 수 계산\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{test_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch_df = con.execute(f\"\"\"\n",
    "            SELECT * FROM read_parquet('{test_parquet_path}')\n",
    "            WHERE protein_name = '{protein}'\n",
    "            LIMIT {batch_size} OFFSET {start}\n",
    "        \"\"\").df()\n",
    "\n",
    "        # 테스트 데이터 전처리\n",
    "        X_test = np.concatenate([\n",
    "            np.array(batch_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            for activation_1, activation_2 in best_combinations[protein]:\n",
    "                if model_type == 'xgboost':\n",
    "                    model = XGBClassifier()\n",
    "                    model_filename = f\"{protein}_xgb_model_iteration_1.pkl\"\n",
    "                elif model_type == 'randomforest':\n",
    "                    model = RandomForestClassifier()\n",
    "                    model_filename = f\"{protein}_rf_model_iteration_1.pkl\"\n",
    "                elif model_type == 'catboost':\n",
    "                    model = CatBoostClassifier()\n",
    "                    model_filename = f\"{protein}_catboost_model_iteration_1.pkl\"\n",
    "                elif model_type == 'lstm':\n",
    "                    input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "                    hidden_dim = 128\n",
    "                    output_dim = 1\n",
    "\n",
    "                    model = Sequential([\n",
    "                        Input(shape=(None, input_dim)),\n",
    "                        LSTM(hidden_dim * 8, return_sequences=True, activation=activation_1, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 4, return_sequences=True, activation=activation_2, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 1, return_sequences=False, activation=activation_1, dropout=0.05),\n",
    "                        Dense(output_dim, activation='sigmoid')\n",
    "                    ])\n",
    "                    model_filename = f\"{protein}_lstm_model_{activation_1}_{activation_2}_{activation_1}_iteration_1.weights.h5\"\n",
    "\n",
    "                # 모델 로드\n",
    "                if model_type == 'lstm':\n",
    "                    model.load_weights(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein} with {activation_1} and {activation_2}\")\n",
    "                else:\n",
    "                    model = joblib.load(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein}\")\n",
    "\n",
    "                # 예측 수행\n",
    "                if model_type == 'lstm':\n",
    "                    X_test_lstm = np.expand_dims(X_test, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "                    y_pred = model.predict(X_test_lstm)\n",
    "                elif model_type == 'randomforest':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'xgboost':\n",
    "                    X_test_gpu = cp.array(X_test)\n",
    "                    y_pred = model.predict_proba(X_test_gpu)[:, 1]\n",
    "                    y_pred = cp.asnumpy(y_pred)\n",
    "                elif model_type == 'catboost':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                # 결과 저장\n",
    "                test_results = pd.DataFrame({\n",
    "                    'id': batch_df['id'],\n",
    "                    'binds': y_pred.flatten()\n",
    "                })\n",
    "                results.append(test_results)\n",
    "\n",
    "                # 메모리 정리\n",
    "                clear_memory()\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 결합\n",
    "final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 결과 저장\n",
    "final_results_df.to_csv('test_results.csv', index=False)\n",
    "print(\"Test results saved to 'test_results.csv'\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train파일검토코드-캐글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (1089744, 7)\n",
      "           id                             buildingblock1_smiles  \\\n",
      "0   200462670  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "1   200462671  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "2   200462672  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "3   200462673  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "4   200462674  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "5   200462675  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "6   200462676  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "7   200462677  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "8   200462678  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "9   200462679  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "10  200462680  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "11  200462681  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "12  200462682  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "13  200462683  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "14  200462684  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "15  200462685  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "16  200462686  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "17  200462687  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "18  200462688  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "19  200462689  O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21   \n",
      "\n",
      "   buildingblock2_smiles    buildingblock3_smiles  \\\n",
      "0   C#CCOc1ccc(CN)cc1.Cl  Br.Br.NCC1CCCN1c1cccnn1   \n",
      "1   C#CCOc1ccc(CN)cc1.Cl  Br.Br.NCC1CCCN1c1cccnn1   \n",
      "2   C#CCOc1ccc(CN)cc1.Cl  Br.Br.NCC1CCCN1c1cccnn1   \n",
      "3   C#CCOc1ccc(CN)cc1.Cl        Br.NCc1cccc(Br)n1   \n",
      "4   C#CCOc1ccc(CN)cc1.Cl        Br.NCc1cccc(Br)n1   \n",
      "5   C#CCOc1ccc(CN)cc1.Cl        Br.NCc1cccc(Br)n1   \n",
      "6   C#CCOc1ccc(CN)cc1.Cl     C#CCOc1ccc(CN)cc1.Cl   \n",
      "7   C#CCOc1ccc(CN)cc1.Cl     C#CCOc1ccc(CN)cc1.Cl   \n",
      "8   C#CCOc1ccc(CN)cc1.Cl     C#CCOc1ccc(CN)cc1.Cl   \n",
      "9   C#CCOc1ccc(CN)cc1.Cl       C=C(C)C(=O)NCCN.Cl   \n",
      "10  C#CCOc1ccc(CN)cc1.Cl       C=C(C)C(=O)NCCN.Cl   \n",
      "11  C#CCOc1ccc(CN)cc1.Cl       C=C(C)C(=O)NCCN.Cl   \n",
      "12  C#CCOc1ccc(CN)cc1.Cl          C=CCNC(=O)CN.Cl   \n",
      "13  C#CCOc1ccc(CN)cc1.Cl          C=CCNC(=O)CN.Cl   \n",
      "14  C#CCOc1ccc(CN)cc1.Cl          C=CCNC(=O)CN.Cl   \n",
      "15  C#CCOc1ccc(CN)cc1.Cl              C=CCOC(C)CN   \n",
      "16  C#CCOc1ccc(CN)cc1.Cl              C=CCOC(C)CN   \n",
      "17  C#CCOc1ccc(CN)cc1.Cl              C=CCOC(C)CN   \n",
      "18  C#CCOc1ccc(CN)cc1.Cl                C=CCOCCCN   \n",
      "19  C#CCOc1ccc(CN)cc1.Cl                C=CCOCCCN   \n",
      "\n",
      "                                      molecule_smiles protein_name  binds  \n",
      "0   C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(Nc3ccc(...         BRD4      0  \n",
      "1   C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(Nc3ccc(...          HSA      0  \n",
      "2   C#CCOc1ccc(CNc2nc(NCC3CCCN3c3cccnn3)nc(Nc3ccc(...          sEH      0  \n",
      "3   C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(Nc3ccc(F)c...         BRD4      0  \n",
      "4   C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(Nc3ccc(F)c...          HSA      0  \n",
      "5   C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(Nc3ccc(F)c...          sEH      0  \n",
      "6   C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(Nc3ccc(...         BRD4      0  \n",
      "7   C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(Nc3ccc(...          HSA      0  \n",
      "8   C#CCOc1ccc(CNc2nc(NCc3ccc(OCC#C)cc3)nc(Nc3ccc(...          sEH      0  \n",
      "9   C#CCOc1ccc(CNc2nc(NCCNC(=O)C(=C)C)nc(Nc3ccc(F)...         BRD4      0  \n",
      "10  C#CCOc1ccc(CNc2nc(NCCNC(=O)C(=C)C)nc(Nc3ccc(F)...          HSA      0  \n",
      "11  C#CCOc1ccc(CNc2nc(NCCNC(=O)C(=C)C)nc(Nc3ccc(F)...          sEH      0  \n",
      "12  C#CCOc1ccc(CNc2nc(NCC(=O)NCC=C)nc(Nc3ccc(F)c(C...         BRD4      0  \n",
      "13  C#CCOc1ccc(CNc2nc(NCC(=O)NCC=C)nc(Nc3ccc(F)c(C...          HSA      0  \n",
      "14  C#CCOc1ccc(CNc2nc(NCC(=O)NCC=C)nc(Nc3ccc(F)c(C...          sEH      0  \n",
      "15  C#CCOc1ccc(CNc2nc(NCC(C)OCC=C)nc(Nc3ccc(F)c(C(...         BRD4      0  \n",
      "16  C#CCOc1ccc(CNc2nc(NCC(C)OCC=C)nc(Nc3ccc(F)c(C(...          HSA      0  \n",
      "17  C#CCOc1ccc(CNc2nc(NCC(C)OCC=C)nc(Nc3ccc(F)c(C(...          sEH      0  \n",
      "18  C#CCOc1ccc(CNc2nc(NCCCOCC=C)nc(Nc3ccc(F)c(C(=O...         BRD4      0  \n",
      "19  C#CCOc1ccc(CNc2nc(NCCCOCC=C)nc(Nc3ccc(F)c(C(=O...          HSA      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# 새로운 Parquet 파일에서 buildingblock1_smiles 컬럼으로 특정 값과 일치하는 행 추출\n",
    "final_parquet_file = './final_train_enc.parquet'\n",
    "final_parquet_file = './test.parquet'\n",
    "final_parquet_file = './train.parquet'\n",
    "\n",
    "# 특정 값\n",
    "target_smiles = 'O=C(Nc1ccc(F)c(C(=O)O)c1F)OCC1c2ccccc2-c2ccccc21'\n",
    "\n",
    "# Parquet 파일을 읽기\n",
    "parquet_file = pq.ParquetFile(final_parquet_file)\n",
    "\n",
    "# 청크 단위로 데이터 읽기 및 필터링\n",
    "matched_smiles_list = []\n",
    "\n",
    "# 각 row group을 청크 단위로 처리\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    df_chunk = parquet_file.read_row_group(i).to_pandas()\n",
    "    matched_smiles_df_chunk = df_chunk[df_chunk['buildingblock1_smiles'].str.startswith(target_smiles)]\n",
    "    matched_smiles_list.append(matched_smiles_df_chunk)\n",
    "\n",
    "# 모든 청크를 합쳐서 하나의 DataFrame으로 만들기\n",
    "matched_smiles_df = pd.concat(matched_smiles_list, ignore_index=True)\n",
    "\n",
    "# 일치하는 데이터 목록 확인\n",
    "print(\"Shape of data:\", matched_smiles_df.shape)\n",
    "print(matched_smiles_df.head(20))\n",
    "\n",
    "# DataFrame 저장 또는 다른 방식으로 활용\n",
    "matched_smiles_df.to_csv('./matched_smiles_data3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train & catboost & 1DCNN 학습모델 & \n",
    "fold 개선& 모듈화 + ECFP 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import psutil\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 파라미터 설정\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'device': 'cuda',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 14,\n",
    "    'updater': 'grow_gpu_hist',\n",
    "    'refresh_leaf': 1,\n",
    "    'process_type': 'default',\n",
    "    'use_label_encoder': False,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 10,\n",
    "    'task_type': 'GPU',\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "def setup_gpu():\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "            print(\"TensorFlow GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"TensorFlow GPU not available\")\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name, fold, model_type):\n",
    "    X_train = cp.asnumpy(X_train)\n",
    "    X_test = cp.asnumpy(X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    map_score = average_precision_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, map_score, accuracy, precision, recall, f1\n",
    "\n",
    "def train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, model_name, fold):\n",
    "    X_train_np = cp.asnumpy(X_train)\n",
    "    X_test_np = cp.asnumpy(X_test)\n",
    "\n",
    "    model.fit(X_train_np, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "    y_pred = model.predict(X_test_np)\n",
    "    y_pred_proba_np = cp.asnumpy(y_pred_proba)\n",
    "    y_pred_np = cp.asnumpy(y_pred)\n",
    "\n",
    "    map_score = average_precision_score(y_test, y_pred_proba_np)\n",
    "    accuracy = accuracy_score(y_test, y_pred_np)\n",
    "    precision = precision_score(y_test, y_pred_np)\n",
    "    recall = recall_score(y_test, y_pred_np)\n",
    "    f1 = f1_score(y_test, y_pred_np)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    del X_train_np, X_test_np, y_pred_proba, y_pred\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    clear_memory()\n",
    "    return model, map_score, accuracy, precision, recall, f1\n",
    "\n",
    "def train_and_evaluate_lstm_tf(model, X_train, X_test, y_train, y_test, model_name, fold, batch_size, num_epochs):\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "    model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, precision, recall, f1, map_score\n",
    "\n",
    "def OneDCNN_model(input_len, num_classes):\n",
    "    hidden_dim = 128\n",
    "    num_filters = 32\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(input_len,))\n",
    "    x = Embedding(input_dim=36, output_dim=hidden_dim, input_length=input_len, mask_zero=True)(inputs)\n",
    "    x = Conv1D(filters=num_filters, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=num_filters*2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=num_filters*3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss = 'binary_crossentropy'\n",
    "    weighted_metrics = [AUC(curve='PR', name='avg_precision')]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn_tf(model, X_train, X_test, y_train, y_test, model_name, fold, batch_size, num_epochs):\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "    es = EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n",
    "    checkpoint = ModelCheckpoint(monitor='val_loss', filepath=f\"{model_name}.h5\",\n",
    "                                 save_best_only=True, save_weights_only=True, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, verbose=1)\n",
    "\n",
    "    model.fit(X_train_tensor, y_train_tensor, validation_data=(X_test_tensor, y_test_tensor),\n",
    "              epochs=num_epochs, callbacks=[checkpoint, reduce_lr_loss, es],\n",
    "              batch_size=batch_size, verbose=1)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, precision, recall, f1, map_score\n",
    "\n",
    "def get_model(model_type, input_len=None, num_classes=None):\n",
    "    if model_type == 'random_forest':\n",
    "        return RandomForestClassifier(**rf_params)\n",
    "    elif model_type == 'xgboost':\n",
    "        return XGBClassifier(**xgb_params)\n",
    "    elif model_type == 'catboost':\n",
    "        return CatBoostClassifier(**catboost_params)\n",
    "    elif model_type == 'cnn':\n",
    "        return OneDCNN_model(input_len, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "def monitor_memory(threshold=0.80):\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    return memory_info.percent / 100 >= threshold\n",
    "\n",
    "# ECFP 생성 함수\n",
    "def generate_ecfp(smiles, radius=2, bits=1024):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    if molecule is None:\n",
    "        return [0] * bits\n",
    "    return list(AllChem.GetMorganFingerprintAsBitVect(molecule, radius, nBits=bits))\n",
    "\n",
    "def process_batch(df):\n",
    "    # 각 SMILES 열에 대해 ECFP 생성\n",
    "    df['molecule_ecfp'] = df['molecule_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock1_ecfp'] = df['buildingblock1_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock2_ecfp'] = df['buildingblock2_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock3_ecfp'] = df['buildingblock3_smiles'].apply(generate_ecfp)\n",
    "    \n",
    "    # 필요한 열만 포함된 DataFrame 반환\n",
    "    return df[['id', 'protein_name', 'molecule_ecfp', 'buildingblock1_ecfp', 'buildingblock2_ecfp', 'buildingblock3_ecfp', 'binds']]\n",
    "\n",
    "def preprocess_and_save_ecfp(input_path, output_path, batch_size=32768):\n",
    "    reader = pq.ParquetFile(input_path)\n",
    "    \n",
    "    # 적절한 스키마로 Parquet writer 초기화\n",
    "    schema = pa.schema([\n",
    "        ('id', pa.int32()),\n",
    "        ('molecule_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock1_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock2_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock3_ecfp', pa.list_(pa.int32())),\n",
    "        ('protein_name', pa.string()),\n",
    "        ('binds', pa.int32())  # test할 때는 제외\n",
    "    ])\n",
    "    \n",
    "    with pq.ParquetWriter(output_path, schema) as writer:\n",
    "        total_batches = reader.metadata.num_row_groups\n",
    "        \n",
    "        with tqdm(total=total_batches, desc=\"Processing\", unit=\"batch\", leave=True) as pbar:\n",
    "            for batch in reader.iter_batches(batch_size=batch_size):\n",
    "                df_batch = batch.to_pandas()\n",
    "                \n",
    "                processed_batch = process_batch(df_batch)\n",
    "                \n",
    "                # 처리된 DataFrame을 Arrow Table로 변환하여 파일에 작성\n",
    "                table = pa.Table.from_pandas(processed_batch, schema=schema)\n",
    "                writer.write_table(table)\n",
    "                \n",
    "                # 진행 상황 업데이트\n",
    "                pbar.update(1)\n",
    "\n",
    "                # 주기적으로 가비지 컬렉션 호출\n",
    "                gc.collect()\n",
    "\n",
    "def process_protein_batches(protein, model_types, input_parquet_path, num_folds, ecfp=False):\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "    print(total_rows)\n",
    "    batch_size = 100000\n",
    "\n",
    "    for offset in range(0, total_rows, batch_size):\n",
    "        filtered_df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "        WHERE protein_name = '{protein}'\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\").df()\n",
    "\n",
    "        print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "\n",
    "        if ecfp:\n",
    "            filtered_df = process_batch(filtered_df)\n",
    "\n",
    "        X = np.concatenate([\n",
    "            np.array(filtered_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "        y = filtered_df['binds'].tolist()\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "            process_model_type(model_type, skf, X, y, protein, offset)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def process_model_type(model_type, skf, X, y, protein, offset):\n",
    "    precision_records = []\n",
    "    best_map_score = 0\n",
    "    best_model = None\n",
    "    best_fold = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {fold + 1} for {protein} with {model_type.upper()} batch {offset}...\")\n",
    "\n",
    "        model = get_model(model_type)\n",
    "        model_filename = f\"{protein}_{model_type}_model_fold_{fold}.pkl\"\n",
    "\n",
    "        try:\n",
    "            model = joblib.load(model_filename)\n",
    "            print(f\"Loaded existing {model_type.upper()} model for {protein}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Initialized new {model_type.upper()} model for {protein}\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        if model_type in ['xgboost', 'catboost']:\n",
    "            model, map_score, accuracy, precision, recall, f1 = train_and_evaluate_model(model, X_train, X_val, y_train, y_val, f\"{protein} - {model_type.upper()}\", fold + 1, model_type)\n",
    "        elif model_type == 'random_forest':\n",
    "            model, map_score, accuracy, precision, recall, f1 = train_and_evaluate_model_rf(model, X_train, X_val, y_train, y_val, f\"{protein} - {model_type.upper()}\", fold + 1)\n",
    "\n",
    "        precision_records.append((protein, model_type, 'N/A', precision, recall, f1, map_score, offset, fold + 1))\n",
    "\n",
    "        if map_score > best_map_score:\n",
    "            best_map_score = map_score\n",
    "            best_model = model\n",
    "            best_fold = fold\n",
    "\n",
    "        if monitor_memory():\n",
    "            print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "            clear_memory()\n",
    "\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        best_model_filename = f\"{protein}_{model_type}_best_model.pkl\"\n",
    "        joblib.dump(best_model, best_model_filename)\n",
    "        print(f\"Best model saved: {best_model_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_gpu()\n",
    "    protein_names = ['BRD4', 'HSA', 'sEH']\n",
    "    model_types = ['catboost']\n",
    "    num_folds = 5\n",
    "    input_parquet_path = 'processed_merged_queried_data.parquet'\n",
    "    ecfp = True  # ECFP 생성 여부 설정\n",
    "\n",
    "    if ecfp:\n",
    "        output_parquet_path = 'processed_merged_queried_ecfp_data.parquet'\n",
    "        preprocess_and_save_ecfp(input_parquet_path, output_parquet_path)\n",
    "        input_parquet_path = output_parquet_path\n",
    "\n",
    "    for protein in protein_names:\n",
    "        process_protein_batches(protein, model_types, input_parquet_path, num_folds, ecfp=ecfp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train & catboost & 1DCNN 학습모델 & \n",
    "fold 개선& 모듈화 - 240702실행 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "1521460\n",
      "Processing BRD4 batch starting at offset 0...\n",
      "Processing BRD4 with XGBOOST...\n",
      "Fold 1 for BRD4 with XGBOOST batch 0...\n",
      "Loaded existing XGBOOST model for BRD4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:56:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 306\u001b[0m\n\u001b[0;32m    303\u001b[0m         process_protein_batches(protein, model_types, input_parquet_path, num_folds)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 303\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m input_parquet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_merged_queried_data.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protein \u001b[38;5;129;01min\u001b[39;00m protein_names:\n\u001b[1;32m--> 303\u001b[0m     \u001b[43mprocess_protein_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_parquet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 246\u001b[0m, in \u001b[0;36mprocess_protein_batches\u001b[1;34m(protein, model_types, input_parquet_path, num_folds)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m model_types:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 246\u001b[0m         \u001b[43mprocess_model_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m con\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[6], line 272\u001b[0m, in \u001b[0;36mprocess_model_type\u001b[1;34m(model_type, skf, X, y, protein, offset)\u001b[0m\n\u001b[0;32m    269\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)[train_idx], np\u001b[38;5;241m.\u001b[39marray(y)[val_idx]\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatboost\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 272\u001b[0m     model, map_score, accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprotein\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    274\u001b[0m     model, map_score, accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m train_and_evaluate_model_rf(model, X_train, X_val, y_train, y_val, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fold \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(model, X_train, X_test, y_train, y_test, model_name, fold, model_type)\u001b[0m\n\u001b[0;32m     64\u001b[0m X_train \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39masnumpy(X_train)\n\u001b[0;32m     65\u001b[0m X_test \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39masnumpy(X_test)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\xgboost\\sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1529\u001b[0m )\n\u001b[1;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import psutil\n",
    "\n",
    "# 파라미터 설정\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'device': 'cuda',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 14,\n",
    "    'updater': 'grow_gpu_hist',\n",
    "    'refresh_leaf': 1,\n",
    "    'process_type': 'default',\n",
    "    'use_label_encoder': False,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 10,\n",
    "    'task_type': 'GPU',\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "def setup_gpu():\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "            print(\"TensorFlow GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"TensorFlow GPU not available\")\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name, fold, model_type):\n",
    "    X_train = cp.asnumpy(X_train)\n",
    "    X_test = cp.asnumpy(X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    map_score = average_precision_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, map_score, accuracy, precision, recall, f1\n",
    "\n",
    "def train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, model_name, fold):\n",
    "    X_train_np = cp.asnumpy(X_train)\n",
    "    X_test_np = cp.asnumpy(X_test)\n",
    "\n",
    "    model.fit(X_train_np, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "    y_pred = model.predict(X_test_np)\n",
    "    y_pred_proba_np = cp.asnumpy(y_pred_proba)\n",
    "    y_pred_np = cp.asnumpy(y_pred)\n",
    "\n",
    "    map_score = average_precision_score(y_test, y_pred_proba_np)\n",
    "    accuracy = accuracy_score(y_test, y_pred_np)\n",
    "    precision = precision_score(y_test, y_pred_np)\n",
    "    recall = recall_score(y_test, y_pred_np)\n",
    "    f1 = f1_score(y_test, y_pred_np)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    del X_train_np, X_test_np, y_pred_proba, y_pred\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    clear_memory()\n",
    "    return model, map_score, accuracy, precision, recall, f1\n",
    "\n",
    "def train_and_evaluate_lstm_tf(model, X_train, X_test, y_train, y_test, model_name, fold, batch_size, num_epochs):\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "    model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, precision, recall, f1, map_score\n",
    "\n",
    "def OneDCNN_model(input_len, num_classes):\n",
    "    hidden_dim = 128\n",
    "    num_filters = 32\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(input_len,))\n",
    "    x = Embedding(input_dim=36, output_dim=hidden_dim, input_length=input_len, mask_zero=True)(inputs)\n",
    "    x = Conv1D(filters=num_filters, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=num_filters*2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=num_filters*3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss = 'binary_crossentropy'\n",
    "    weighted_metrics = [AUC(curve='PR', name='avg_precision')]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn_tf(model, X_train, X_test, y_train, y_test, model_name, fold, batch_size, num_epochs):\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "    es = EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n",
    "    checkpoint = ModelCheckpoint(monitor='val_loss', filepath=f\"{model_name}.h5\",\n",
    "                                 save_best_only=True, save_weights_only=True, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, verbose=1)\n",
    "\n",
    "    model.fit(X_train_tensor, y_train_tensor, validation_data=(X_test_tensor, y_test_tensor),\n",
    "              epochs=num_epochs, callbacks=[checkpoint, reduce_lr_loss, es],\n",
    "              batch_size=batch_size, verbose=1)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, precision, recall, f1, map_score\n",
    "\n",
    "def get_model(model_type, input_len=None, num_classes=None):\n",
    "    if model_type == 'random_forest':\n",
    "        return RandomForestClassifier(**rf_params)\n",
    "    elif model_type == 'xgboost':\n",
    "        return XGBClassifier(**xgb_params)\n",
    "    elif model_type == 'catboost':\n",
    "        return CatBoostClassifier(**catboost_params)\n",
    "    elif model_type == 'cnn':\n",
    "        return OneDCNN_model(input_len, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "def monitor_memory(threshold=0.80):\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    return memory_info.percent / 100 >= threshold\n",
    "\n",
    "def process_protein_batches(protein, model_types, input_parquet_path, num_folds):\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "    print(total_rows)\n",
    "    batch_size = 100000\n",
    "\n",
    "    for offset in range(0, total_rows, batch_size):\n",
    "        filtered_df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "        WHERE protein_name = '{protein}'\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\").df()\n",
    "\n",
    "        print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "\n",
    "        X = np.concatenate([\n",
    "            np.array(filtered_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "        y = filtered_df['binds'].tolist()\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "            process_model_type(model_type, skf, X, y, protein, offset)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def process_protein_batches_type2(protein, model_types, input_parquet_path, num_folds):\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "    print(total_rows)\n",
    "    batch_size = 99999\n",
    "\n",
    "    for offset in range(0, total_rows, batch_size):\n",
    "        filtered_df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\").df()\n",
    "\n",
    "        print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "        \n",
    "        binds_1 = filtered_df[filtered_df['bind'] == 1]\n",
    "        binds_0 = filtered_df[filtered_df['bind'] == 0].sample(n=len(binds_1) * 2, random_state=42)\n",
    "\n",
    "        sampled_df = pd.concat([binds_1, binds_0]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        X = np.array(sampled_df[[f'enc{i}' for i in range(142)]].tolist(), dtype=np.float32)\n",
    "        y = sampled_df['bind'].tolist()\n",
    "\n",
    "        # 명시적으로 메모리 해제\n",
    "        del filtered_df, binds_1, binds_0, sampled_df\n",
    "        clear_memory()\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "            process_model_type(model_type, skf, X, y, protein, offset)\n",
    "    con.close()\n",
    "\n",
    "def process_model_type(model_type, skf, X, y, protein, offset):\n",
    "    precision_records = []\n",
    "    best_map_score = 0\n",
    "    best_model = None\n",
    "    best_fold = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {fold + 1} for {protein} with {model_type.upper()} batch {offset}...\")\n",
    "\n",
    "        model = get_model(model_type)\n",
    "        model_filename = f\"{protein}_{model_type}_model_fold_{fold}.pkl\"\n",
    "\n",
    "        try:\n",
    "            model = joblib.load(model_filename)\n",
    "            print(f\"Loaded existing {model_type.upper()} model for {protein}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Initialized new {model_type.upper()} model for {protein}\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        if model_type in ['xgboost', 'catboost']:\n",
    "            model, map_score, accuracy, precision, recall, f1 = train_and_evaluate_model(model, X_train, X_val, y_train, y_val, f\"{protein} - {model_type.upper()}\", fold + 1, model_type)\n",
    "        elif model_type == 'random_forest':\n",
    "            model, map_score, accuracy, precision, recall, f1 = train_and_evaluate_model_rf(model, X_train, X_val, y_train, y_val, f\"{protein} - {model_type.upper()}\", fold + 1)\n",
    "\n",
    "        precision_records.append((protein, model_type, 'N/A', precision, recall, f1, map_score, offset, fold + 1))\n",
    "\n",
    "        if map_score > best_map_score:\n",
    "            best_map_score = map_score\n",
    "            best_model = model\n",
    "            best_fold = fold\n",
    "\n",
    "        if monitor_memory():\n",
    "            print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "            clear_memory()\n",
    "\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        best_model_filename = f\"{protein}_{model_type}_best_model.pkl\"\n",
    "        joblib.dump(best_model, best_model_filename)\n",
    "        print(f\"Best model saved: {best_model_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_gpu()\n",
    "    protein_names = ['BRD4', 'HSA', 'sEH']\n",
    "    model_types = ['catboost']\n",
    "    num_folds = 5\n",
    "    input_parquet_path = 'processed_merged_queried_data.parquet'\n",
    "\n",
    "    for protein in protein_names:\n",
    "        # process_protein_batches(protein, model_types, input_parquet_path, num_folds)\n",
    "        if model_types != \"transformer\":\n",
    "            process_protein_batches(protein, model_types, input_parquet_path, num_folds)\n",
    "        else:\n",
    "            input_parquet_path = f\"train_enc_{protein}.parquet\"\n",
    "            process_protein_batches_type2(protein, model_types, input_parquet_path, num_folds)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train & catboost & 1DCNN 학습모델 & \n",
    "fold 개선& 모듈화 - 240702실행 성공 + single fold transformer -> 실패\n",
    "lightGBM 추가 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "1521460\n",
      "Processing BRD4 batch starting at offset 0...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 0...\n",
      "Initialized new LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24015, number of negative: 55985\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 39 dense feature groups (3.05 MB) transferred to GPU in 0.006602 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300187 -> initscore=-0.846405\n",
      "[LightGBM] [Info] Start training from score -0.846405\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94730\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89788\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86274\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.87996\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 0...\n",
      "Initialized new LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24015, number of negative: 55985\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.002844 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300187 -> initscore=-0.846405\n",
      "[LightGBM] [Info] Start training from score -0.846405\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95221\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89933\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86757\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88316\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 0...\n",
      "Initialized new LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24014, number of negative: 55986\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.006551 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300175 -> initscore=-0.846465\n",
      "[LightGBM] [Info] Start training from score -0.846465\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.95051\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90203\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86492\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88309\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 0...\n",
      "Initialized new LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24014, number of negative: 55986\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.011668 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300175 -> initscore=-0.846465\n",
      "[LightGBM] [Info] Start training from score -0.846465\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94800\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89574\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86426\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.87972\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 0...\n",
      "Initialized new LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24014, number of negative: 55986\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (1.53 MB) transferred to GPU in 0.003557 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300175 -> initscore=-0.846465\n",
      "[LightGBM] [Info] Start training from score -0.846465\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94850\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.90037\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86093\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88020\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 100000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23984, number of negative: 56016\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (3.05 MB) transferred to GPU in 0.010432 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299800 -> initscore=-0.848250\n",
      "[LightGBM] [Info] Start training from score -0.848250\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94790\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89926\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86656\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88260\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23983, number of negative: 56017\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.003443 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299787 -> initscore=-0.848310\n",
      "[LightGBM] [Info] Start training from score -0.848310\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94846\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89516\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86007\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.87726\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23983, number of negative: 56017\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.007687 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299787 -> initscore=-0.848310\n",
      "[LightGBM] [Info] Start training from score -0.848310\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.95308\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90291\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86541\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88376\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23983, number of negative: 56017\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.003546 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299787 -> initscore=-0.848310\n",
      "[LightGBM] [Info] Start training from score -0.848310\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94919\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89950\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86274\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88074\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23983, number of negative: 56017\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.006633 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299787 -> initscore=-0.848310\n",
      "[LightGBM] [Info] Start training from score -0.848310\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.95072\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89917\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86708\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88283\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 200000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24048, number of negative: 55952\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.003692 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300600 -> initscore=-0.844442\n",
      "[LightGBM] [Info] Start training from score -0.844442\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.95065\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.90014\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86361\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88149\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24048, number of negative: 55952\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.003752 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300600 -> initscore=-0.844442\n",
      "[LightGBM] [Info] Start training from score -0.844442\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94971\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89874\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86660\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88238\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24048, number of negative: 55952\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7316\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3658\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (1.53 MB) transferred to GPU in 0.003442 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300600 -> initscore=-0.844442\n",
      "[LightGBM] [Info] Start training from score -0.844442\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94662\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89632\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86277\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87923\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24048, number of negative: 55952\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.006886 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300600 -> initscore=-0.844442\n",
      "[LightGBM] [Info] Start training from score -0.844442\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.95287\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.90206\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86710\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88423\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24048, number of negative: 55952\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.003449 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300600 -> initscore=-0.844442\n",
      "[LightGBM] [Info] Start training from score -0.844442\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94446\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89187\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86294\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.87717\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 300000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23921, number of negative: 56079\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005424 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299013 -> initscore=-0.852005\n",
      "[LightGBM] [Info] Start training from score -0.852005\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94988\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.90339\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86472\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88363\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23921, number of negative: 56079\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.003885 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299013 -> initscore=-0.852005\n",
      "[LightGBM] [Info] Start training from score -0.852005\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95105\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89885\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86488\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88154\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23921, number of negative: 56079\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.007712 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299013 -> initscore=-0.852005\n",
      "[LightGBM] [Info] Start training from score -0.852005\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94685\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89981\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86054\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87973\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23921, number of negative: 56079\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.005615 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299013 -> initscore=-0.852005\n",
      "[LightGBM] [Info] Start training from score -0.852005\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94811\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89830\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.87592\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88697\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23920, number of negative: 56080\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.010197 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.299000 -> initscore=-0.852064\n",
      "[LightGBM] [Info] Start training from score -0.852064\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94853\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89937\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86223\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88041\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 400000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24176, number of negative: 55824\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (1.53 MB) transferred to GPU in 0.003912 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302200 -> initscore=-0.836843\n",
      "[LightGBM] [Info] Start training from score -0.836843\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.95153\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89862\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86962\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88388\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24176, number of negative: 55824\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (1.53 MB) transferred to GPU in 0.003299 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302200 -> initscore=-0.836843\n",
      "[LightGBM] [Info] Start training from score -0.836843\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95230\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.90659\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86549\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88556\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24176, number of negative: 55824\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.011364 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302200 -> initscore=-0.836843\n",
      "[LightGBM] [Info] Start training from score -0.836843\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94936\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89716\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86168\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87906\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24176, number of negative: 55824\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.006451 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302200 -> initscore=-0.836843\n",
      "[LightGBM] [Info] Start training from score -0.836843\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.95212\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.90014\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86797\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88376\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24176, number of negative: 55824\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.008406 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302200 -> initscore=-0.836843\n",
      "[LightGBM] [Info] Start training from score -0.836843\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.95019\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89744\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86433\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88057\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 500000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24152, number of negative: 55848\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004148 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301900 -> initscore=-0.838267\n",
      "[LightGBM] [Info] Start training from score -0.838267\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94840\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89668\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86682\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88150\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24151, number of negative: 55849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (1.22 MB) transferred to GPU in 0.003341 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301887 -> initscore=-0.838326\n",
      "[LightGBM] [Info] Start training from score -0.838326\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95058\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.90152\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86568\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88324\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24151, number of negative: 55849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.005285 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301887 -> initscore=-0.838326\n",
      "[LightGBM] [Info] Start training from score -0.838326\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.95107\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90057\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86105\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88037\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24151, number of negative: 55849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.006134 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301887 -> initscore=-0.838326\n",
      "[LightGBM] [Info] Start training from score -0.838326\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94575\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89524\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86337\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.87902\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24151, number of negative: 55849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.003518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301887 -> initscore=-0.838326\n",
      "[LightGBM] [Info] Start training from score -0.838326\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.95072\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89677\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86469\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88044\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 600000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 600000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24121, number of negative: 55879\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.008275 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301513 -> initscore=-0.840106\n",
      "[LightGBM] [Info] Start training from score -0.840106\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94947\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89761\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.85920\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.87799\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 600000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24121, number of negative: 55879\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005837 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301513 -> initscore=-0.840106\n",
      "[LightGBM] [Info] Start training from score -0.840106\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95028\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.90182\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86368\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88234\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 600000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24121, number of negative: 55879\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.006914 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301513 -> initscore=-0.840106\n",
      "[LightGBM] [Info] Start training from score -0.840106\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94542\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89573\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.85904\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87700\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 600000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24121, number of negative: 55879\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7316\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3658\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.007303 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301513 -> initscore=-0.840106\n",
      "[LightGBM] [Info] Start training from score -0.840106\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.95058\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.90227\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86352\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88247\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 600000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24120, number of negative: 55880\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004273 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301500 -> initscore=-0.840165\n",
      "[LightGBM] [Info] Start training from score -0.840165\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94822\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89832\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86138\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.87947\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 700000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 700000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23856, number of negative: 56144\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.004150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298200 -> initscore=-0.855884\n",
      "[LightGBM] [Info] Start training from score -0.855884\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94810\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.90049\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.85594\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.87765\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 700000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23855, number of negative: 56145\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.010068 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298187 -> initscore=-0.855944\n",
      "[LightGBM] [Info] Start training from score -0.855944\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94615\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89586\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86402\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.87965\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 700000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23855, number of negative: 56145\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (1.53 MB) transferred to GPU in 0.005005 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298187 -> initscore=-0.855944\n",
      "[LightGBM] [Info] Start training from score -0.855944\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94920\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89825\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.85262\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87484\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 700000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23855, number of negative: 56145\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005294 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298187 -> initscore=-0.855944\n",
      "[LightGBM] [Info] Start training from score -0.855944\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94846\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89829\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86184\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.87969\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 700000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23855, number of negative: 56145\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004090 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298187 -> initscore=-0.855944\n",
      "[LightGBM] [Info] Start training from score -0.855944\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.95046\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89633\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86402\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.87988\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 800000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 800000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24053, number of negative: 55947\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (1.53 MB) transferred to GPU in 0.004940 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300662 -> initscore=-0.844145\n",
      "[LightGBM] [Info] Start training from score -0.844145\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94465\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89710\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.85816\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.87720\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 800000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24053, number of negative: 55947\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.005258 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300662 -> initscore=-0.844145\n",
      "[LightGBM] [Info] Start training from score -0.844145\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95086\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89806\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86132\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.87931\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 800000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24054, number of negative: 55946\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.006529 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300675 -> initscore=-0.844086\n",
      "[LightGBM] [Info] Start training from score -0.844086\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.95070\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90387\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.87095\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88710\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 800000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24054, number of negative: 55946\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004627 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300675 -> initscore=-0.844086\n",
      "[LightGBM] [Info] Start training from score -0.844086\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94769\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89930\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.85997\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.87920\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 800000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24054, number of negative: 55946\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.006765 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300675 -> initscore=-0.844086\n",
      "[LightGBM] [Info] Start training from score -0.844086\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94897\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89643\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86945\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88274\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 900000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 900000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24011, number of negative: 55989\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004492 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300138 -> initscore=-0.846643\n",
      "[LightGBM] [Info] Start training from score -0.846643\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94786\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89848\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86405\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88092\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 900000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24011, number of negative: 55989\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 40 dense feature groups (3.05 MB) transferred to GPU in 0.014288 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300138 -> initscore=-0.846643\n",
      "[LightGBM] [Info] Start training from score -0.846643\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94784\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89962\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86604\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88251\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 900000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24010, number of negative: 55990\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005327 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300125 -> initscore=-0.846703\n",
      "[LightGBM] [Info] Start training from score -0.846703\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94967\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90309\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.85690\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87939\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 900000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24010, number of negative: 55990\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.003942 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300125 -> initscore=-0.846703\n",
      "[LightGBM] [Info] Start training from score -0.846703\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94961\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89935\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.87073\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88481\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 900000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24010, number of negative: 55990\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.004535 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300125 -> initscore=-0.846703\n",
      "[LightGBM] [Info] Start training from score -0.846703\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94880\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89879\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86690\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88256\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 1000000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24193, number of negative: 55807\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.006401 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302412 -> initscore=-0.835836\n",
      "[LightGBM] [Info] Start training from score -0.835836\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94747\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89886\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86394\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88106\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24193, number of negative: 55807\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.009729 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302412 -> initscore=-0.835836\n",
      "[LightGBM] [Info] Start training from score -0.835836\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.95106\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.90365\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.87287\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88799\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24194, number of negative: 55806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (1.53 MB) transferred to GPU in 0.006768 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302425 -> initscore=-0.835777\n",
      "[LightGBM] [Info] Start training from score -0.835777\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94554\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89583\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86028\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87770\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24194, number of negative: 55806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.006672 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302425 -> initscore=-0.835777\n",
      "[LightGBM] [Info] Start training from score -0.835777\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94707\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.90099\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86062\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88034\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24194, number of negative: 55806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.005665 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302425 -> initscore=-0.835777\n",
      "[LightGBM] [Info] Start training from score -0.835777\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.95182\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.90244\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.87335\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88766\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 1100000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23909, number of negative: 56091\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004895 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298862 -> initscore=-0.852720\n",
      "[LightGBM] [Info] Start training from score -0.852720\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94676\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89277\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86364\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.87797\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23909, number of negative: 56091\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.006941 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298862 -> initscore=-0.852720\n",
      "[LightGBM] [Info] Start training from score -0.852720\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94926\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89915\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86214\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88025\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23909, number of negative: 56091\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.007018 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298862 -> initscore=-0.852720\n",
      "[LightGBM] [Info] Start training from score -0.852720\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94925\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.89602\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86933\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88247\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23909, number of negative: 56091\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.006321 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298862 -> initscore=-0.852720\n",
      "[LightGBM] [Info] Start training from score -0.852720\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94758\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89951\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86264\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88069\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23908, number of negative: 56092\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.011483 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298850 -> initscore=-0.852780\n",
      "[LightGBM] [Info] Start training from score -0.852780\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94736\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.90074\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.85915\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.87945\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 1200000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23888, number of negative: 56112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.006508 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298600 -> initscore=-0.853973\n",
      "[LightGBM] [Info] Start training from score -0.853973\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94614\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89492\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86975\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88215\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23889, number of negative: 56111\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.011847 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298613 -> initscore=-0.853914\n",
      "[LightGBM] [Info] Start training from score -0.853914\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94861\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89919\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.85884\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.87855\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23889, number of negative: 56111\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (3.05 MB) transferred to GPU in 0.019474 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298613 -> initscore=-0.853914\n",
      "[LightGBM] [Info] Start training from score -0.853914\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94971\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90211\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.85951\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88029\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23889, number of negative: 56111\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.007693 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298613 -> initscore=-0.853914\n",
      "[LightGBM] [Info] Start training from score -0.853914\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94976\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89910\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86537\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88191\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 23889, number of negative: 56111\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.005988 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298613 -> initscore=-0.853914\n",
      "[LightGBM] [Info] Start training from score -0.853914\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94680\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89976\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86119\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88005\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 1300000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24059, number of negative: 55941\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.006078 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300737 -> initscore=-0.843788\n",
      "[LightGBM] [Info] Start training from score -0.843788\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.95017\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89893\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.86664\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.88249\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24059, number of negative: 55941\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.005906 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300737 -> initscore=-0.843788\n",
      "[LightGBM] [Info] Start training from score -0.843788\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94991\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.90216\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86930\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.88543\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24058, number of negative: 55942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.013978 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300725 -> initscore=-0.843848\n",
      "[LightGBM] [Info] Start training from score -0.843848\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.95082\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90276\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86584\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88391\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24058, number of negative: 55942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004864 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300725 -> initscore=-0.843848\n",
      "[LightGBM] [Info] Start training from score -0.843848\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.95197\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89821\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.86700\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.88233\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24058, number of negative: 55942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.007402 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300725 -> initscore=-0.843848\n",
      "[LightGBM] [Info] Start training from score -0.843848\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.94752\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.90227\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.85952\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88037\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 1400000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24088, number of negative: 55912\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.010710 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301100 -> initscore=-0.842065\n",
      "[LightGBM] [Info] Start training from score -0.842065\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.94709\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.90255\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.85804\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.87973\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24089, number of negative: 55911\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (3.05 MB) transferred to GPU in 0.010790 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301113 -> initscore=-0.842006\n",
      "[LightGBM] [Info] Start training from score -0.842006\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94808\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.89551\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.86101\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.87792\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24089, number of negative: 55911\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.007724 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301113 -> initscore=-0.842006\n",
      "[LightGBM] [Info] Start training from score -0.842006\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.95267\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90490\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.86433\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.88415\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24089, number of negative: 55911\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (2.44 MB) transferred to GPU in 0.010609 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301113 -> initscore=-0.842006\n",
      "[LightGBM] [Info] Start training from score -0.842006\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.94888\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.89249\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.85603\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.87388\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 24089, number of negative: 55911\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.301113 -> initscore=-0.842006\n",
      "[LightGBM] [Info] Start training from score -0.842006\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.95073\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.90178\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.86599\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.88352\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "Processing BRD4 batch starting at offset 1500000...\n",
      "Processing BRD4 with LGBM...\n",
      "Fold 1 for BRD4 with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 5100, number of negative: 12068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7178\n",
      "[LightGBM] [Info] Number of data points in the train set: 17168, number of used features: 3589\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (0.33 MB) transferred to GPU in 0.003175 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.297064 -> initscore=-0.861317\n",
      "[LightGBM] [Info] Start training from score -0.861317\n",
      "BRD4 - LGBM - Fold 1 - Mean Average Precision (mAP): 0.93928\n",
      "BRD4 - LGBM - Fold 1 - Precision: 0.89103\n",
      "BRD4 - LGBM - Fold 1 - Recall: 0.83438\n",
      "BRD4 - LGBM - Fold 1 - F1 Score: 0.86178\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_0.pkl\n",
      "Fold 2 for BRD4 with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 5099, number of negative: 12069\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7192\n",
      "[LightGBM] [Info] Number of data points in the train set: 17168, number of used features: 3596\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (0.33 MB) transferred to GPU in 0.005020 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.297006 -> initscore=-0.861596\n",
      "[LightGBM] [Info] Start training from score -0.861596\n",
      "BRD4 - LGBM - Fold 2 - Mean Average Precision (mAP): 0.94201\n",
      "BRD4 - LGBM - Fold 2 - Precision: 0.90144\n",
      "BRD4 - LGBM - Fold 2 - Recall: 0.83216\n",
      "BRD4 - LGBM - Fold 2 - F1 Score: 0.86542\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_1.pkl\n",
      "Fold 3 for BRD4 with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 5099, number of negative: 12069\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7174\n",
      "[LightGBM] [Info] Number of data points in the train set: 17168, number of used features: 3587\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (0.65 MB) transferred to GPU in 0.008189 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.297006 -> initscore=-0.861596\n",
      "[LightGBM] [Info] Start training from score -0.861596\n",
      "BRD4 - LGBM - Fold 3 - Mean Average Precision (mAP): 0.94839\n",
      "BRD4 - LGBM - Fold 3 - Precision: 0.90157\n",
      "BRD4 - LGBM - Fold 3 - Recall: 0.85490\n",
      "BRD4 - LGBM - Fold 3 - F1 Score: 0.87762\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_2.pkl\n",
      "Fold 4 for BRD4 with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 5099, number of negative: 12069\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7184\n",
      "[LightGBM] [Info] Number of data points in the train set: 17168, number of used features: 3592\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (0.59 MB) transferred to GPU in 0.007510 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.297006 -> initscore=-0.861596\n",
      "[LightGBM] [Info] Start training from score -0.861596\n",
      "BRD4 - LGBM - Fold 4 - Mean Average Precision (mAP): 0.93723\n",
      "BRD4 - LGBM - Fold 4 - Precision: 0.90017\n",
      "BRD4 - LGBM - Fold 4 - Recall: 0.84863\n",
      "BRD4 - LGBM - Fold 4 - F1 Score: 0.87364\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_3.pkl\n",
      "Fold 5 for BRD4 with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for BRD4\n",
      "[LightGBM] [Info] Number of positive: 5099, number of negative: 12069\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7174\n",
      "[LightGBM] [Info] Number of data points in the train set: 17168, number of used features: 3587\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (0.33 MB) transferred to GPU in 0.004261 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.297006 -> initscore=-0.861596\n",
      "[LightGBM] [Info] Start training from score -0.861596\n",
      "BRD4 - LGBM - Fold 5 - Mean Average Precision (mAP): 0.92831\n",
      "BRD4 - LGBM - Fold 5 - Precision: 0.89016\n",
      "BRD4 - LGBM - Fold 5 - Recall: 0.85176\n",
      "BRD4 - LGBM - Fold 5 - F1 Score: 0.87054\n",
      "Memory cleared.\n",
      "Model saved: BRD4_lgbm_model_fold_4.pkl\n",
      "Best model saved: BRD4_lgbm_best_model.pkl\n",
      "1474768\n",
      "Processing HSA batch starting at offset 0...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 0...\n",
      "Initialized new LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22052, number of negative: 57948\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.007486 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275650 -> initscore=-0.966143\n",
      "[LightGBM] [Info] Start training from score -0.966143\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.89042\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86344\n",
      "HSA - LGBM - Fold 1 - Recall: 0.73875\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79625\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 0...\n",
      "Initialized new LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22051, number of negative: 57949\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.012280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275637 -> initscore=-0.966205\n",
      "[LightGBM] [Info] Start training from score -0.966205\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88691\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86411\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74515\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.80023\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 0...\n",
      "Initialized new LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22051, number of negative: 57949\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.014149 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275637 -> initscore=-0.966205\n",
      "[LightGBM] [Info] Start training from score -0.966205\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88703\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86908\n",
      "HSA - LGBM - Fold 3 - Recall: 0.73934\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79898\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 0...\n",
      "Initialized new LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22051, number of negative: 57949\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.012187 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275637 -> initscore=-0.966205\n",
      "[LightGBM] [Info] Start training from score -0.966205\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.87897\n",
      "HSA - LGBM - Fold 4 - Precision: 0.85876\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73118\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.78985\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 0...\n",
      "Initialized new LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22051, number of negative: 57949\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.005802 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275637 -> initscore=-0.966205\n",
      "[LightGBM] [Info] Start training from score -0.966205\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.89177\n",
      "HSA - LGBM - Fold 5 - Precision: 0.87188\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74061\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.80090\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 100000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22303, number of negative: 57697\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004938 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278788 -> initscore=-0.950484\n",
      "[LightGBM] [Info] Start training from score -0.950484\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88856\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86745\n",
      "HSA - LGBM - Fold 1 - Recall: 0.74422\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.80112\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22303, number of negative: 57697\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.006991 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278788 -> initscore=-0.950484\n",
      "[LightGBM] [Info] Start training from score -0.950484\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88460\n",
      "HSA - LGBM - Fold 2 - Precision: 0.85741\n",
      "HSA - LGBM - Fold 2 - Recall: 0.73345\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79060\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22302, number of negative: 57698\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.012466 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278775 -> initscore=-0.950546\n",
      "[LightGBM] [Info] Start training from score -0.950546\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88818\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86902\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74247\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.80077\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22302, number of negative: 57698\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.007040 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278775 -> initscore=-0.950546\n",
      "[LightGBM] [Info] Start training from score -0.950546\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88829\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86262\n",
      "HSA - LGBM - Fold 4 - Recall: 0.74659\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.80042\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22302, number of negative: 57698\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.007859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278775 -> initscore=-0.950546\n",
      "[LightGBM] [Info] Start training from score -0.950546\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88564\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86168\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73960\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79599\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 200000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22093, number of negative: 57907\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.276162 -> initscore=-0.963577\n",
      "[LightGBM] [Info] Start training from score -0.963577\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.89100\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86155\n",
      "HSA - LGBM - Fold 1 - Recall: 0.75376\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.80406\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22093, number of negative: 57907\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.008281 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.276162 -> initscore=-0.963577\n",
      "[LightGBM] [Info] Start training from score -0.963577\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88794\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86112\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74434\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79848\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22093, number of negative: 57907\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.012261 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.276162 -> initscore=-0.963577\n",
      "[LightGBM] [Info] Start training from score -0.963577\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.89327\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86607\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74814\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.80280\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22093, number of negative: 57907\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.013786 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.276162 -> initscore=-0.963577\n",
      "[LightGBM] [Info] Start training from score -0.963577\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88903\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86770\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73982\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79867\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22092, number of negative: 57908\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.009636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.276150 -> initscore=-0.963640\n",
      "[LightGBM] [Info] Start training from score -0.963640\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.87901\n",
      "HSA - LGBM - Fold 5 - Precision: 0.85281\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73841\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79150\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 300000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22247, number of negative: 57753\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.012330 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278087 -> initscore=-0.953968\n",
      "[LightGBM] [Info] Start training from score -0.953968\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88381\n",
      "HSA - LGBM - Fold 1 - Precision: 0.85277\n",
      "HSA - LGBM - Fold 1 - Recall: 0.74159\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79331\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22247, number of negative: 57753\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (3.05 MB) transferred to GPU in 0.013523 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278087 -> initscore=-0.953968\n",
      "[LightGBM] [Info] Start training from score -0.953968\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.89395\n",
      "HSA - LGBM - Fold 2 - Precision: 0.87210\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74303\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.80241\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22246, number of negative: 57754\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.006477 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278075 -> initscore=-0.954030\n",
      "[LightGBM] [Info] Start training from score -0.954030\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88237\n",
      "HSA - LGBM - Fold 3 - Precision: 0.85652\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74272\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79557\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22246, number of negative: 57754\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.006594 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278075 -> initscore=-0.954030\n",
      "[LightGBM] [Info] Start training from score -0.954030\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88499\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86238\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73571\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79402\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22246, number of negative: 57754\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.005712 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278075 -> initscore=-0.954030\n",
      "[LightGBM] [Info] Start training from score -0.954030\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88935\n",
      "HSA - LGBM - Fold 5 - Precision: 0.85934\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74254\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79668\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 400000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22309, number of negative: 57691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.007103 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278863 -> initscore=-0.950111\n",
      "[LightGBM] [Info] Start training from score -0.950111\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88127\n",
      "HSA - LGBM - Fold 1 - Precision: 0.85723\n",
      "HSA - LGBM - Fold 1 - Recall: 0.74287\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79597\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22309, number of negative: 57691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.007254 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278863 -> initscore=-0.950111\n",
      "[LightGBM] [Info] Start training from score -0.950111\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88805\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86646\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74574\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.80158\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22309, number of negative: 57691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.008554 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278863 -> initscore=-0.950111\n",
      "[LightGBM] [Info] Start training from score -0.950111\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88873\n",
      "HSA - LGBM - Fold 3 - Precision: 0.85502\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74233\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79470\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22309, number of negative: 57691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008582 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278863 -> initscore=-0.950111\n",
      "[LightGBM] [Info] Start training from score -0.950111\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88613\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86301\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73875\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79606\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22308, number of negative: 57692\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004982 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278850 -> initscore=-0.950173\n",
      "[LightGBM] [Info] Start training from score -0.950173\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88946\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86788\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73718\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79721\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 500000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 500000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22240, number of negative: 57760\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.006642 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278000 -> initscore=-0.954404\n",
      "[LightGBM] [Info] Start training from score -0.954404\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.89185\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86206\n",
      "HSA - LGBM - Fold 1 - Recall: 0.74762\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.80077\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 500000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22239, number of negative: 57761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005311 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277987 -> initscore=-0.954466\n",
      "[LightGBM] [Info] Start training from score -0.954466\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.89251\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86223\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74964\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.80200\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 500000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22239, number of negative: 57761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.003743 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277987 -> initscore=-0.954466\n",
      "[LightGBM] [Info] Start training from score -0.954466\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.89161\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86927\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74388\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.80171\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 500000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22239, number of negative: 57761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004982 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277987 -> initscore=-0.954466\n",
      "[LightGBM] [Info] Start training from score -0.954466\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88589\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86262\n",
      "HSA - LGBM - Fold 4 - Recall: 0.74083\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79710\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 500000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22239, number of negative: 57761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (1.53 MB) transferred to GPU in 0.005490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277987 -> initscore=-0.954466\n",
      "[LightGBM] [Info] Start training from score -0.954466\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88822\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86185\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74388\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79853\n",
      "Memory cleared.\n",
      "Memory usage exceeded threshold. Clearing memory...\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 600000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 600000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22044, number of negative: 57956\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004788 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275550 -> initscore=-0.966644\n",
      "[LightGBM] [Info] Start training from score -0.966644\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.89011\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86888\n",
      "HSA - LGBM - Fold 1 - Recall: 0.73471\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79619\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 600000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22044, number of negative: 57956\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004069 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275550 -> initscore=-0.966644\n",
      "[LightGBM] [Info] Start training from score -0.966644\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.89172\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86488\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74451\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.80020\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 600000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22044, number of negative: 57956\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004047 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275550 -> initscore=-0.966644\n",
      "[LightGBM] [Info] Start training from score -0.966644\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88457\n",
      "HSA - LGBM - Fold 3 - Precision: 0.85388\n",
      "HSA - LGBM - Fold 3 - Recall: 0.73907\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79234\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 600000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22044, number of negative: 57956\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004642 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275550 -> initscore=-0.966644\n",
      "[LightGBM] [Info] Start training from score -0.966644\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88735\n",
      "HSA - LGBM - Fold 4 - Precision: 0.85936\n",
      "HSA - LGBM - Fold 4 - Recall: 0.74288\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79689\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 600000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22044, number of negative: 57956\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (1.53 MB) transferred to GPU in 0.004794 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275550 -> initscore=-0.966644\n",
      "[LightGBM] [Info] Start training from score -0.966644\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88535\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86805\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73653\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79690\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 700000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 700000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21956, number of negative: 58044\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004772 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274450 -> initscore=-0.972161\n",
      "[LightGBM] [Info] Start training from score -0.972161\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88576\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86082\n",
      "HSA - LGBM - Fold 1 - Recall: 0.73802\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79470\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 700000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21956, number of negative: 58044\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008228 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274450 -> initscore=-0.972161\n",
      "[LightGBM] [Info] Start training from score -0.972161\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88627\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86476\n",
      "HSA - LGBM - Fold 2 - Recall: 0.73274\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79329\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 700000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21956, number of negative: 58044\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.008811 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274450 -> initscore=-0.972161\n",
      "[LightGBM] [Info] Start training from score -0.972161\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.89494\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86679\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74567\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.80168\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 700000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21956, number of negative: 58044\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.009649 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274450 -> initscore=-0.972161\n",
      "[LightGBM] [Info] Start training from score -0.972161\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88245\n",
      "HSA - LGBM - Fold 4 - Precision: 0.85303\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73911\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79200\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 700000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21956, number of negative: 58044\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.009112 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274450 -> initscore=-0.972161\n",
      "[LightGBM] [Info] Start training from score -0.972161\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.89275\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86728\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74167\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79957\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 800000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 800000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22034, number of negative: 57966\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.007507 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275425 -> initscore=-0.967270\n",
      "[LightGBM] [Info] Start training from score -0.967270\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88440\n",
      "HSA - LGBM - Fold 1 - Precision: 0.85285\n",
      "HSA - LGBM - Fold 1 - Recall: 0.74170\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79340\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 800000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22034, number of negative: 57966\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.005064 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275425 -> initscore=-0.967270\n",
      "[LightGBM] [Info] Start training from score -0.967270\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88633\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86356\n",
      "HSA - LGBM - Fold 2 - Recall: 0.73643\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79494\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 800000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22034, number of negative: 57966\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.008652 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275425 -> initscore=-0.967270\n",
      "[LightGBM] [Info] Start training from score -0.967270\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.89352\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86877\n",
      "HSA - LGBM - Fold 3 - Recall: 0.75349\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.80704\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 800000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22035, number of negative: 57965\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (1.22 MB) transferred to GPU in 0.003524 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275438 -> initscore=-0.967207\n",
      "[LightGBM] [Info] Start training from score -0.967207\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88272\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86411\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73312\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79324\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 800000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22035, number of negative: 57965\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009554 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.275438 -> initscore=-0.967207\n",
      "[LightGBM] [Info] Start training from score -0.967207\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88760\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86540\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74237\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79918\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 900000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 900000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22172, number of negative: 57828\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004447 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277150 -> initscore=-0.958643\n",
      "[LightGBM] [Info] Start training from score -0.958643\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.87774\n",
      "HSA - LGBM - Fold 1 - Precision: 0.85845\n",
      "HSA - LGBM - Fold 1 - Recall: 0.72975\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.78888\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 900000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22172, number of negative: 57828\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004387 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277150 -> initscore=-0.958643\n",
      "[LightGBM] [Info] Start training from score -0.958643\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.89101\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86521\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74346\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79973\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 900000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22172, number of negative: 57828\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004724 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277150 -> initscore=-0.958643\n",
      "[LightGBM] [Info] Start training from score -0.958643\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.89316\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86403\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74400\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79953\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 900000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22172, number of negative: 57828\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.003886 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277150 -> initscore=-0.958643\n",
      "[LightGBM] [Info] Start training from score -0.958643\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88713\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86624\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73841\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79723\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 900000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22172, number of negative: 57828\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (1.53 MB) transferred to GPU in 0.006407 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.277150 -> initscore=-0.958643\n",
      "[LightGBM] [Info] Start training from score -0.958643\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88975\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86236\n",
      "HSA - LGBM - Fold 5 - Recall: 0.75392\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.80450\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 1000000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22314, number of negative: 57686\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.009931 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278925 -> initscore=-0.949800\n",
      "[LightGBM] [Info] Start training from score -0.949800\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88505\n",
      "HSA - LGBM - Fold 1 - Precision: 0.85805\n",
      "HSA - LGBM - Fold 1 - Recall: 0.75421\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.80279\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22314, number of negative: 57686\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009825 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278925 -> initscore=-0.949800\n",
      "[LightGBM] [Info] Start training from score -0.949800\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88818\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86203\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74148\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79722\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22314, number of negative: 57686\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.005656 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278925 -> initscore=-0.949800\n",
      "[LightGBM] [Info] Start training from score -0.949800\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88501\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86048\n",
      "HSA - LGBM - Fold 3 - Recall: 0.72643\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.78779\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22313, number of negative: 57687\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.004437 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278913 -> initscore=-0.949862\n",
      "[LightGBM] [Info] Start training from score -0.949862\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.89605\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86636\n",
      "HSA - LGBM - Fold 4 - Recall: 0.75300\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.80572\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22313, number of negative: 57687\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.004673 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278913 -> initscore=-0.949862\n",
      "[LightGBM] [Info] Start training from score -0.949862\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88876\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86593\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74207\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79923\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 1100000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21980, number of negative: 58020\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.004312 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274750 -> initscore=-0.970655\n",
      "[LightGBM] [Info] Start training from score -0.970655\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.89050\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86973\n",
      "HSA - LGBM - Fold 1 - Recall: 0.73139\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79458\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21980, number of negative: 58020\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.005229 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274750 -> initscore=-0.970655\n",
      "[LightGBM] [Info] Start training from score -0.970655\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88732\n",
      "HSA - LGBM - Fold 2 - Precision: 0.85814\n",
      "HSA - LGBM - Fold 2 - Recall: 0.73976\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79457\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21980, number of negative: 58020\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004095 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274750 -> initscore=-0.970655\n",
      "[LightGBM] [Info] Start training from score -0.970655\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88020\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86049\n",
      "HSA - LGBM - Fold 3 - Recall: 0.73412\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79230\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21980, number of negative: 58020\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.005237 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274750 -> initscore=-0.970655\n",
      "[LightGBM] [Info] Start training from score -0.970655\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88787\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86044\n",
      "HSA - LGBM - Fold 4 - Recall: 0.73940\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79534\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 21980, number of negative: 58020\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (1.53 MB) transferred to GPU in 0.005337 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274750 -> initscore=-0.970655\n",
      "[LightGBM] [Info] Start training from score -0.970655\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88224\n",
      "HSA - LGBM - Fold 5 - Precision: 0.85748\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73139\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.78943\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 1200000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22296, number of negative: 57704\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004924 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278700 -> initscore=-0.950919\n",
      "[LightGBM] [Info] Start training from score -0.950919\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88914\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86334\n",
      "HSA - LGBM - Fold 1 - Recall: 0.74251\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79838\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22295, number of negative: 57705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.010505 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278687 -> initscore=-0.950981\n",
      "[LightGBM] [Info] Start training from score -0.950981\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88877\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86755\n",
      "HSA - LGBM - Fold 2 - Recall: 0.73915\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79822\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22295, number of negative: 57705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.005983 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278687 -> initscore=-0.950981\n",
      "[LightGBM] [Info] Start training from score -0.950981\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88773\n",
      "HSA - LGBM - Fold 3 - Precision: 0.85593\n",
      "HSA - LGBM - Fold 3 - Recall: 0.73861\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79295\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22295, number of negative: 57705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.009693 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278687 -> initscore=-0.950981\n",
      "[LightGBM] [Info] Start training from score -0.950981\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.89261\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86772\n",
      "HSA - LGBM - Fold 4 - Recall: 0.75081\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.80504\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22295, number of negative: 57705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.010488 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278687 -> initscore=-0.950981\n",
      "[LightGBM] [Info] Start training from score -0.950981\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.89058\n",
      "HSA - LGBM - Fold 5 - Precision: 0.87130\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73968\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.80012\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 1300000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22290, number of negative: 57710\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.010248 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278625 -> initscore=-0.951292\n",
      "[LightGBM] [Info] Start training from score -0.951292\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88287\n",
      "HSA - LGBM - Fold 1 - Precision: 0.85759\n",
      "HSA - LGBM - Fold 1 - Recall: 0.73695\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79270\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22290, number of negative: 57710\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005775 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278625 -> initscore=-0.951292\n",
      "[LightGBM] [Info] Start training from score -0.951292\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88528\n",
      "HSA - LGBM - Fold 2 - Precision: 0.85567\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74359\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.79570\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22290, number of negative: 57710\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.006323 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278625 -> initscore=-0.951292\n",
      "[LightGBM] [Info] Start training from score -0.951292\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88796\n",
      "HSA - LGBM - Fold 3 - Precision: 0.86420\n",
      "HSA - LGBM - Fold 3 - Recall: 0.74682\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.80123\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22291, number of negative: 57709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.004866 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278637 -> initscore=-0.951230\n",
      "[LightGBM] [Info] Start training from score -0.951230\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.89169\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86881\n",
      "HSA - LGBM - Fold 4 - Recall: 0.74049\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79953\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 22291, number of negative: 57709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (1.22 MB) transferred to GPU in 0.003662 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.278637 -> initscore=-0.951230\n",
      "[LightGBM] [Info] Start training from score -0.951230\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88958\n",
      "HSA - LGBM - Fold 5 - Precision: 0.86087\n",
      "HSA - LGBM - Fold 5 - Recall: 0.74623\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79946\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "Processing HSA batch starting at offset 1400000...\n",
      "Processing HSA with LGBM...\n",
      "Fold 1 for HSA with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 16402, number of negative: 43412\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 59814, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (0.91 MB) transferred to GPU in 0.003679 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274217 -> initscore=-0.973333\n",
      "[LightGBM] [Info] Start training from score -0.973333\n",
      "HSA - LGBM - Fold 1 - Mean Average Precision (mAP): 0.88538\n",
      "HSA - LGBM - Fold 1 - Precision: 0.86845\n",
      "HSA - LGBM - Fold 1 - Recall: 0.72780\n",
      "HSA - LGBM - Fold 1 - F1 Score: 0.79193\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_0.pkl\n",
      "Fold 2 for HSA with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 16401, number of negative: 43413\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 59814, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.05 MB) transferred to GPU in 0.007913 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274200 -> initscore=-0.973417\n",
      "[LightGBM] [Info] Start training from score -0.973417\n",
      "HSA - LGBM - Fold 2 - Mean Average Precision (mAP): 0.88651\n",
      "HSA - LGBM - Fold 2 - Precision: 0.86510\n",
      "HSA - LGBM - Fold 2 - Recall: 0.74592\n",
      "HSA - LGBM - Fold 2 - F1 Score: 0.80110\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_1.pkl\n",
      "Fold 3 for HSA with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 16401, number of negative: 43413\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 59814, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.05 MB) transferred to GPU in 0.005933 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274200 -> initscore=-0.973417\n",
      "[LightGBM] [Info] Start training from score -0.973417\n",
      "HSA - LGBM - Fold 3 - Mean Average Precision (mAP): 0.88451\n",
      "HSA - LGBM - Fold 3 - Precision: 0.85436\n",
      "HSA - LGBM - Fold 3 - Recall: 0.73665\n",
      "HSA - LGBM - Fold 3 - F1 Score: 0.79115\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_2.pkl\n",
      "Fold 4 for HSA with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 16402, number of negative: 43413\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7318\n",
      "[LightGBM] [Info] Number of data points in the train set: 59815, number of used features: 3659\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (2.28 MB) transferred to GPU in 0.006794 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274212 -> initscore=-0.973356\n",
      "[LightGBM] [Info] Start training from score -0.973356\n",
      "HSA - LGBM - Fold 4 - Mean Average Precision (mAP): 0.88569\n",
      "HSA - LGBM - Fold 4 - Precision: 0.86917\n",
      "HSA - LGBM - Fold 4 - Recall: 0.74049\n",
      "HSA - LGBM - Fold 4 - F1 Score: 0.79968\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_3.pkl\n",
      "Fold 5 for HSA with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for HSA\n",
      "[LightGBM] [Info] Number of positive: 16402, number of negative: 43413\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7320\n",
      "[LightGBM] [Info] Number of data points in the train set: 59815, number of used features: 3660\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.14 MB) transferred to GPU in 0.005801 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.274212 -> initscore=-0.973356\n",
      "[LightGBM] [Info] Start training from score -0.973356\n",
      "HSA - LGBM - Fold 5 - Mean Average Precision (mAP): 0.88196\n",
      "HSA - LGBM - Fold 5 - Precision: 0.85853\n",
      "HSA - LGBM - Fold 5 - Recall: 0.73561\n",
      "HSA - LGBM - Fold 5 - F1 Score: 0.79233\n",
      "Memory cleared.\n",
      "Model saved: HSA_lgbm_model_fold_4.pkl\n",
      "Best model saved: HSA_lgbm_best_model.pkl\n",
      "1785555\n",
      "Processing sEH batch starting at offset 0...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 0...\n",
      "Initialized new LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32576, number of negative: 47424\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008801 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407200 -> initscore=-0.375553\n",
      "[LightGBM] [Info] Start training from score -0.375553\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98592\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95577\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94979\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95277\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 0...\n",
      "Initialized new LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32577, number of negative: 47423\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.009365 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407212 -> initscore=-0.375501\n",
      "[LightGBM] [Info] Start training from score -0.375501\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98771\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95836\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94953\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95393\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 0...\n",
      "Initialized new LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32577, number of negative: 47423\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008545 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407212 -> initscore=-0.375501\n",
      "[LightGBM] [Info] Start training from score -0.375501\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98833\n",
      "sEH - LGBM - Fold 3 - Precision: 0.96157\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94622\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95383\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 0...\n",
      "Initialized new LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32577, number of negative: 47423\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.011185 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407212 -> initscore=-0.375501\n",
      "[LightGBM] [Info] Start training from score -0.375501\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98641\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95804\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94192\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.94991\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 0...\n",
      "Initialized new LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32577, number of negative: 47423\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.007235 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407212 -> initscore=-0.375501\n",
      "[LightGBM] [Info] Start training from score -0.375501\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98715\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95772\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94843\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95305\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 100000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32384, number of negative: 47616\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.010861 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404800 -> initscore=-0.385504\n",
      "[LightGBM] [Info] Start training from score -0.385504\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98631\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95617\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94578\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95094\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32384, number of negative: 47616\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.008244 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404800 -> initscore=-0.385504\n",
      "[LightGBM] [Info] Start training from score -0.385504\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98609\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95463\n",
      "sEH - LGBM - Fold 2 - Recall: 0.95121\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95292\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32384, number of negative: 47616\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.006970 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404800 -> initscore=-0.385504\n",
      "[LightGBM] [Info] Start training from score -0.385504\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98728\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95590\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94516\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95050\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32384, number of negative: 47616\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.009159 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404800 -> initscore=-0.385504\n",
      "[LightGBM] [Info] Start training from score -0.385504\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98557\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95837\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94689\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95259\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32384, number of negative: 47616\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.007641 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404800 -> initscore=-0.385504\n",
      "[LightGBM] [Info] Start training from score -0.385504\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98703\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95835\n",
      "sEH - LGBM - Fold 5 - Recall: 0.95208\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95520\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 200000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32400, number of negative: 47600\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.011375 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405000 -> initscore=-0.384674\n",
      "[LightGBM] [Info] Start training from score -0.384674\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98589\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95967\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94000\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.94973\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32400, number of negative: 47600\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.007558 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405000 -> initscore=-0.384674\n",
      "[LightGBM] [Info] Start training from score -0.384674\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98502\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95682\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94383\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95028\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32400, number of negative: 47600\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.010327 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405000 -> initscore=-0.384674\n",
      "[LightGBM] [Info] Start training from score -0.384674\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98803\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95536\n",
      "sEH - LGBM - Fold 3 - Recall: 0.95123\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95329\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32400, number of negative: 47600\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004950 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405000 -> initscore=-0.384674\n",
      "[LightGBM] [Info] Start training from score -0.384674\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98799\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95693\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94630\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95158\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32400, number of negative: 47600\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.011000 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405000 -> initscore=-0.384674\n",
      "[LightGBM] [Info] Start training from score -0.384674\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98688\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95777\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94642\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95206\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 300000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32655, number of negative: 47345\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.007659 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408187 -> initscore=-0.371463\n",
      "[LightGBM] [Info] Start training from score -0.371463\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98867\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95638\n",
      "sEH - LGBM - Fold 1 - Recall: 0.95075\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95356\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32655, number of negative: 47345\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.009481 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408187 -> initscore=-0.371463\n",
      "[LightGBM] [Info] Start training from score -0.371463\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98594\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95677\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94622\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95147\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32654, number of negative: 47346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.011567 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408175 -> initscore=-0.371515\n",
      "[LightGBM] [Info] Start training from score -0.371515\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98634\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95742\n",
      "sEH - LGBM - Fold 3 - Recall: 0.95027\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95383\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32654, number of negative: 47346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.009614 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408175 -> initscore=-0.371515\n",
      "[LightGBM] [Info] Start training from score -0.371515\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98753\n",
      "sEH - LGBM - Fold 4 - Precision: 0.96130\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94623\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95370\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32654, number of negative: 47346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.011735 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408175 -> initscore=-0.371515\n",
      "[LightGBM] [Info] Start training from score -0.371515\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98505\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95507\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94512\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95007\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 400000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32487, number of negative: 47513\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.005982 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406087 -> initscore=-0.380163\n",
      "[LightGBM] [Info] Start training from score -0.380163\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98720\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95932\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94951\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95439\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32487, number of negative: 47513\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.009315 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406087 -> initscore=-0.380163\n",
      "[LightGBM] [Info] Start training from score -0.380163\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98730\n",
      "sEH - LGBM - Fold 2 - Precision: 0.96009\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94496\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95246\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32486, number of negative: 47514\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.009706 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406075 -> initscore=-0.380215\n",
      "[LightGBM] [Info] Start training from score -0.380215\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98502\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95899\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94447\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95168\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32486, number of negative: 47514\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.007308 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406075 -> initscore=-0.380215\n",
      "[LightGBM] [Info] Start training from score -0.380215\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98655\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95676\n",
      "sEH - LGBM - Fold 4 - Recall: 0.95087\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95381\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32486, number of negative: 47514\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.008845 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406075 -> initscore=-0.380215\n",
      "[LightGBM] [Info] Start training from score -0.380215\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98623\n",
      "sEH - LGBM - Fold 5 - Precision: 0.96046\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94804\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95421\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 500000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32432, number of negative: 47568\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009837 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405400 -> initscore=-0.383015\n",
      "[LightGBM] [Info] Start training from score -0.383015\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98512\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95898\n",
      "sEH - LGBM - Fold 1 - Recall: 0.95141\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95518\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32432, number of negative: 47568\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.013424 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405400 -> initscore=-0.383015\n",
      "[LightGBM] [Info] Start training from score -0.383015\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98821\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95710\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94660\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95182\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32432, number of negative: 47568\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.007050 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405400 -> initscore=-0.383015\n",
      "[LightGBM] [Info] Start training from score -0.383015\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98723\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95813\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94277\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95039\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32432, number of negative: 47568\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.007160 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405400 -> initscore=-0.383015\n",
      "[LightGBM] [Info] Start training from score -0.383015\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98792\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95888\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94327\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95101\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32432, number of negative: 47568\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008776 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405400 -> initscore=-0.383015\n",
      "[LightGBM] [Info] Start training from score -0.383015\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98785\n",
      "sEH - LGBM - Fold 5 - Precision: 0.96039\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94808\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95420\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 600000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32167, number of negative: 47833\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.009518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.402087 -> initscore=-0.396775\n",
      "[LightGBM] [Info] Start training from score -0.396775\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98829\n",
      "sEH - LGBM - Fold 1 - Precision: 0.96105\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94827\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95462\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32167, number of negative: 47833\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 39 dense feature groups (3.05 MB) transferred to GPU in 0.009309 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.402087 -> initscore=-0.396775\n",
      "[LightGBM] [Info] Start training from score -0.396775\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98946\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95960\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94827\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95390\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32166, number of negative: 47834\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.004909 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.402075 -> initscore=-0.396827\n",
      "[LightGBM] [Info] Start training from score -0.396827\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98634\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95791\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94529\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95156\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32166, number of negative: 47834\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008868 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.402075 -> initscore=-0.396827\n",
      "[LightGBM] [Info] Start training from score -0.396827\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98765\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95780\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94541\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95156\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32166, number of negative: 47834\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.008876 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.402075 -> initscore=-0.396827\n",
      "[LightGBM] [Info] Start training from score -0.396827\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98407\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95426\n",
      "sEH - LGBM - Fold 5 - Recall: 0.93919\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.94667\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 700000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32139, number of negative: 47861\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.009979 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.401738 -> initscore=-0.398231\n",
      "[LightGBM] [Info] Start training from score -0.398231\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98755\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95720\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94635\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95174\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32139, number of negative: 47861\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.006928 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.401738 -> initscore=-0.398231\n",
      "[LightGBM] [Info] Start training from score -0.398231\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98548\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95900\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94337\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95112\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32138, number of negative: 47862\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.008164 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.401725 -> initscore=-0.398283\n",
      "[LightGBM] [Info] Start training from score -0.398283\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98656\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95307\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94536\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.94920\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32138, number of negative: 47862\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.009146 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.401725 -> initscore=-0.398283\n",
      "[LightGBM] [Info] Start training from score -0.398283\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98819\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95930\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94760\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95342\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32138, number of negative: 47862\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 28 dense feature groups (1.22 MB) transferred to GPU in 0.004694 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.401725 -> initscore=-0.398283\n",
      "[LightGBM] [Info] Start training from score -0.398283\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98609\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95854\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94387\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95115\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 800000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 800000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32577, number of negative: 47423\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.012402 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407212 -> initscore=-0.375501\n",
      "[LightGBM] [Info] Start training from score -0.375501\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98748\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95332\n",
      "sEH - LGBM - Fold 1 - Recall: 0.95028\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95180\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 800000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32577, number of negative: 47423\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.012941 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407212 -> initscore=-0.375501\n",
      "[LightGBM] [Info] Start training from score -0.375501\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98759\n",
      "sEH - LGBM - Fold 2 - Precision: 0.96227\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94868\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95543\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 800000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32578, number of negative: 47422\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.007270 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407225 -> initscore=-0.375449\n",
      "[LightGBM] [Info] Start training from score -0.375449\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98693\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95596\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94610\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95100\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 800000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32578, number of negative: 47422\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.007991 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407225 -> initscore=-0.375449\n",
      "[LightGBM] [Info] Start training from score -0.375449\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98678\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95860\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94953\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95404\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 800000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32578, number of negative: 47422\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.008400 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407225 -> initscore=-0.375449\n",
      "[LightGBM] [Info] Start training from score -0.375449\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98607\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95888\n",
      "sEH - LGBM - Fold 5 - Recall: 0.95064\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95474\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 900000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 900000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32617, number of negative: 47383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.010991 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407712 -> initscore=-0.373430\n",
      "[LightGBM] [Info] Start training from score -0.373430\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98580\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95871\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94825\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95345\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 900000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32617, number of negative: 47383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.008959 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407712 -> initscore=-0.373430\n",
      "[LightGBM] [Info] Start training from score -0.373430\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98661\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95724\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94444\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95080\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 900000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32617, number of negative: 47383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.009998 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407712 -> initscore=-0.373430\n",
      "[LightGBM] [Info] Start training from score -0.373430\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98719\n",
      "sEH - LGBM - Fold 3 - Precision: 0.96000\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94481\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95235\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 900000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32617, number of negative: 47383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.009279 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407712 -> initscore=-0.373430\n",
      "[LightGBM] [Info] Start training from score -0.373430\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98567\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95289\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94260\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.94772\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 900000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32616, number of negative: 47384\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.009331 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.407700 -> initscore=-0.373482\n",
      "[LightGBM] [Info] Start training from score -0.373482\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98667\n",
      "sEH - LGBM - Fold 5 - Precision: 0.96044\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94666\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95350\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1000000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32543, number of negative: 47457\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.004566 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406788 -> initscore=-0.377262\n",
      "[LightGBM] [Info] Start training from score -0.377262\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98934\n",
      "sEH - LGBM - Fold 1 - Precision: 0.96094\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94948\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95518\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32543, number of negative: 47457\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.004401 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406788 -> initscore=-0.377262\n",
      "[LightGBM] [Info] Start training from score -0.377262\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98804\n",
      "sEH - LGBM - Fold 2 - Precision: 0.96030\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94838\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95430\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32543, number of negative: 47457\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.011623 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406788 -> initscore=-0.377262\n",
      "[LightGBM] [Info] Start training from score -0.377262\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98805\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95819\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94653\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95233\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32543, number of negative: 47457\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.011012 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406788 -> initscore=-0.377262\n",
      "[LightGBM] [Info] Start training from score -0.377262\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98710\n",
      "sEH - LGBM - Fold 4 - Precision: 0.96206\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94752\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95473\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1000000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32544, number of negative: 47456\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009286 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406800 -> initscore=-0.377210\n",
      "[LightGBM] [Info] Start training from score -0.377210\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98670\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95858\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94739\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95295\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1100000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32437, number of negative: 47563\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.009838 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405463 -> initscore=-0.382755\n",
      "[LightGBM] [Info] Start training from score -0.382755\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98774\n",
      "sEH - LGBM - Fold 1 - Precision: 0.96116\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94611\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95358\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32437, number of negative: 47563\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009311 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405463 -> initscore=-0.382755\n",
      "[LightGBM] [Info] Start training from score -0.382755\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98598\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95326\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94574\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.94949\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32437, number of negative: 47563\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.011132 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405463 -> initscore=-0.382755\n",
      "[LightGBM] [Info] Start training from score -0.382755\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98642\n",
      "sEH - LGBM - Fold 3 - Precision: 0.96034\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94648\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95336\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32437, number of negative: 47563\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 31 dense feature groups (2.44 MB) transferred to GPU in 0.007948 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405463 -> initscore=-0.382755\n",
      "[LightGBM] [Info] Start training from score -0.382755\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98662\n",
      "sEH - LGBM - Fold 4 - Precision: 0.96002\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94155\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95069\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1100000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32436, number of negative: 47564\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009711 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405450 -> initscore=-0.382807\n",
      "[LightGBM] [Info] Start training from score -0.382807\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98633\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95667\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94735\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95199\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1200000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32476, number of negative: 47524\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.008313 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405950 -> initscore=-0.380733\n",
      "[LightGBM] [Info] Start training from score -0.380733\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98583\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95416\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94358\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.94884\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32475, number of negative: 47525\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (1.22 MB) transferred to GPU in 0.006530 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405938 -> initscore=-0.380785\n",
      "[LightGBM] [Info] Start training from score -0.380785\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98673\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95747\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94544\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95141\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32475, number of negative: 47525\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.009897 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405938 -> initscore=-0.380785\n",
      "[LightGBM] [Info] Start training from score -0.380785\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98676\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95752\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94396\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95069\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32475, number of negative: 47525\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009085 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405938 -> initscore=-0.380785\n",
      "[LightGBM] [Info] Start training from score -0.380785\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98528\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95759\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94556\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95154\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1200000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32475, number of negative: 47525\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009117 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.405938 -> initscore=-0.380785\n",
      "[LightGBM] [Info] Start training from score -0.380785\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98760\n",
      "sEH - LGBM - Fold 5 - Precision: 0.96051\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94975\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95510\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1300000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32336, number of negative: 47664\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.75 MB) transferred to GPU in 0.010224 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404200 -> initscore=-0.387995\n",
      "[LightGBM] [Info] Start training from score -0.387995\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98791\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95694\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94557\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95122\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32336, number of negative: 47664\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.008447 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404200 -> initscore=-0.387995\n",
      "[LightGBM] [Info] Start training from score -0.387995\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98658\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95899\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94891\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95393\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32336, number of negative: 47664\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (1.53 MB) transferred to GPU in 0.006186 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404200 -> initscore=-0.387995\n",
      "[LightGBM] [Info] Start training from score -0.387995\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98607\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95339\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94631\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.94984\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32336, number of negative: 47664\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.013255 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404200 -> initscore=-0.387995\n",
      "[LightGBM] [Info] Start training from score -0.387995\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98638\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95803\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94582\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95188\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1300000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32336, number of negative: 47664\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.009795 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404200 -> initscore=-0.387995\n",
      "[LightGBM] [Info] Start training from score -0.387995\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98701\n",
      "sEH - LGBM - Fold 5 - Precision: 0.96061\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94433\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95240\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1400000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32765, number of negative: 47235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.012040 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.409562 -> initscore=-0.365774\n",
      "[LightGBM] [Info] Start training from score -0.365774\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98699\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95651\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94507\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95076\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32765, number of negative: 47235\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.007132 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.409562 -> initscore=-0.365774\n",
      "[LightGBM] [Info] Start training from score -0.365774\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98635\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95930\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94666\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95294\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32766, number of negative: 47234\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.010821 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.409575 -> initscore=-0.365723\n",
      "[LightGBM] [Info] Start training from score -0.365723\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98502\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95206\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94567\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.94886\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32766, number of negative: 47234\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 37 dense feature groups (3.05 MB) transferred to GPU in 0.011948 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.409575 -> initscore=-0.365723\n",
      "[LightGBM] [Info] Start training from score -0.365723\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98663\n",
      "sEH - LGBM - Fold 4 - Precision: 0.96037\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94958\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95494\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1400000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32766, number of negative: 47234\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.007438 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.409575 -> initscore=-0.365723\n",
      "[LightGBM] [Info] Start training from score -0.365723\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98798\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95862\n",
      "sEH - LGBM - Fold 5 - Recall: 0.95031\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95445\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1500000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32504, number of negative: 47496\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.007560 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406300 -> initscore=-0.379282\n",
      "[LightGBM] [Info] Start training from score -0.379282\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98547\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95826\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94351\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95083\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32504, number of negative: 47496\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.011485 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406300 -> initscore=-0.379282\n",
      "[LightGBM] [Info] Start training from score -0.379282\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98586\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95691\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94561\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95123\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32504, number of negative: 47496\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.007854 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406300 -> initscore=-0.379282\n",
      "[LightGBM] [Info] Start training from score -0.379282\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98464\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95865\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94438\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95146\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32504, number of negative: 47496\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7322\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3661\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.008459 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406300 -> initscore=-0.379282\n",
      "[LightGBM] [Info] Start training from score -0.379282\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98702\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95942\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94561\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95246\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1500000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32504, number of negative: 47496\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.75 MB) transferred to GPU in 0.011447 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406300 -> initscore=-0.379282\n",
      "[LightGBM] [Info] Start training from score -0.379282\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98723\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95357\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94782\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95069\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1600000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32339, number of negative: 47661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (1.53 MB) transferred to GPU in 0.005565 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404238 -> initscore=-0.387840\n",
      "[LightGBM] [Info] Start training from score -0.387840\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98604\n",
      "sEH - LGBM - Fold 1 - Precision: 0.96072\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94372\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.95214\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32339, number of negative: 47661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 33 dense feature groups (2.75 MB) transferred to GPU in 0.008061 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404238 -> initscore=-0.387840\n",
      "[LightGBM] [Info] Start training from score -0.387840\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98670\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95620\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94508\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95061\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32339, number of negative: 47661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.010961 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404238 -> initscore=-0.387840\n",
      "[LightGBM] [Info] Start training from score -0.387840\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98754\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95817\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94632\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.95221\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32339, number of negative: 47661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7326\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3663\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 32 dense feature groups (2.44 MB) transferred to GPU in 0.007365 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404238 -> initscore=-0.387840\n",
      "[LightGBM] [Info] Start training from score -0.387840\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98546\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95684\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94867\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95274\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1600000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 32340, number of negative: 47660\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7328\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 3664\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.75 MB) transferred to GPU in 0.010916 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.404250 -> initscore=-0.387788\n",
      "[LightGBM] [Info] Start training from score -0.387788\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98778\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95643\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94508\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.95072\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n",
      "Processing sEH batch starting at offset 1700000...\n",
      "Processing sEH with LGBM...\n",
      "Fold 1 for sEH with LGBM batch 1700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 27792, number of negative: 40652\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 68444, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 38 dense feature groups (2.61 MB) transferred to GPU in 0.008959 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406055 -> initscore=-0.380300\n",
      "[LightGBM] [Info] Start training from score -0.380300\n",
      "sEH - LGBM - Fold 1 - Mean Average Precision (mAP): 0.98605\n",
      "sEH - LGBM - Fold 1 - Precision: 0.95744\n",
      "sEH - LGBM - Fold 1 - Recall: 0.94215\n",
      "sEH - LGBM - Fold 1 - F1 Score: 0.94974\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_0.pkl\n",
      "Fold 2 for sEH with LGBM batch 1700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 27793, number of negative: 40651\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 68444, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.35 MB) transferred to GPU in 0.011888 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406069 -> initscore=-0.380239\n",
      "[LightGBM] [Info] Start training from score -0.380239\n",
      "sEH - LGBM - Fold 2 - Mean Average Precision (mAP): 0.98711\n",
      "sEH - LGBM - Fold 2 - Precision: 0.95403\n",
      "sEH - LGBM - Fold 2 - Recall: 0.94991\n",
      "sEH - LGBM - Fold 2 - F1 Score: 0.95197\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_1.pkl\n",
      "Fold 3 for sEH with LGBM batch 1700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 27793, number of negative: 40651\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 68444, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 36 dense feature groups (2.35 MB) transferred to GPU in 0.010263 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406069 -> initscore=-0.380239\n",
      "[LightGBM] [Info] Start training from score -0.380239\n",
      "sEH - LGBM - Fold 3 - Mean Average Precision (mAP): 0.98554\n",
      "sEH - LGBM - Fold 3 - Precision: 0.95092\n",
      "sEH - LGBM - Fold 3 - Recall: 0.94531\n",
      "sEH - LGBM - Fold 3 - F1 Score: 0.94811\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_2.pkl\n",
      "Fold 4 for sEH with LGBM batch 1700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 27793, number of negative: 40651\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 68444, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 34 dense feature groups (2.35 MB) transferred to GPU in 0.010675 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406069 -> initscore=-0.380239\n",
      "[LightGBM] [Info] Start training from score -0.380239\n",
      "sEH - LGBM - Fold 4 - Mean Average Precision (mAP): 0.98752\n",
      "sEH - LGBM - Fold 4 - Precision: 0.95816\n",
      "sEH - LGBM - Fold 4 - Recall: 0.94257\n",
      "sEH - LGBM - Fold 4 - F1 Score: 0.95030\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_3.pkl\n",
      "Fold 5 for sEH with LGBM batch 1700000...\n",
      "Loaded existing LGBM model for sEH\n",
      "[LightGBM] [Info] Number of positive: 27793, number of negative: 40651\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 7324\n",
      "[LightGBM] [Info] Number of data points in the train set: 68444, number of used features: 3662\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 35 dense feature groups (2.35 MB) transferred to GPU in 0.012114 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.406069 -> initscore=-0.380239\n",
      "[LightGBM] [Info] Start training from score -0.380239\n",
      "sEH - LGBM - Fold 5 - Mean Average Precision (mAP): 0.98455\n",
      "sEH - LGBM - Fold 5 - Precision: 0.95501\n",
      "sEH - LGBM - Fold 5 - Recall: 0.94401\n",
      "sEH - LGBM - Fold 5 - F1 Score: 0.94948\n",
      "Memory cleared.\n",
      "Model saved: sEH_lgbm_model_fold_4.pkl\n",
      "Best model saved: sEH_lgbm_best_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import psutil\n",
    "\n",
    "# 파라미터 설정\n",
    "xgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'device': 'cuda',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 14,\n",
    "    'updater': 'grow_gpu_hist',\n",
    "    'refresh_leaf': 1,\n",
    "    'process_type': 'default',\n",
    "    'use_label_encoder': False,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 1000,\n",
    "    'learning_rate': 0.1,\n",
    "    'depth': 10,\n",
    "    'task_type': 'GPU',\n",
    "    'verbose': 100\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 31,\n",
    "    'objective': 'binary',\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "def setup_gpu():\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "            print(\"TensorFlow GPU memory growth enabled\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"TensorFlow GPU not available\")\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name, fold, model_type):\n",
    "    X_train = cp.asnumpy(X_train)\n",
    "    X_test = cp.asnumpy(X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    map_score = average_precision_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, map_score, accuracy, precision, recall, f1\n",
    "\n",
    "def train_and_evaluate_model_rf(model, X_train, X_test, y_train, y_test, model_name, fold):\n",
    "    X_train_np = cp.asnumpy(X_train)\n",
    "    X_test_np = cp.asnumpy(X_test)\n",
    "\n",
    "    model.fit(X_train_np, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "    y_pred = model.predict(X_test_np)\n",
    "    y_pred_proba_np = cp.asnumpy(y_pred_proba)\n",
    "    y_pred_np = cp.asnumpy(y_pred)\n",
    "\n",
    "    map_score = average_precision_score(y_test, y_pred_proba_np)\n",
    "    accuracy = accuracy_score(y_test, y_pred_np)\n",
    "    precision = precision_score(y_test, y_pred_np)\n",
    "    recall = recall_score(y_test, y_pred_np)\n",
    "    f1 = f1_score(y_test, y_pred_np)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    del X_train_np, X_test_np, y_pred_proba, y_pred\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    clear_memory()\n",
    "    return model, map_score, accuracy, precision, recall, f1\n",
    "\n",
    "def train_and_evaluate_lstm_tf(model, X_train, X_test, y_train, y_test, model_name, fold, batch_size, num_epochs):\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "    model.fit(X_train_tensor, y_train_tensor, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, precision, recall, f1, map_score\n",
    "\n",
    "def OneDCNN_model(input_len, num_classes):\n",
    "    hidden_dim = 128\n",
    "    num_filters = 32\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(input_len,))\n",
    "    x = Embedding(input_dim=36, output_dim=hidden_dim, input_length=input_len, mask_zero=True)(inputs)\n",
    "    x = Conv1D(filters=num_filters, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=num_filters*2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = Conv1D(filters=num_filters*3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    \n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss = 'binary_crossentropy'\n",
    "    weighted_metrics = [AUC(curve='PR', name='avg_precision')]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn_tf(model, X_train, X_test, y_train, y_test, model_name, fold, batch_size, num_epochs):\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy',\n",
    "                  metrics=[tf.keras.metrics.Recall()])\n",
    "\n",
    "    es = EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n",
    "    checkpoint = ModelCheckpoint(monitor='val_loss', filepath=f\"{model_name}.h5\",\n",
    "                                 save_best_only=True, save_weights_only=True, mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, verbose=1)\n",
    "\n",
    "    model.fit(X_train_tensor, y_train_tensor, validation_data=(X_test_tensor, y_test_tensor),\n",
    "              epochs=num_epochs, callbacks=[checkpoint, reduce_lr_loss, es],\n",
    "              batch_size=batch_size, verbose=1)\n",
    "\n",
    "    y_pred_proba = model.predict(X_test_tensor)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    map_score = average_precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "    precision = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "    recall = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "    f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "    print(f\"{model_name} - Fold {fold} - Mean Average Precision (mAP): {map_score:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Precision: {precision:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - Recall: {recall:.5f}\")\n",
    "    print(f\"{model_name} - Fold {fold} - F1 Score: {f1:.5f}\")\n",
    "\n",
    "    clear_memory()\n",
    "    return model, precision, recall, f1, map_score\n",
    "\n",
    "def get_model(model_type, input_len=None, num_classes=None):\n",
    "    if model_type == 'random_forest':\n",
    "        return RandomForestClassifier(**rf_params)\n",
    "    elif model_type == 'xgboost':\n",
    "        return XGBClassifier(**xgb_params)\n",
    "    elif model_type == 'catboost':\n",
    "        return CatBoostClassifier(**catboost_params)\n",
    "    elif model_type == 'cnn':\n",
    "        return OneDCNN_model(input_len, num_classes)\n",
    "    elif model_type == 'lgbm':\n",
    "        return LGBMClassifier(**lgbm_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "def monitor_memory(threshold=0.80):\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    return memory_info.percent / 100 >= threshold\n",
    "\n",
    "def process_protein_batches(protein, model_types, input_parquet_path, num_folds):\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "    print(total_rows)\n",
    "    batch_size = 100000\n",
    "\n",
    "    for offset in range(0, total_rows, batch_size):\n",
    "        filtered_df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "        WHERE protein_name = '{protein}'\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\").df()\n",
    "\n",
    "        print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "\n",
    "        X = np.concatenate([\n",
    "            np.array(filtered_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(filtered_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "        y = filtered_df['binds'].tolist()\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "            process_model_type(model_type, skf, X, y, protein, offset)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def process_protein_batches_type2(protein, model_types, input_parquet_path, num_folds):\n",
    "    con = duckdb.connect()\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "    print(total_rows)\n",
    "    batch_size = 99999\n",
    "\n",
    "    for offset in range(0, total_rows, batch_size):\n",
    "        filtered_df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{input_parquet_path}') \n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\").df()\n",
    "\n",
    "        print(f\"Processing {protein} batch starting at offset {offset}...\")\n",
    "        \n",
    "        binds_1 = filtered_df[filtered_df['bind'] == 1]\n",
    "        binds_0 = filtered_df[filtered_df['bind'] == 0].sample(n=len(binds_1) * 2, random_state=42)\n",
    "\n",
    "        sampled_df = pd.concat([binds_1, binds_0]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        X = np.array(sampled_df[[f'enc{i}' for i in range(142)]].tolist(), dtype=np.float32)\n",
    "        y = sampled_df['bind'].tolist()\n",
    "\n",
    "        # 명시적으로 메모리 해제\n",
    "        del filtered_df, binds_1, binds_0, sampled_df\n",
    "        clear_memory()\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"Processing {protein} with {model_type.upper()}...\")\n",
    "            process_model_type(model_type, skf, X, y, protein, offset)\n",
    "    con.close()\n",
    "\n",
    "def process_model_type(model_type, skf, X, y, protein, offset):\n",
    "    precision_records = []\n",
    "    best_map_score = 0\n",
    "    best_model = None\n",
    "    best_fold = None\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {fold + 1} for {protein} with {model_type.upper()} batch {offset}...\")\n",
    "\n",
    "        model = get_model(model_type)\n",
    "        model_filename = f\"{protein}_{model_type}_model_fold_{fold}.pkl\"\n",
    "\n",
    "        try:\n",
    "            model = joblib.load(model_filename)\n",
    "            print(f\"Loaded existing {model_type.upper()} model for {protein}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Initialized new {model_type.upper()} model for {protein}\")\n",
    "\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = np.array(y)[train_idx], np.array(y)[val_idx]\n",
    "\n",
    "        if model_type in ['xgboost', 'catboost', 'lgbm']:\n",
    "            model, map_score, accuracy, precision, recall, f1 = train_and_evaluate_model(model, X_train, X_val, y_train, y_val, f\"{protein} - {model_type.upper()}\", fold + 1, model_type)\n",
    "        elif model_type == 'random_forest':\n",
    "            model, map_score, accuracy, precision, recall, f1 = train_and_evaluate_model_rf(model, X_train, X_val, y_train, y_val, f\"{protein} - {model_type.upper()}\", fold + 1)\n",
    "\n",
    "        precision_records.append((protein, model_type, 'N/A', precision, recall, f1, map_score, offset, fold + 1))\n",
    "\n",
    "        if map_score > best_map_score:\n",
    "            best_map_score = map_score\n",
    "            best_model = model\n",
    "            best_fold = fold\n",
    "\n",
    "        if monitor_memory():\n",
    "            print(\"Memory usage exceeded threshold. Clearing memory...\")\n",
    "            clear_memory()\n",
    "\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Model saved: {model_filename}\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        best_model_filename = f\"{protein}_{model_type}_best_model.pkl\"\n",
    "        joblib.dump(best_model, best_model_filename)\n",
    "        print(f\"Best model saved: {best_model_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_gpu()\n",
    "    protein_names = ['BRD4', 'HSA', 'sEH']\n",
    "    model_types = ['lgbm']#'catboost', \n",
    "    num_folds = 5\n",
    "    input_parquet_path = 'processed_merged_queried_data.parquet'\n",
    "\n",
    "    for protein in protein_names:\n",
    "        # process_protein_batches(protein, model_types, input_parquet_path, num_folds)\n",
    "        if model_types != \"transformer\":\n",
    "            process_protein_batches(protein, model_types, input_parquet_path, num_folds)\n",
    "        else:\n",
    "            input_parquet_path = f\"train_enc_{protein}.parquet\"\n",
    "            process_protein_batches_type2(protein, model_types, input_parquet_path, num_folds)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test코드 1DCNN 추가 + transformer(취소)\n",
    "lgbm 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for BRD4\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for HSA\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Loaded LGBM model for sEH\n",
      "Memory cleared.\n",
      "Test results saved to 'test_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# GPU 설정 확인 및 메모리 사용량 조절\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"TensorFlow GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"TensorFlow GPU not available\")\n",
    "\n",
    "# 데이터베이스 연결\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# 테스트 데이터 경로 설정\n",
    "test_parquet_path = 'test_processed_data.parquet'\n",
    "\n",
    "# 각 protein에 대한 최적의 조합을 설정\n",
    "best_combinations = {\n",
    "    'BRD4': [('elu', 'relu')],\n",
    "    'HSA': [('relu', 'elu')],\n",
    "    'sEH': [('elu', 'relu')]\n",
    "}\n",
    "\n",
    "# 사용할 모델 종류 지정\n",
    "model_types = ['lgbm']  # 여기에 LGBM 모델을 추가했습니다'catboost', 'cnn', \n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "batch_size = 20000  # 적절한 배치 크기로 설정\n",
    "\n",
    "# 메모리 정리 함수\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "# 1D-CNN 모델 정의 함수\n",
    "def OneDCNN_model(input_len, num_classes):\n",
    "    hidden_dim = 128\n",
    "    num_filters = 32\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(input_len,))\n",
    "    x = Embedding(input_dim=36, output_dim=hidden_dim, input_length=input_len, mask_zero=True)(inputs)\n",
    "    x = Conv1D(filters=num_filters, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    x = Conv1D(filters=num_filters*2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    x = Conv1D(filters=num_filters*3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss = 'binary_crossentropy'\n",
    "    weighted_metrics = [tf.keras.metrics.AUC(curve='PR', name='avg_precision')]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "for protein in best_combinations.keys():\n",
    "    # 총 행 수 계산\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{test_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch_df = con.execute(f\"\"\"\n",
    "            SELECT * FROM read_parquet('{test_parquet_path}')\n",
    "            WHERE protein_name = '{protein}'\n",
    "            LIMIT {batch_size} OFFSET {start}\n",
    "        \"\"\").df()\n",
    "\n",
    "        # 테스트 데이터 전처리\n",
    "        X_test = np.concatenate([\n",
    "            np.array(batch_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            for activation_1, activation_2 in best_combinations[protein]:\n",
    "                if model_type == 'xgboost':\n",
    "                    model = XGBClassifier()\n",
    "                    model_filename = f\"{protein}_xgboost_model.pkl\"\n",
    "                elif model_type == 'randomforest':\n",
    "                    model = RandomForestClassifier()\n",
    "                    model_filename = f\"{protein}_rf_model_iteration_1.pkl\"\n",
    "                elif model_type == 'catboost':\n",
    "                    model = CatBoostClassifier()\n",
    "                    model_filename = f\"{protein}_catboost_model_iteration_1.pkl\"\n",
    "                elif model_type == 'lgbm':\n",
    "                    model = LGBMClassifier()\n",
    "                    model_filename = f\"{protein}_lgbm_best_model.pkl\"\n",
    "                elif model_type == 'lstm':\n",
    "                    input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "                    hidden_dim = 128\n",
    "                    output_dim = 1\n",
    "\n",
    "                    model = Sequential([\n",
    "                        Input(shape=(None, input_dim)),\n",
    "                        LSTM(hidden_dim * 8, return_sequences=True, activation=activation_1, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 4, return_sequences=True, activation=activation_2, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 1, return_sequences=False, activation=activation_1, dropout=0.05),\n",
    "                        Dense(output_dim, activation='sigmoid')\n",
    "                    ])\n",
    "                    model_filename = f\"{protein}_lstm_model_{activation_1}_{activation_2}_{activation_1}_iteration_1.weights.h5\"\n",
    "                elif model_type == 'cnn':\n",
    "                    input_len = 1024 * 4\n",
    "                    num_classes = 1\n",
    "                    model = OneDCNN_model(input_len, num_classes)\n",
    "                    # model_filename = f\"{protein}_cnn_model_iteration_1.h5\"\n",
    "                    model_filename = f\"{protein} - CNN.h5\"\n",
    "\n",
    "                # 모델 로드\n",
    "                if model_type in ['lstm', 'cnn']:\n",
    "                    model.load_weights(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein} with {activation_1} and {activation_2}\")\n",
    "                else:\n",
    "                    model = joblib.load(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein}\")\n",
    "\n",
    "                # 예측 수행\n",
    "                if model_type == 'lstm':\n",
    "                    X_test_lstm = np.expand_dims(X_test, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "                    y_pred = model.predict(X_test_lstm)\n",
    "                elif model_type == 'randomforest':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'xgboost':\n",
    "                    X_test_gpu = cp.array(X_test)\n",
    "                    y_pred = model.predict_proba(X_test_gpu)[:, 1]\n",
    "                    y_pred = cp.asnumpy(y_pred)\n",
    "                elif model_type == 'catboost':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'lgbm':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'cnn':\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                # 결과 저장\n",
    "                test_results = pd.DataFrame({\n",
    "                    'id': batch_df['id'],\n",
    "                    'binds': y_pred.flatten()\n",
    "                })\n",
    "                results.append(test_results)\n",
    "\n",
    "                # 메모리 정리\n",
    "                clear_memory()\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 결합\n",
    "final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 결과 저장\n",
    "final_results_df.to_csv('test_results.csv', index=False)\n",
    "print(\"Test results saved to 'test_results.csv'\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singlefold transformer 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Conv1dBnRelu 정의\n",
    "class Conv1dBnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, is_bn=True):\n",
    "        super(Conv1dBnRelu, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)\n",
    "        ]\n",
    "        if is_bn:\n",
    "            layers.append(nn.BatchNorm1d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Tokenization ====================================\n",
    "MOLECULE_DICT = {\n",
    "    'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n",
    "    '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25,\n",
    "    '=': 26, '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36\n",
    "}\n",
    "MAX_MOLECULE_ID = np.max(list(MOLECULE_DICT.values()))\n",
    "VOCAB_SIZE = MAX_MOLECULE_ID + 10\n",
    "UNK = 255  # disallow: will cause error\n",
    "BOS = MAX_MOLECULE_ID + 1\n",
    "EOS = MAX_MOLECULE_ID + 2\n",
    "PAD = 0\n",
    "MAX_LENGTH = 160\n",
    "\n",
    "MOLECULE_LUT = np.full(256, fill_value=UNK, dtype=np.uint8)\n",
    "for k, v in MOLECULE_DICT.items():\n",
    "    ascii = ord(k)\n",
    "    MOLECULE_LUT[ascii] = v\n",
    "\n",
    "def make_token(s):\n",
    "    t = np.frombuffer(s, np.uint8)\n",
    "    t = MOLECULE_LUT[t]\n",
    "    t = t.tolist()\n",
    "    \n",
    "    # Ensure token ids are within VOCAB_SIZE\n",
    "    t = [tok for tok in t if tok < VOCAB_SIZE]\n",
    "    \n",
    "    # Truncate if length is greater than MAX_LENGTH - 2\n",
    "    if len(t) > MAX_LENGTH - 2:\n",
    "        t = t[:MAX_LENGTH - 2]\n",
    "    \n",
    "    L = len(t) + 2\n",
    "    token_id = [BOS] + t + [EOS] + [PAD] * (MAX_LENGTH - L)\n",
    "    token_mask = [1] * L + [0] * (MAX_LENGTH - L)\n",
    "    \n",
    "    return token_id, token_mask\n",
    "\n",
    "def load_and_sample_data_batch(filepath, protein_type, ratio=1/3, batch_size=200001, random_state=42):\n",
    "    conn = duckdb.connect(database=':memory:')\n",
    "    conn.execute(f\"INSTALL 'parquet';\")\n",
    "    conn.execute(f\"LOAD 'parquet';\")\n",
    "    \n",
    "    print(f\"Loading data for protein type: {protein_type}, batch size: {batch_size}\")\n",
    "    \n",
    "    pos_sample_size = int(batch_size * ratio)\n",
    "    neg_sample_size = batch_size - pos_sample_size\n",
    "    \n",
    "    pos_query = f\"\"\"\n",
    "    SELECT * FROM read_parquet('{filepath}')\n",
    "    WHERE protein_name = '{protein_type}' AND binds = 1\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT {pos_sample_size};\n",
    "    \"\"\"\n",
    "    \n",
    "    neg_query = f\"\"\"\n",
    "    SELECT * FROM read_parquet('{filepath}')\n",
    "    WHERE protein_name = '{protein_type}' AND binds = 0\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT {neg_sample_size};\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_df = conn.execute(pos_query).df()\n",
    "    neg_df = conn.execute(neg_query).df()\n",
    "    \n",
    "    sampled_df = pd.concat([pos_df, neg_df]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    del pos_df, neg_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class SMILESDataSet(Dataset):\n",
    "    def __init__(self, dataframe, max_length=160):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 열 이름을 출력하여 확인\n",
    "        print(\"Dataframe columns:\", self.dataframe.columns)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # 각각의 SMILES 문자열을 개별적으로 가져옵니다.\n",
    "        buildingblock1_smiles = row['buildingblock1_smiles']\n",
    "        buildingblock2_smiles = row['buildingblock2_smiles']\n",
    "        buildingblock3_smiles = row['buildingblock3_smiles']\n",
    "        molecule_smiles = row['molecule_smiles']\n",
    "\n",
    "        # 각각의 SMILES 문자열을 개별적으로 토큰화합니다.\n",
    "        token_id_1, token_mask_1 = make_token(buildingblock1_smiles.encode('utf-8'))\n",
    "        token_id_2, token_mask_2 = make_token(buildingblock2_smiles.encode('utf-8'))\n",
    "        token_id_3, token_mask_3 = make_token(buildingblock3_smiles.encode('utf-8'))\n",
    "        token_id_molecule, token_mask_molecule = make_token(molecule_smiles.encode('utf-8'))\n",
    "\n",
    "        target = row['binds']\n",
    "\n",
    "        return {\n",
    "            'buildingblock1_smiles_token_id': torch.tensor(token_id_1, dtype=torch.long),\n",
    "            'buildingblock1_smiles_token_mask': torch.tensor(token_mask_1, dtype=torch.long),\n",
    "            'buildingblock2_smiles_token_id': torch.tensor(token_id_2, dtype=torch.long),\n",
    "            'buildingblock2_smiles_token_mask': torch.tensor(token_mask_2, dtype=torch.long),\n",
    "            'buildingblock3_smiles_token_id': torch.tensor(token_id_3, dtype=torch.long),\n",
    "            'buildingblock3_smiles_token_mask': torch.tensor(token_mask_3, dtype=torch.long),\n",
    "            'molecule_smiles_token_id': torch.tensor(token_id_molecule, dtype=torch.long),\n",
    "            'molecule_smiles_token_mask': torch.tensor(token_mask_molecule, dtype=torch.long),\n",
    "            'bind': torch.tensor(target, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# 모델 정의\n",
    "class FlashAttentionTransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim_model,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            dim_feedforward,\n",
    "            dropout=0.0,\n",
    "            norm_first=False,\n",
    "            activation=F.gelu,\n",
    "            rotary_emb_dim=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,\n",
    "            norm_first=norm_first\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask):\n",
    "        return self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=256):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embed_dim = 512\n",
    "        self.output_type = ['infer', 'loss']\n",
    "        self.pe = PositionalEncoding(embed_dim, max_len=256)\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, 64, padding_idx=PAD)\n",
    "        self.conv_embedding = nn.Sequential(\n",
    "            Conv1dBnRelu(64, embed_dim, kernel_size=3, stride=1, padding=1, is_bn=True),\n",
    "        )\n",
    "        self.tx_encoder = FlashAttentionTransformerEncoder(\n",
    "            dim_model=embed_dim,\n",
    "            num_heads=8,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=0.1,\n",
    "            norm_first=False,\n",
    "            activation=F.gelu,\n",
    "            rotary_emb_dim=0,\n",
    "            num_layers=7,\n",
    "        )\n",
    "        self.bind = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, 1),  # multiply by 4 because we concatenate 4 outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        combined_output = []\n",
    "        for key in ['buildingblock1_smiles_token_id', 'buildingblock2_smiles_token_id', 'buildingblock3_smiles_token_id', 'molecule_smiles_token_id']:\n",
    "            smiles_token_id = batch[key].long()\n",
    "            smiles_token_mask = batch[key.replace('_id', '_mask')].long()\n",
    "            B, L = smiles_token_id.shape\n",
    "            x = self.embedding(smiles_token_id)\n",
    "            x = x.permute(0, 2, 1).float()\n",
    "            x = self.conv_embedding(x)\n",
    "            x = x.permute(0, 2, 1).contiguous()\n",
    "            x = self.pe(x)\n",
    "            z = self.tx_encoder(\n",
    "                x=x,\n",
    "                src_key_padding_mask=smiles_token_mask == 0,\n",
    "            )\n",
    "            m = smiles_token_mask.unsqueeze(-1).float()\n",
    "            pool = (z * m).sum(1) / m.sum(1)\n",
    "            combined_output.append(pool)\n",
    "        \n",
    "        combined_output = torch.cat(combined_output, dim=1)\n",
    "        bind = self.bind(combined_output)\n",
    "        output = {}\n",
    "        if 'loss' in self.output_type:\n",
    "            if 'bind' in batch:\n",
    "                target = batch['bind'].unsqueeze(1)\n",
    "                output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float(), reduction='mean')\n",
    "            else:\n",
    "                output['bce_loss'] = torch.tensor(0.0, requires_grad=True)  # 예외 처리\n",
    "        if 'infer' in self.output_type:\n",
    "            output['bind'] = torch.sigmoid(bind)\n",
    "        return output\n",
    "\n",
    "# 학습 함수 수정\n",
    "def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch in train_loader_tqdm:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs['bind'], batch['bind'].unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            train_loader_tqdm.set_postfix(loss=running_loss / (train_loader_tqdm.n + 1))\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader_tqdm)}\")\n",
    "\n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{num_epochs}\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader_tqdm:\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs['bind'], batch['bind'].unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                val_loader_tqdm.set_postfix(val_loss=val_loss / (val_loader_tqdm.n + 1))\n",
    "                \n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader_tqdm)}\")\n",
    "        model.train()\n",
    "        \n",
    "        # 가비지 컬렉션 수행\n",
    "        gc.collect()\n",
    "\n",
    "# 전체 학습 과정 함수화\n",
    "def train_and_save_model(protein_type, data_filepath, model_save_dir, num_epochs=5, ratio=1/2, batch_size=200000):\n",
    "    for batch_num in tqdm(range(50), desc=\"Batches\"):\n",
    "        print(f\"Processing batch {batch_num + 1} / 50\")\n",
    "        \n",
    "        # 데이터 샘플링\n",
    "        sampled_df = load_and_sample_data_batch(data_filepath, protein_type, ratio, batch_size)\n",
    "        \n",
    "        print(f\"Sampled {len(sampled_df)} data points for batch {batch_num + 1}\")\n",
    "        \n",
    "        # 데이터셋 생성\n",
    "        dataset = SMILESDataSet(sampled_df)\n",
    "        \n",
    "        # 데이터셋 나누기 (학습/검증)\n",
    "        train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42)\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        \n",
    "        print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")\n",
    "        \n",
    "        # DataLoader 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # 모델, 손실 함수, 최적화 알고리즘 정의\n",
    "        model = Net()\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 모델 학습\n",
    "        print(f\"Training {protein_type} model on batch {batch_num + 1} ...\")\n",
    "        train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs)\n",
    "        \n",
    "        # 모델 저장\n",
    "        if not os.path.exists(model_save_dir):\n",
    "            os.makedirs(model_save_dir)\n",
    "        \n",
    "        model_save_path = os.path.join(model_save_dir, f'{protein_type}_model_batch{batch_num + 1}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pth')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "        \n",
    "        # 가비지 컬렉션 수행\n",
    "        del sampled_df, dataset, train_dataset, val_dataset, model, criterion, optimizer\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 / 50\n",
      "Loading data for protein type: HSA, batch size: 200000\n",
      "Sampled 200000 data points for batch 1\n",
      "Dataframe columns: Index(['id', 'buildingblock1_smiles', 'buildingblock2_smiles',\n",
      "       'buildingblock3_smiles', 'molecule_smiles', 'protein_name', 'binds'],\n",
      "      dtype='object')\n",
      "Train dataset size: 160000, Validation dataset size: 40000\n",
      "Training HSA model on batch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  67%|██████▋   | 3326/5000 [9:23:27<4:43:35, 10.16s/it, loss=0.703]\n",
      "Batches:   0%|          | 0/50 [9:23:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 사용 예시\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_and_save_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprotein_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHSA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_filepath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./train.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# # BRD4 모델 학습 및 저장\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# train_and_save_model('BRD4', data_filepath, model_save_dir)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# # sEH 모델 학습 및 저장\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# train_and_save_model('sEH', data_filepath, model_save_dir)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 307\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[1;34m(protein_type, data_filepath, model_save_dir, num_epochs, ratio, batch_size)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model on batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_save_dir):\n",
      "Cell \u001b[1;32mIn[1], line 251\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_loader, val_loader, model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m    249\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[0;32m    250\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbind\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbind\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 251\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    254\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 함수 호출 예제\n",
    "data_filepath = './train.parquet'\n",
    "model_save_dir = './models'\n",
    "\n",
    "# 사용 예시\n",
    "train_and_save_model(\n",
    "    protein_type='HSA',\n",
    "    data_filepath = './train.parquet',\n",
    "    model_save_dir = './models',\n",
    "    num_epochs=5,\n",
    "    ratio=1/2,\n",
    "    batch_size=200000\n",
    ")\n",
    "\n",
    "# # BRD4 모델 학습 및 저장\n",
    "# train_and_save_model('BRD4', data_filepath, model_save_dir)\n",
    "\n",
    "# # HSA 모델 학습 및 저장\n",
    "# train_and_save_model('HSA', data_filepath, model_save_dir)\n",
    "\n",
    "# # sEH 모델 학습 및 저장\n",
    "# train_and_save_model('sEH', data_filepath, model_save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test코드 1DCNN 추가 - 실행버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for BRD4 with elu and relu\n",
      "590/590 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for HSA with relu and elu\n",
      "560/560 [==============================] - 4s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 7ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "625/625 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Loaded CNN model for sEH with elu and relu\n",
      "567/567 [==============================] - 5s 8ms/step\n",
      "Memory cleared.\n",
      "Test results saved to 'test_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Conv1D, GlobalMaxPooling1D, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# GPU 설정 확인 및 메모리 사용량 조절\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"TensorFlow GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"TensorFlow GPU not available\")\n",
    "\n",
    "# 데이터베이스 연결\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# 테스트 데이터 경로 설정\n",
    "test_parquet_path = 'test_processed_data.parquet'\n",
    "\n",
    "# 각 protein에 대한 최적의 조합을 설정\n",
    "best_combinations = {\n",
    "    'BRD4': [('elu', 'relu')],\n",
    "    'HSA': [('relu', 'elu')],\n",
    "    'sEH': [('elu', 'relu')]\n",
    "}\n",
    "\n",
    "# 사용할 모델 종류 지정\n",
    "model_types = ['catboost', 'cnn']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "model_types = ['cnn']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "batch_size = 20000  # 적절한 배치 크기로 설정\n",
    "\n",
    "# 메모리 정리 함수\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "# 1D-CNN 모델 정의 함수\n",
    "def OneDCNN_model(input_len, num_classes):\n",
    "    hidden_dim = 128\n",
    "    num_filters = 32\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(input_len,))\n",
    "    x = Embedding(input_dim=36, output_dim=hidden_dim, input_length=input_len, mask_zero=True)(inputs)\n",
    "    x = Conv1D(filters=num_filters, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    x = Conv1D(filters=num_filters*2, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    x = Conv1D(filters=num_filters*3, kernel_size=3, activation='relu', padding='valid', strides=1)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss = 'binary_crossentropy'\n",
    "    weighted_metrics = [tf.keras.metrics.AUC(curve='PR', name='avg_precision')]\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=weighted_metrics,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "for protein in best_combinations.keys():\n",
    "    # 총 행 수 계산\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{test_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch_df = con.execute(f\"\"\"\n",
    "            SELECT * FROM read_parquet('{test_parquet_path}')\n",
    "            WHERE protein_name = '{protein}'\n",
    "            LIMIT {batch_size} OFFSET {start}\n",
    "        \"\"\").df()\n",
    "\n",
    "        # 테스트 데이터 전처리\n",
    "        X_test = np.concatenate([\n",
    "            np.array(batch_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            for activation_1, activation_2 in best_combinations[protein]:\n",
    "                if model_type == 'xgboost':\n",
    "                    model = XGBClassifier()\n",
    "                    model_filename = f\"{protein}_xgboost_model.pkl\"\n",
    "                elif model_type == 'randomforest':\n",
    "                    model = RandomForestClassifier()\n",
    "                    model_filename = f\"{protein}_rf_model_iteration_1.pkl\"\n",
    "                elif model_type == 'catboost':\n",
    "                    model = CatBoostClassifier()\n",
    "                    model_filename = f\"{protein}_catboost_model_iteration_1.pkl\"\n",
    "                elif model_type == 'lstm':\n",
    "                    input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "                    hidden_dim = 128\n",
    "                    output_dim = 1\n",
    "\n",
    "                    model = Sequential([\n",
    "                        Input(shape=(None, input_dim)),\n",
    "                        LSTM(hidden_dim * 8, return_sequences=True, activation=activation_1, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 4, return_sequences=True, activation=activation_2, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 1, return_sequences=False, activation=activation_1, dropout=0.05),\n",
    "                        Dense(output_dim, activation='sigmoid')\n",
    "                    ])\n",
    "                    model_filename = f\"{protein}_lstm_model_{activation_1}_{activation_2}_{activation_1}_iteration_1.weights.h5\"\n",
    "                elif model_type == 'cnn':\n",
    "                    input_len = 1024 * 4\n",
    "                    num_classes = 1\n",
    "                    model = OneDCNN_model(input_len, num_classes)\n",
    "                    #model_filename = f\"{protein}_cnn_model_iteration_1.h5\"\n",
    "                    model_filename = f\"{protein} - CNN.h5\"\n",
    "\n",
    "                # 모델 로드\n",
    "                if model_type in ['lstm', 'cnn']:\n",
    "                    model.load_weights(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein} with {activation_1} and {activation_2}\")\n",
    "                else:\n",
    "                    model = joblib.load(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein}\")\n",
    "\n",
    "                # 예측 수행\n",
    "                if model_type == 'lstm':\n",
    "                    X_test_lstm = np.expand_dims(X_test, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "                    y_pred = model.predict(X_test_lstm)\n",
    "                elif model_type == 'randomforest':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'xgboost':\n",
    "                    X_test_gpu = cp.array(X_test)\n",
    "                    y_pred = model.predict_proba(X_test_gpu)[:, 1]\n",
    "                    y_pred = cp.asnumpy(y_pred)\n",
    "                elif model_type == 'catboost':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'cnn':\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                # 결과 저장\n",
    "                test_results = pd.DataFrame({\n",
    "                    'id': batch_df['id'],\n",
    "                    'binds': y_pred.flatten()\n",
    "                })\n",
    "                results.append(test_results)\n",
    "\n",
    "                # 메모리 정리\n",
    "                clear_memory()\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 결합\n",
    "final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 결과 저장\n",
    "final_results_df.to_csv('test_results.csv', index=False)\n",
    "print(\"Test results saved to 'test_results.csv'\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the scaler and IncrementalPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [35:00<00:00, 44.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the data using IncrementalPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [3:01:53<00:00, 232.20s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all chunks into a single DataFrame...\n",
      "Fitting the scaler and IncrementalPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [35:04<00:00, 44.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming the data using IncrementalPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [2:54:22<00:00, 222.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all chunks into a single DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Initialize the IncrementalPCA and StandardScaler\n",
    "n_components = 142\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=1000)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Connect to DuckDB and read the data in chunks\n",
    "con = duckdb.connect()\n",
    "\n",
    "proteins = [\"HSA\", \"sEH\"]# \"BRD4\", \n",
    "\n",
    "for protein in proteins:\n",
    "    # Determine total number of rows for progress tracking\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM 'train_enc_{protein}.parquet'\").fetchone()[0]\n",
    "\n",
    "    # First pass: fit the scaler and IPCA\n",
    "    chunksize = 2097152\n",
    "    offset = 0\n",
    "    \n",
    "    ipca_model_path = f'ipca_model_{protein}.pkl'\n",
    "    scaler_model_path = f'scaler_model_{protein}.pkl'\n",
    "\n",
    "    if not os.path.exists(ipca_model_path) and not os.path.exists(scaler_model_path):\n",
    "        print(\"Fitting the scaler and IncrementalPCA...\")\n",
    "        for offset in tqdm(range(0, total_rows, chunksize)):\n",
    "            # Read a chunk of data\n",
    "            query = f\"SELECT * FROM 'train_enc_BRD4.parquet' LIMIT {chunksize} OFFSET {offset}\"\n",
    "            chunk = con.execute(query).fetchdf()\n",
    "            if chunk.empty:\n",
    "                break\n",
    "            \n",
    "            X_chunk = chunk.drop(columns=['bind'])\n",
    "            scaler.partial_fit(X_chunk)\n",
    "            \n",
    "            X_scaled = scaler.transform(X_chunk)\n",
    "            ipca.partial_fit(X_scaled)\n",
    "            \n",
    "            # Collect garbage to free memory\n",
    "            del chunk, X_chunk, X_scaled\n",
    "            gc.collect()\n",
    "        joblib.dump(ipca, f'ipca_model_{protein}.pkl')\n",
    "        joblib.dump(scaler, f'scaler_model_{protein}.pkl')\n",
    "    else:\n",
    "        # Load the saved models\n",
    "        ipca = joblib.load(f'ipca_model_{protein}.pkl')\n",
    "        scaler = joblib.load(f'scaler_model_{protein}.pkl')\n",
    "\n",
    "    # Directory to save chunk results\n",
    "    output_dir = f'pca_chunks_{protein}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    chunk_files = []\n",
    "\n",
    "    # Second pass: transform the data\n",
    "    print(\"Transforming the data using IncrementalPCA...\")\n",
    "    for offset in tqdm(range(0, total_rows, chunksize)):\n",
    "        # Read a chunk of data\n",
    "        query = f\"SELECT * FROM 'train_enc_{protein}.parquet' LIMIT {chunksize} OFFSET {offset}\"\n",
    "        chunk = con.execute(query).fetchdf()\n",
    "        if chunk.empty:\n",
    "            break\n",
    "        \n",
    "        X_chunk = chunk.drop(columns=['bind'])\n",
    "        y_chunk = chunk['bind']\n",
    "        \n",
    "        X_scaled = scaler.transform(X_chunk)\n",
    "        pcs = ipca.transform(X_scaled)\n",
    "        \n",
    "        pca_columns = [f'PC{i}' for i in range(n_components)]\n",
    "        pca_chunk_df = pd.DataFrame(data=pcs, columns=pca_columns)\n",
    "        pca_chunk_df['bind'] = y_chunk.values\n",
    "        \n",
    "        chunk_file = os.path.join(output_dir, f'pca_chunk_{protein}_{offset}.csv')\n",
    "        pca_chunk_df.to_csv(chunk_file, index=False)\n",
    "        chunk_files.append(chunk_file)\n",
    "        \n",
    "        # Collect garbage to free memory\n",
    "        del chunk, X_chunk, y_chunk, X_scaled, pcs, pca_chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "    # Merge all chunk files into one DataFrame\n",
    "    print(\"Merging all chunks into a single DataFrame...\")\n",
    "    # merged_df = pd.concat([pd.read_csv(file) for file in chunk_files])\n",
    "\n",
    "        # 병합을 포기함 용량이너무 많음\n",
    "\n",
    "    # # 파일을 처음 생성할 때 컬럼 헤더를 포함하여 작성\n",
    "    # is_first_chunk = True\n",
    "    # merged_output_file = f'transformed_pca_data_{protein}.csv'\n",
    "\n",
    "    # # 병합할 청크 수 (메모리 상황에 맞게 조절)\n",
    "    # merge_batch_size = 4\n",
    "\n",
    "    # # 임시 저장 파일 리스트\n",
    "    # temp_files = []\n",
    "\n",
    "    # # tqdm을 사용하여 진행률 표시\n",
    "    # for i in tqdm(range(0, len(chunk_files), merge_batch_size), desc=\"Merging chunks in batches\"):\n",
    "    #     batch_files = chunk_files[i:i + merge_batch_size]\n",
    "    #     batch_df = pd.concat((pd.read_csv(os.path.join(f'pca_chunk_{protein}', file)) for file in batch_files))\n",
    "    #     temp_file = f'temp_merged_{i // merge_batch_size}.csv'\n",
    "    #     batch_df.to_csv(temp_file, index=False)\n",
    "    #     temp_files.append(temp_file)\n",
    "    #     del batch_df  # 데이터프레임 삭제\n",
    "    #     gc.collect()  # 가비지 컬렉터 호출\n",
    "\n",
    "    # # 최종 병합\n",
    "    # is_first_chunk = True\n",
    "    # for temp_file in tqdm(temp_files, desc=\"Final merging\"):\n",
    "    #     temp_df = pd.read_csv(temp_file)\n",
    "    #     temp_df.to_csv(merged_output_file, mode='a', index=False, header=is_first_chunk)\n",
    "    #     is_first_chunk = False\n",
    "    #     del temp_df  # 데이터프레임 삭제\n",
    "    #     gc.collect()  # 가비지 컬렉터 호출\n",
    "\n",
    "    # # 임시 파일 삭제\n",
    "    # for temp_file in temp_files:\n",
    "    #     os.remove(temp_file)\n",
    "\n",
    "    # print(f\"All PCA results saved to {merged_output_file}\")\n",
    "\n",
    "    # merged_df = pd.read_csv(merged_output_file)\n",
    "    # # Calculate the correlation between principal components and bind\n",
    "    # correlation = merged_df.corr()['bind'][:-1]\n",
    "\n",
    "    # # Plot the correlation of each principal component with bind\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.bar(correlation.index, correlation.values)\n",
    "    # plt.xlabel('Principal Components')\n",
    "    # plt.ylabel('Correlation with bind')\n",
    "    # plt.title('Correlation of Principal Components with bind')\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.show()\n",
    "\n",
    "    # # Display the correlations\n",
    "    # correlation_df = correlation.reset_index()\n",
    "    # correlation_df.columns = ['Principal Component', 'Correlation with bind']\n",
    "\n",
    "    # # Sort by the absolute value of the correlation\n",
    "    # correlation_df = correlation_df.sort_values(by='Correlation with bind', key=abs, ascending=False)\n",
    "\n",
    "    # # Output the top 10 principal components most correlated with bind\n",
    "    # print(\"Top 10 principal components most correlated with bind:\")\n",
    "    # print(correlation_df.head(10))\n",
    "\n",
    "    # # Output the top 10 principal components least correlated with bind\n",
    "    # print(\"Top 10 principal components least correlated with bind:\")\n",
    "    # print(correlation_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로딩 스코어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로딩 값이 loadings_values_BRD4.csv에 저장되었습니다.\n",
      "Scores 값이 scores_values_BRD4.csv에 저장되었습니다.\n",
      "로딩 값이 loadings_values_HSA.csv에 저장되었습니다.\n",
      "Scores 값이 scores_values_HSA.csv에 저장되었습니다.\n",
      "로딩 값이 loadings_values_sEH.csv에 저장되었습니다.\n",
      "Scores 값이 scores_values_sEH.csv에 저장되었습니다.\n",
      "모든 단백질에 대한 로딩 값과 Scores 값이 각각 loadings_values_sEH.csv와 scores_values_sEH.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# IncrementalPCA와 StandardScaler 초기화\n",
    "n_components = 142\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect()\n",
    "\n",
    "proteins = [\"BRD4\", \"HSA\", \"sEH\"]\n",
    "\n",
    "# 청크 크기 설정\n",
    "chunksize = 100000\n",
    "\n",
    "for protein in proteins:\n",
    "    # 결과 파일 초기화\n",
    "    loadings_output_file = f'loadings_values_{protein}.csv'\n",
    "    scores_output_file = f'scores_values_{protein}.csv'\n",
    "    if os.path.exists(loadings_output_file):\n",
    "        os.remove(loadings_output_file)\n",
    "    if os.path.exists(scores_output_file):\n",
    "        os.remove(scores_output_file)\n",
    "\n",
    "    # 저장된 모델 로드\n",
    "    ipca_model_path = f'ipca_model_{protein}.pkl'\n",
    "    scaler_model_path = f'scaler_model_{protein}.pkl'\n",
    "\n",
    "    ipca = joblib.load(ipca_model_path)\n",
    "    scaler = joblib.load(scaler_model_path)\n",
    "\n",
    "    # PCA 결과 청크 파일들이 있는 폴더 경로\n",
    "    pca_chunks_folder = f'pca_chunks_{protein}'\n",
    "\n",
    "    # 주성분 로딩(loading) 값을 저장\n",
    "    loadings = pd.DataFrame(ipca.components_.T, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    loadings['feature'] = [f'feature_{i}' for i in range(loadings.shape[0])]\n",
    "    loadings = loadings.set_index('feature')\n",
    "    \n",
    "    if os.path.exists(loadings_output_file):\n",
    "        loadings.to_csv(loadings_output_file, mode='a', header=False)\n",
    "    else:\n",
    "        loadings.to_csv(loadings_output_file, mode='w', header=True)\n",
    "\n",
    "    print(f\"로딩 값이 {loadings_output_file}에 저장되었습니다.\")\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        # 해당 단백질의 테스트 데이터 청크 단위로 선택\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM 'test_enc.parquet' \n",
    "        WHERE protein_name = '{protein}'\n",
    "        LIMIT {chunksize} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        protein_test_df = con.execute(query).fetchdf()\n",
    "\n",
    "        if protein_test_df.empty:\n",
    "            break\n",
    "\n",
    "        # 테스트 데이터에 동일한 변환 적용\n",
    "        X_test = protein_test_df.drop(columns=['protein_name', 'id'])\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_test_pcs = ipca.transform(X_test_scaled)\n",
    "\n",
    "        # 테스트 데이터프레임에 PCA 결과 추가\n",
    "        pca_columns = [f'PC{i}' for i in range(n_components)]\n",
    "        test_pca_df = pd.DataFrame(data=X_test_pcs, columns=pca_columns)\n",
    "\n",
    "        # Scores 값을 저장\n",
    "        scores = test_pca_df.copy()\n",
    "        scores['id'] = protein_test_df['id']\n",
    "        \n",
    "        if os.path.exists(scores_output_file):\n",
    "            scores.to_csv(scores_output_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            scores.to_csv(scores_output_file, mode='w', header=True, index=False)\n",
    "\n",
    "        offset += chunksize\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Scores 값이 {scores_output_file}에 저장되었습니다.\")\n",
    "\n",
    "print(f\"모든 단백질에 대한 로딩 값과 Scores 값이 각각 {loadings_output_file}와 {scores_output_file}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PCA chunks for BRD4: 100%|██████████| 47/47 [1:11:03<00:00, 90.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bind 값과 상관관계가 낮은 주성분 (BRD4):\n",
      "['PC4', 'PC6', 'PC53', 'PC72', 'PC67', 'PC13', 'PC64', 'PC54', 'PC65', 'PC140', 'PC88', 'PC43', 'PC80', 'PC108', 'PC113', 'PC66', 'PC96', 'PC45', 'PC5', 'PC107', 'PC78', 'PC109', 'PC112', 'PC62', 'PC79', 'PC131', 'PC122', 'PC124', 'PC138', 'PC121', 'PC134', 'PC128', 'PC85', 'PC127', 'PC99', 'PC102', 'PC92', 'PC117', 'PC75', 'PC120', 'PC136', 'PC129', 'PC135', 'PC132', 'PC110', 'PC83', 'PC130', 'PC139', 'PC73', 'PC93', 'PC126', 'PC82', 'PC116', 'PC118', 'PC103', 'PC104', 'PC89', 'PC111', 'PC137', 'PC94', 'PC61', 'PC133', 'PC98', 'PC105', 'PC123', 'PC63', 'PC90', 'PC91', 'PC34', 'PC77', 'PC86', 'PC125', 'PC50', 'PC95', 'PC101', 'PC3', 'PC30', 'PC41', 'PC24', 'PC81', 'PC23', 'PC38', 'PC115', 'PC71', 'PC12', 'PC70', 'PC9', 'PC16', 'PC44', 'PC74']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PCA chunks for HSA: 100%|██████████| 47/47 [1:12:32<00:00, 92.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bind 값과 상관관계가 낮은 주성분 (HSA):\n",
      "['PC75', 'PC59', 'PC14', 'PC73', 'PC74', 'PC105', 'PC30', 'PC22', 'PC92', 'PC91', 'PC78', 'PC117', 'PC99', 'PC102', 'PC101', 'PC86', 'PC120', 'PC90', 'PC97', 'PC93', 'PC94', 'PC89', 'PC124', 'PC113', 'PC111', 'PC110', 'PC121', 'PC126', 'PC80', 'PC139', 'PC107', 'PC137', 'PC132', 'PC82', 'PC103', 'PC138', 'PC123', 'PC79', 'PC135', 'PC85', 'PC125', 'PC127', 'PC136', 'PC98', 'PC133', 'PC128', 'PC7', 'PC108', 'PC134', 'PC104', 'PC129', 'PC83', 'PC131', 'PC130', 'PC81', 'PC122', 'PC112', 'PC109', 'PC116', 'PC115', 'PC88', 'PC49', 'PC8', 'PC95', 'PC118', 'PC52', 'PC21', 'PC45', 'PC87', 'PC48', 'PC55']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PCA chunks for sEH: 100%|██████████| 47/47 [1:12:49<00:00, 92.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bind 값과 상관관계가 낮은 주성분 (sEH):\n",
      "['PC64', 'PC12', 'PC61', 'PC22', 'PC39', 'PC21', 'PC85', 'PC140', 'PC43', 'PC84', 'PC7', 'PC87', 'PC133', 'PC78', 'PC139', 'PC89', 'PC97', 'PC111', 'PC127', 'PC136', 'PC98', 'PC92', 'PC108', 'PC123', 'PC135', 'PC88', 'PC77', 'PC81', 'PC102', 'PC138', 'PC110', 'PC132', 'PC103', 'PC106', 'PC129', 'PC82', 'PC112', 'PC131', 'PC117', 'PC126', 'PC71', 'PC105', 'PC109', 'PC124', 'PC122', 'PC137', 'PC115', 'PC128', 'PC130', 'PC121', 'PC8', 'PC66', 'PC90', 'PC125', 'PC80', 'PC120', 'PC34', 'PC116', 'PC96', 'PC91', 'PC101', 'PC107', 'PC134', 'PC94', 'PC14', 'PC58', 'PC55', 'PC25', 'PC45', 'PC118', 'PC57', 'PC53', 'PC24', 'PC38']\n",
      "테스트 데이터에서 모든 모델에서 0으로 판정된 데이터가 uncorrelated_test_zero_data.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# IncrementalPCA와 StandardScaler 초기화\n",
    "n_components = 142\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect()\n",
    "\n",
    "proteins = [\"BRD4\", \"HSA\", \"sEH\"]\n",
    "\n",
    "# 청크 크기 설정\n",
    "chunksize = 100000\n",
    "\n",
    "# 결과 파일 초기화\n",
    "output_file = 'uncorrelated_test_zero_data.csv'\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "for protein in proteins:\n",
    "\n",
    "    # correlation 저장 파일 초기화\n",
    "    correlation_file = F'correlation_values_{protein}.csv'\n",
    "    if os.path.exists(correlation_file):\n",
    "        os.remove(correlation_file)\n",
    "    # 저장된 모델 로드\n",
    "    ipca_model_path = f'ipca_model_{protein}.pkl'\n",
    "    scaler_model_path = f'scaler_model_{protein}.pkl'\n",
    "\n",
    "    ipca = joblib.load(ipca_model_path)\n",
    "    scaler = joblib.load(scaler_model_path)\n",
    "\n",
    "    # PCA 결과 청크 파일들이 있는 폴더 경로\n",
    "    pca_chunks_folder = f'pca_chunks_{protein}'\n",
    "\n",
    "    # 상관관계가 낮은 주성분 찾기\n",
    "    correlation = pd.Series(dtype=float)\n",
    "    chunk_files = [f for f in os.listdir(pca_chunks_folder) if f.endswith('.csv')]\n",
    "    for chunk_file in tqdm(chunk_files, desc=f'Processing PCA chunks for {protein}'):\n",
    "        chunk_df = pd.read_csv(os.path.join(pca_chunks_folder, chunk_file))\n",
    "        chunk_corr = chunk_df.corr()['bind'][:-1]\n",
    "        correlation = pd.concat([correlation, chunk_corr])\n",
    "\n",
    "    correlation = correlation.groupby(correlation.index).mean()\n",
    "\n",
    "    # correlation을 파일에 저장\n",
    "    correlation_df = pd.DataFrame({\n",
    "        'protein': [protein] * len(correlation),\n",
    "        'pc': correlation.index,\n",
    "        'correlation': correlation.values\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(correlation_file):\n",
    "        correlation_df.to_csv(correlation_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        correlation_df.to_csv(correlation_file, mode='w', header=True, index=False)\n",
    "\n",
    "    # bind와 상관관계가 낮은 주성분 찾기\n",
    "    threshold = 0.001  # 상관관계 임계값 (절대값 기준)\n",
    "    uncorrelated_pcs = correlation[correlation.abs() < threshold].sort_values().index.tolist()\n",
    "\n",
    "    print(f\"bind 값과 상관관계가 낮은 주성분 ({protein}):\")\n",
    "    print(uncorrelated_pcs)\n",
    "\n",
    "    offset = 0\n",
    "    first_example_found = False\n",
    "    while True:\n",
    "        # 해당 단백질의 테스트 데이터 청크 단위로 선택\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM 'test_enc.parquet' \n",
    "        WHERE protein_name = '{protein}'\n",
    "        LIMIT {chunksize} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        protein_test_df = con.execute(query).fetchdf()\n",
    "\n",
    "        if protein_test_df.empty:\n",
    "            break\n",
    "\n",
    "        # 테스트 데이터에 동일한 변환 적용\n",
    "        X_test = protein_test_df.drop(columns=['protein_name', 'id'])\n",
    "\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        X_test_pcs = ipca.transform(X_test_scaled)\n",
    "\n",
    "        # 테스트 데이터프레임에 PCA 결과 추가\n",
    "        pca_columns = [f'PC{i}' for i in range(n_components)]\n",
    "        test_pca_df = pd.DataFrame(data=X_test_pcs, columns=pca_columns)\n",
    "\n",
    "        # 상관관계가 낮은 주성분을 사용하여 데이터 탐지\n",
    "        uncorrelated_test_data = test_pca_df[uncorrelated_pcs]\n",
    "\n",
    "        # 각 모델에서 상관관계가 낮은 PC 값을 기반으로 0으로 판정된 행을 찾기\n",
    "        zero_rows = uncorrelated_test_data[(uncorrelated_test_data.abs() < threshold).all(axis=1)]\n",
    "\n",
    "        if not zero_rows.empty:\n",
    "            if not first_example_found:\n",
    "                first_example_id = protein_test_df.loc[zero_rows.index[0], 'id']\n",
    "                print(f\"첫 번째 0으로 판정된 예시 ({protein}): id = {first_example_id}\")\n",
    "                first_example_found = True\n",
    "\n",
    "            # 해당 행을 저장\n",
    "            zero_ids = protein_test_df.loc[zero_rows.index, 'id']\n",
    "            zero_df = pd.DataFrame({'id': zero_ids, 'binds': 0})\n",
    "            zero_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "\n",
    "        offset += chunksize\n",
    "        gc.collect()\n",
    "\n",
    "print(f\"테스트 데이터에서 모든 모델에서 0으로 판정된 데이터가 {output_file}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 임계값만 조정해서 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bind 값과 상관관계가 낮은 주성분 (BRD4):\n",
      "['PC92', 'PC117', 'PC75', 'PC120', 'PC136', 'PC129']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and processing training data for BRD4:   0%|          | 0/98415610 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_train_scaled_chunk, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     75\u001b[0m     X_train_scaled_chunk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_train_scaled_chunk, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# Ensure X_train_scaled_chunk is a numpy array\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m X_train_pcs_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_scaled_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloadings_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m train_pca_df_chunk \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mX_train_pcs_chunk, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train_pcs_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect()\n",
    "\n",
    "proteins = [\"BRD4\", \"HSA\", \"sEH\"]\n",
    "\n",
    "# 청크 크기 설정\n",
    "chunksize = 100000\n",
    "\n",
    "# 결과 파일 초기화\n",
    "output_file = 'logistic_regression_predictions.csv'\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "for protein in proteins:\n",
    "    # 저장된 모델 로드\n",
    "    scaler_model_path = f'scaler_model_{protein}.pkl'\n",
    "    scaler = joblib.load(scaler_model_path)\n",
    "\n",
    "    # correlation 파일 불러오기\n",
    "    correlation_file = f'correlation_values_{protein}.csv'\n",
    "    correlation_df = pd.read_csv(correlation_file)\n",
    "    correlation_df = correlation_df.set_index('pc')\n",
    "    correlation = correlation_df['correlation']\n",
    "\n",
    "    # 상관관계가 낮은 주성분 찾기\n",
    "    threshold = 0.00001  # 상관관계 임계값 (절대값 기준)\n",
    "    uncorrelated_pcs = correlation[correlation.abs() < threshold].sort_values().index.tolist()\n",
    "\n",
    "    if not uncorrelated_pcs:\n",
    "        print(f\"{protein}: No uncorrelated principal components found with threshold {threshold}.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"bind 값과 상관관계가 낮은 주성분 ({protein}):\")\n",
    "    print(uncorrelated_pcs)\n",
    "\n",
    "    # Logistic Regression 모델 초기화\n",
    "    logistic_model = LogisticRegression()\n",
    "    loadings_file = f'loadings_values_{protein}.csv'\n",
    "    scores_file = f'scores_values_{protein}.csv'\n",
    "\n",
    "    loadings_df = pd.read_csv(loadings_file)\n",
    "    scores_df = pd.read_csv(scores_file)\n",
    "\n",
    "    # 학습 데이터 로드 및 전처리 (청크 단위)\n",
    "    offset = 0\n",
    "    train_query = f\"SELECT COUNT(*) FROM 'train_enc_{protein}.parquet'\"\n",
    "    total_train_rows = con.execute(train_query).fetchone()[0]\n",
    "\n",
    "    with tqdm(total=total_train_rows, desc=f\"Loading and processing training data for {protein}\") as pbar:\n",
    "        while offset < total_train_rows:\n",
    "            train_query = f\"\"\"\n",
    "            SELECT * FROM 'train_enc_{protein}.parquet'\n",
    "            LIMIT {chunksize} OFFSET {offset}\n",
    "            \"\"\"\n",
    "            train_df = con.execute(train_query).fetchdf()\n",
    "\n",
    "            X_train_chunk = train_df.drop(columns=['bind'])\n",
    "            y_train_chunk = train_df['bind']\n",
    "\n",
    "            # 열 수가 맞지 않을 경우 처리\n",
    "            if X_train_chunk.shape[1] != loadings_df.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in number of features: {X_train_chunk.shape[1]} in X_train_chunk, {loadings_df.shape[0]} in loadings_df\")\n",
    "\n",
    "            X_train_scaled_chunk = scaler.transform(X_train_chunk)\n",
    "            if isinstance(X_train_scaled_chunk, list):\n",
    "                X_train_scaled_chunk = np.array(X_train_scaled_chunk, dtype=float)  # Ensure X_train_scaled_chunk is a numpy array\n",
    "            else:\n",
    "                X_train_scaled_chunk = X_train_scaled_chunk.astype(float)\n",
    "\n",
    "            X_train_pcs_chunk = X_train_scaled_chunk.dot(loadings_df.iloc[:, :-1].values)\n",
    "            train_pca_df_chunk = pd.DataFrame(data=X_train_pcs_chunk, columns=[f'PC{i}' for i in range(X_train_pcs_chunk.shape[1])])\n",
    "\n",
    "            if offset == 0:\n",
    "                X_train_uncorrelated = train_pca_df_chunk[uncorrelated_pcs]\n",
    "                y_train = y_train_chunk\n",
    "            else:\n",
    "                X_train_uncorrelated = pd.concat([X_train_uncorrelated, train_pca_df_chunk[uncorrelated_pcs]], ignore_index=True)\n",
    "                y_train = pd.concat([y_train, y_train_chunk], ignore_index=True)\n",
    "\n",
    "            offset += chunksize\n",
    "            pbar.update(len(train_df))\n",
    "            gc.collect()\n",
    "\n",
    "    # Logistic Regression 모델 학습\n",
    "    logistic_model.fit(X_train_uncorrelated, y_train)\n",
    "    print(f\"{protein} Logistic Regression 모델 학습 완료.\")\n",
    "\n",
    "    # 테스트 데이터 처리 및 예측\n",
    "    offset = 0\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM 'test_enc.parquet' WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "    with tqdm(total=total_rows, desc=f\"Processing {protein}\") as pbar:\n",
    "        while True:\n",
    "            # 해당 단백질의 테스트 데이터 청크 단위로 선택\n",
    "            query = f\"\"\"\n",
    "            SELECT * FROM 'test_enc.parquet' \n",
    "            WHERE protein_name = '{protein}'\n",
    "            LIMIT {chunksize} OFFSET {offset}\n",
    "            \"\"\"\n",
    "            protein_test_df = con.execute(query).fetchdf()\n",
    "\n",
    "            if protein_test_df.empty:\n",
    "                break\n",
    "\n",
    "            # 테스트 데이터에 동일한 변환 적용\n",
    "            X_test = protein_test_df.drop(columns=['protein_name', 'id'])\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            if isinstance(X_test_scaled, list):\n",
    "                X_test_scaled = np.array(X_test_scaled, dtype=float)  # Ensure X_test_scaled is a numpy array\n",
    "            else:\n",
    "                X_test_scaled = X_test_scaled.astype(float)\n",
    "\n",
    "            # 열 수가 맞지 않을 경우 처리\n",
    "            if X_test.shape[1] != loadings_df.shape[0]:\n",
    "                raise ValueError(f\"Mismatch in number of features: {X_test.shape[1]} in X_test, {loadings_df.shape[0]} in loadings_df\")\n",
    "\n",
    "            # 로딩 값을 이용하여 주성분 점수 계산\n",
    "            X_test_pcs = X_test_scaled.dot(loadings_df.iloc[:, :-1].values)\n",
    "            test_pca_df = pd.DataFrame(data=X_test_pcs, columns=[f'PC{i}' for i in range(X_test_pcs.shape[1])])\n",
    "\n",
    "            # 상관관계 낮은 주성분 데이터 선택\n",
    "            X_test_uncorrelated = test_pca_df[uncorrelated_pcs]\n",
    "\n",
    "            # 예측 수행\n",
    "            predictions = logistic_model.predict(X_test_uncorrelated)\n",
    "\n",
    "            # 예측 결과 저장\n",
    "            result_df = pd.DataFrame({\n",
    "                'id': protein_test_df['id'],\n",
    "                'protein_name': protein,\n",
    "                'predicted_bind': predictions\n",
    "            })\n",
    "            result_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "\n",
    "            offset += chunksize\n",
    "            pbar.update(len(protein_test_df))\n",
    "            gc.collect()\n",
    "\n",
    "print(f\"테스트 데이터에 대한 예측 결과가 {output_file}에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train에 GNNs 적용(그래프 네트워크 모델) 취소\n",
    "다른 방향으로 전처리 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_parquet_path = './train.parquet'\n",
    "protein = 'BRD4'\n",
    "\n",
    "# 데이터 쿼리 실행\n",
    "df = con.execute(f\"\"\"\n",
    "(SELECT * FROM parquet_scan('{input_parquet_path}') WHERE binds = 1 ORDER BY RANDOM() LIMIT 10)\n",
    "UNION All\n",
    "(SELECT * FROM parquet_scan('{input_parquet_path}') WHERE binds = 0 ORDER BY RANDOM() LIMIT 10)\n",
    "\"\"\"\n",
    ").fetchdf()\n",
    "\n",
    "# 데이터프레임 셔플\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습&테스트 새 기준 randomforest & catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test predictions saved to test_predictions_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "def smiles_to_features(smiles_list):\n",
    "    features = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            features.extend([Descriptors.MolWt(mol), Descriptors.NumRotatableBonds(mol), Descriptors.TPSA(mol)])\n",
    "        else:\n",
    "            features.extend([0, 0, 0])  # If invalid SMILES, append zeros\n",
    "    return features\n",
    "\n",
    "def get_model(model_type):\n",
    "    if model_type == 'random_forest':\n",
    "        return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif model_type == 'catboost':\n",
    "        return CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=10, random_seed=42, verbose=, task_type='GPU')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "def process_protein(input_parquet_path, protein, model_path, model_type):\n",
    "    # 데이터 쿼리 실행\n",
    "    df = con.execute(f\"\"\"\n",
    "    (SELECT * FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}' AND binds = 1 ORDER BY RANDOM() LIMIT 40000)\n",
    "    UNION ALL\n",
    "    (SELECT * FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}' AND binds = 0 ORDER BY RANDOM() LIMIT 40000)\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    # 데이터프레임 셔플\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # SMILES 및 레이블 데이터를 준비합니다.\n",
    "    smiles_data = df[['molecule_smiles', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles']].values  # 여러 SMILES 문자열 리스트\n",
    "    labels = df['binds']  # 합성 가능성 레이블 (0 또는 1)\n",
    "\n",
    "    # 피처 추출\n",
    "    features = [smiles_to_features(smiles) for smiles in tqdm(smiles_data, desc=f\"Extracting features for {protein}\")]\n",
    "\n",
    "    # 데이터 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 모델 선택 및 학습\n",
    "    model = get_model(model_type)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 저장\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model for {protein} with {model_type} saved to {model_path}\")\n",
    "\n",
    "    # 평가\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Results for {protein} with {model_type}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print()\n",
    "\n",
    "def test_proteins(test_parquet_path, proteins, output_csv_path, model_type):\n",
    "    # 전체 예측 결과를 저장할 리스트\n",
    "    all_results = []\n",
    "\n",
    "    # 모델을 불러오고 예측을 수행\n",
    "    for protein in tqdm(proteins, desc=\"Testing proteins\"):\n",
    "        model_path = f'{model_type}_model_{protein}.pkl'\n",
    "\n",
    "        # 데이터 쿼리 실행\n",
    "        df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{test_parquet_path}') WHERE protein_name = '{protein}'\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # SMILES 데이터를 준비합니다.\n",
    "        smiles_data = df[['molecule_smiles', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles']].values  # 여러 SMILES 문자열 리스트\n",
    "        ids = df['id']  # id 컬럼\n",
    "\n",
    "        # 피처 추출\n",
    "        features = [smiles_to_features(smiles) for smiles in tqdm(smiles_data, desc=f\"Extracting features for {protein} (test)\")]\n",
    "\n",
    "        # 모델 불러오기\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "        # 예측 확률\n",
    "        y_pred_proba = model.predict_proba(features)[:, 1]\n",
    "\n",
    "        # 결과를 데이터프레임으로 구성\n",
    "        results_df = pd.DataFrame({'id': ids, 'binds': y_pred_proba})\n",
    "        all_results.append(results_df)\n",
    "\n",
    "    # 모든 결과를 하나의 데이터프레임으로 합치기\n",
    "    final_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # CSV 파일로 저장\n",
    "    final_results_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"All test predictions saved to {output_csv_path}\")\n",
    "\n",
    "# 데이터베이스 연결 열기\n",
    "con = duckdb.connect()\n",
    "\n",
    "input_parquet_path = './train.parquet'\n",
    "test_parquet_path = './test.parquet'\n",
    "proteins = ['BRD4', 'HSA', 'sEH']\n",
    "output_csv_path = 'test_predictions_combined.csv'\n",
    "model_type = 'catboost'  # 'random_forest' 또는 'catboost' 중 하나\n",
    "\n",
    "# 각 단백질에 대해 모델을 학습하고 저장\n",
    "for protein in tqdm(proteins, desc=\"Processing proteins\"):\n",
    "    model_path = f'{model_type}_model_{protein}.pkl'\n",
    "    process_protein(input_parquet_path, protein, model_path, model_type)\n",
    "\n",
    "# 테스트 파일에 대해 예측하고 결과를 합쳐서 저장\n",
    "test_proteins(test_parquet_path, proteins, output_csv_path, model_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뎁스별 cat boost 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CatBoost with depth: 10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Query interrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m protein \u001b[38;5;129;01min\u001b[39;00m tqdm(proteins, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing proteins\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     55\u001b[0m         model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatboost_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotein\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_depth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdepth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 56\u001b[0m         \u001b[43mprocess_protein_with_depth\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_parquet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 테스트 파일에 대해 예측하고 결과를 합쳐서 저장\u001b[39;00m\n\u001b[0;32m     59\u001b[0m test_proteins(test_parquet_path, proteins, output_csv_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatboost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[51], line 9\u001b[0m, in \u001b[0;36mprocess_protein_with_depth\u001b[1;34m(input_parquet_path, protein, model_path, depth)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_protein_with_depth\u001b[39m(input_parquet_path, protein, model_path, depth):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# 데이터 쿼리 실행\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;43m    (SELECT * FROM parquet_scan(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_parquet_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) WHERE protein_name = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprotein\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m AND binds = 1 ORDER BY RANDOM() LIMIT 40000)\u001b[39;49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;43m    UNION ALL\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;43m    (SELECT * FROM parquet_scan(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_parquet_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m) WHERE protein_name = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprotein\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m AND binds = 0 ORDER BY RANDOM() LIMIT 160000)\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchdf()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m데이터 쿼리 완료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# 데이터프레임 셔플\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Query interrupted"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_model_with_depth(depth, task_type='GPU'):\n",
    "    return CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=depth, random_seed=42, verbose=0, task_type=task_type)\n",
    "def process_protein_with_depth(input_parquet_path, protein, model_path, depth):\n",
    "    # 데이터 쿼리 실행\n",
    "    df = con.execute(f\"\"\"\n",
    "    (SELECT * FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}' AND binds = 1 ORDER BY RANDOM() LIMIT 40000)\n",
    "    UNION ALL\n",
    "    (SELECT * FROM parquet_scan('{input_parquet_path}') WHERE protein_name = '{protein}' AND binds = 0 ORDER BY RANDOM() LIMIT 160000)\n",
    "    \"\"\").fetchdf()\n",
    "    print(\"데이터 쿼리 완료\")\n",
    "    # 데이터프레임 셔플\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # SMILES 및 레이블 데이터를 준비합니다.\n",
    "    smiles_data = df[['molecule_smiles', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles']].values\n",
    "    labels = df['binds']\n",
    "    print(\"smile 데이터 준비 완료\")\n",
    "    # 피처 추출\n",
    "    features = [smiles_to_features(smiles) for smiles in tqdm(smiles_data, desc=f\"Extracting features for {protein}\")]\n",
    "    \n",
    "    # 데이터 분할\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "    print(\"피처 추출 및 데이터 분할 완료\")\n",
    "    # 모델 선택 및 학습\n",
    "    model = get_model_with_depth(depth)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 모델 저장\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model for {protein} with depth {depth} saved to {model_path}\")\n",
    "\n",
    "    # 평가\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Results for {protein} with depth {depth}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# 테스트할 depth 값들\n",
    "depth_values = [10]\n",
    "\n",
    "for depth in depth_values:\n",
    "    print(f\"Testing CatBoost with depth: {depth}\")\n",
    "    for protein in tqdm(proteins, desc=\"Processing proteins\"):\n",
    "        model_path = f'catboost_model_{protein}_depth{depth}.pkl'\n",
    "        process_protein_with_depth(input_parquet_path, protein, model_path, depth)\n",
    "\n",
    "# 테스트 파일에 대해 예측하고 결과를 합쳐서 저장\n",
    "test_proteins(test_parquet_path, proteins, output_csv_path, 'catboost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>protein_name</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>466</td>\n",
       "      <td>C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n",
       "      <td>C#CCOc1ccc(CN)cc1.Cl</td>\n",
       "      <td>Cc1cc2cc(CN)ccc2[nH]1</td>\n",
       "      <td>C#CCOc1ccc(CNc2nc(NCc3ccc4[nH]c(C)cc4c3)nc(N[C...</td>\n",
       "      <td>HSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>467</td>\n",
       "      <td>C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n",
       "      <td>C#CCOc1ccc(CN)cc1.Cl</td>\n",
       "      <td>Cc1cc2cc(CN)ccc2[nH]1</td>\n",
       "      <td>C#CCOc1ccc(CNc2nc(NCc3ccc4[nH]c(C)cc4c3)nc(N[C...</td>\n",
       "      <td>sEH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>683</td>\n",
       "      <td>C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n",
       "      <td>C#CCOc1ccc(CN)cc1.Cl</td>\n",
       "      <td>Cl.NCC12CC3CC(CC(C3)C1)C2</td>\n",
       "      <td>C#CCOc1ccc(CNc2nc(NCC34CC5CC(CC(C5)C3)C4)nc(N[...</td>\n",
       "      <td>sEH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1321</td>\n",
       "      <td>C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n",
       "      <td>C#CCOc1ccc(CN)cc1.Cl</td>\n",
       "      <td>Nc1n[nH]c2ncccc12</td>\n",
       "      <td>C#CCOc1ccc(CNc2nc(Nc3n[nH]c4ncccc34)nc(N[C@@H]...</td>\n",
       "      <td>HSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2141</td>\n",
       "      <td>C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21</td>\n",
       "      <td>C#CCOc1cccc(CN)c1.Cl</td>\n",
       "      <td>Cc1cc2cc(CN)ccc2[nH]1</td>\n",
       "      <td>C#CCOc1cccc(CNc2nc(NCc3ccc4[nH]c(C)cc4c3)nc(N[...</td>\n",
       "      <td>sEH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>5065194</td>\n",
       "      <td>C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>N#Cc1cc(N)ccc1F</td>\n",
       "      <td>NCc1cc(F)cc(F)c1</td>\n",
       "      <td>C#CC[C@H](Nc1nc(NCc2cc(F)cc(F)c2)nc(Nc2ccc(F)c...</td>\n",
       "      <td>BRD4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>5066638</td>\n",
       "      <td>C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>N#Cc1ccc(N)c([N+](=O)[O-])c1</td>\n",
       "      <td>Cc1cc2cc(CN)ccc2[nH]1</td>\n",
       "      <td>C#CC[C@H](Nc1nc(NCc2ccc3[nH]c(C)cc3c2)nc(Nc2cc...</td>\n",
       "      <td>HSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>5066935</td>\n",
       "      <td>C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>N#Cc1ccc(N)c([N+](=O)[O-])c1</td>\n",
       "      <td>Cl.Cl.NCc1cncc(F)c1</td>\n",
       "      <td>C#CC[C@H](Nc1nc(NCc2cncc(F)c2)nc(Nc2ccc(C#N)cc...</td>\n",
       "      <td>HSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5066973</td>\n",
       "      <td>C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>N#Cc1ccc(N)c([N+](=O)[O-])c1</td>\n",
       "      <td>Cl.Cn1cc(N)ccc1=O</td>\n",
       "      <td>C#CC[C@H](Nc1nc(Nc2ccc(=O)n(C)c2)nc(Nc2ccc(C#N...</td>\n",
       "      <td>BRD4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>5067011</td>\n",
       "      <td>C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>N#Cc1ccc(N)c([N+](=O)[O-])c1</td>\n",
       "      <td>Cl.NCC1(O)C2C3CC4C5C3CC2C5C41</td>\n",
       "      <td>C#CC[C@H](Nc1nc(NCC2(O)C3C4CC5C6C4CC3C6C52)nc(...</td>\n",
       "      <td>sEH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                            buildingblock1_smiles  \\\n",
       "0         466  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21   \n",
       "1         467  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21   \n",
       "2         683  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21   \n",
       "3        1321  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21   \n",
       "4        2141  C#CC[C@@H](CC(=O)O)NC(=O)OCC1c2ccccc2-c2ccccc21   \n",
       "...       ...                                              ...   \n",
       "9995  5065194    C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "9996  5066638    C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "9997  5066935    C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "9998  5066973    C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "9999  5067011    C#CC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "\n",
       "             buildingblock2_smiles          buildingblock3_smiles  \\\n",
       "0             C#CCOc1ccc(CN)cc1.Cl          Cc1cc2cc(CN)ccc2[nH]1   \n",
       "1             C#CCOc1ccc(CN)cc1.Cl          Cc1cc2cc(CN)ccc2[nH]1   \n",
       "2             C#CCOc1ccc(CN)cc1.Cl      Cl.NCC12CC3CC(CC(C3)C1)C2   \n",
       "3             C#CCOc1ccc(CN)cc1.Cl              Nc1n[nH]c2ncccc12   \n",
       "4             C#CCOc1cccc(CN)c1.Cl          Cc1cc2cc(CN)ccc2[nH]1   \n",
       "...                            ...                            ...   \n",
       "9995               N#Cc1cc(N)ccc1F               NCc1cc(F)cc(F)c1   \n",
       "9996  N#Cc1ccc(N)c([N+](=O)[O-])c1          Cc1cc2cc(CN)ccc2[nH]1   \n",
       "9997  N#Cc1ccc(N)c([N+](=O)[O-])c1            Cl.Cl.NCc1cncc(F)c1   \n",
       "9998  N#Cc1ccc(N)c([N+](=O)[O-])c1              Cl.Cn1cc(N)ccc1=O   \n",
       "9999  N#Cc1ccc(N)c([N+](=O)[O-])c1  Cl.NCC1(O)C2C3CC4C5C3CC2C5C41   \n",
       "\n",
       "                                        molecule_smiles protein_name  binds  \n",
       "0     C#CCOc1ccc(CNc2nc(NCc3ccc4[nH]c(C)cc4c3)nc(N[C...          HSA      1  \n",
       "1     C#CCOc1ccc(CNc2nc(NCc3ccc4[nH]c(C)cc4c3)nc(N[C...          sEH      1  \n",
       "2     C#CCOc1ccc(CNc2nc(NCC34CC5CC(CC(C5)C3)C4)nc(N[...          sEH      1  \n",
       "3     C#CCOc1ccc(CNc2nc(Nc3n[nH]c4ncccc34)nc(N[C@@H]...          HSA      1  \n",
       "4     C#CCOc1cccc(CNc2nc(NCc3ccc4[nH]c(C)cc4c3)nc(N[...          sEH      1  \n",
       "...                                                 ...          ...    ...  \n",
       "9995  C#CC[C@H](Nc1nc(NCc2cc(F)cc(F)c2)nc(Nc2ccc(F)c...         BRD4      1  \n",
       "9996  C#CC[C@H](Nc1nc(NCc2ccc3[nH]c(C)cc3c2)nc(Nc2cc...          HSA      1  \n",
       "9997  C#CC[C@H](Nc1nc(NCc2cncc(F)c2)nc(Nc2ccc(C#N)cc...          HSA      1  \n",
       "9998  C#CC[C@H](Nc1nc(Nc2ccc(=O)n(C)c2)nc(Nc2ccc(C#N...         BRD4      1  \n",
       "9999  C#CC[C@H](Nc1nc(NCC2(O)C3C4CC5C6C4CC3C6C52)nc(...          sEH      1  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "con = duckdb.connect()\n",
    "train_path = './train.parquet'\n",
    "#train_path = './train_enc_sEH.parquet'\n",
    "#train_path = './train_enc_HSA.parquet'\n",
    "\n",
    "# 데이터 쿼리 실행\n",
    "df = con.execute(f\"\"\"\n",
    "(SELECT * FROM parquet_scan('{train_path}') WHERE binds = 1 LIMIT 10000)\n",
    "\"\"\").fetchdf()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 Test 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자꾸램이터져서 test 코드 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow GPU memory growth enabled\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for BRD4 with elu and relu\n",
      "590/590 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 2s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for HSA with relu and elu\n",
      "560/560 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "625/625 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded LSTM model for sEH with elu and relu\n",
      "567/567 [==============================] - 1s 2ms/step\n",
      "Memory cleared.\n",
      "Test results saved to 'test_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cupy as cp\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# GPU 설정 확인 및 메모리 사용량 조절\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"TensorFlow GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"TensorFlow GPU not available\")\n",
    "\n",
    "# 데이터베이스 연결\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# 테스트 데이터 경로 설정\n",
    "test_parquet_path = 'test_processed_data.parquet'\n",
    "\n",
    "# 각 protein에 대한 최적의 조합을 설정\n",
    "best_combinations = {\n",
    "    'BRD4': [('elu', 'relu')],\n",
    "    'HSA': [('relu', 'elu')],\n",
    "    'sEH': [('elu', 'relu')]\n",
    "}\n",
    "\n",
    "# 사용할 모델 종류 지정\n",
    "model_types = ['lstm']  # 여기서 원하는 모델 종류를 리스트로 선택하세요\n",
    "# model_types = ['xgboost']\n",
    "# model_types = ['randomforest']\n",
    "\n",
    "# 결과 저장을 위한 리스트\n",
    "results = []\n",
    "\n",
    "batch_size = 20000  # 적절한 배치 크기로 설정\n",
    "\n",
    "# 메모리 정리 함수\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "    print(\"Memory cleared.\")\n",
    "\n",
    "for protein in best_combinations.keys():\n",
    "    # 총 행 수 계산\n",
    "    total_rows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{test_parquet_path}') WHERE protein_name = '{protein}'\").fetchone()[0]\n",
    "\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = min(start + batch_size, total_rows)\n",
    "        batch_df = con.execute(f\"\"\"\n",
    "            SELECT * FROM read_parquet('{test_parquet_path}')\n",
    "            WHERE protein_name = '{protein}'\n",
    "            LIMIT {batch_size} OFFSET {start}\n",
    "        \"\"\").df()\n",
    "\n",
    "        # 테스트 데이터 전처리\n",
    "        X_test = np.concatenate([\n",
    "            np.array(batch_df['molecule_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock1_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock2_ecfp'].tolist(), dtype=np.float32),\n",
    "            np.array(batch_df['buildingblock3_ecfp'].tolist(), dtype=np.float32)\n",
    "        ], axis=1)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            for activation_1, activation_2 in best_combinations[protein]:\n",
    "                if model_type == 'xgboost':\n",
    "                    model = XGBClassifier()\n",
    "                    model_filename = f\"{protein}_xgb_model_iteration_1.pkl\"\n",
    "                elif model_type == 'randomforest':\n",
    "                    model = RandomForestClassifier()\n",
    "                    model_filename = f\"{protein}_rf_model_iteration_1.pkl\"\n",
    "                elif model_type == 'lstm':\n",
    "                    input_dim = 1024 * 4  # ECFP 길이에 맞게 조정\n",
    "                    hidden_dim = 128\n",
    "                    output_dim = 1\n",
    "\n",
    "                    model = Sequential([\n",
    "                        Input(shape=(None, input_dim)),\n",
    "                        LSTM(hidden_dim * 8, return_sequences=True, activation=activation_1, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 4, return_sequences=True, activation=activation_2, dropout=0.05),\n",
    "                        LSTM(hidden_dim * 1, return_sequences=False, activation=activation_1, dropout=0.05),\n",
    "                        Dense(output_dim, activation='sigmoid')\n",
    "                    ])\n",
    "                    model_filename = f\"{protein}_lstm_model_{activation_1}_{activation_2}_{activation_1}_iteration_1.weights.h5\"\n",
    "\n",
    "                # 모델 로드\n",
    "                if model_type == 'lstm':\n",
    "                    model.load_weights(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein} with {activation_1} and {activation_2}\")\n",
    "                else:\n",
    "                    model = joblib.load(model_filename)\n",
    "                    print(f\"Loaded {model_type.upper()} model for {protein}\")\n",
    "\n",
    "                # 예측 수행\n",
    "                if model_type == 'lstm':\n",
    "                    X_test_lstm = np.expand_dims(X_test, axis=1)  # LSTM이 기대하는 3D 입력으로 변환\n",
    "                    y_pred = model.predict(X_test_lstm)\n",
    "                elif model_type == 'randomforest':\n",
    "                    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "                elif model_type == 'xgboost':\n",
    "                    X_test_gpu = cp.array(X_test)\n",
    "                    y_pred = model.predict_proba(X_test_gpu)[:, 1]\n",
    "                    y_pred = cp.asnumpy(y_pred)\n",
    "\n",
    "                # 결과 저장\n",
    "                test_results = pd.DataFrame({\n",
    "                    'id': batch_df['id'],\n",
    "                    'binds': y_pred.flatten()\n",
    "                })\n",
    "                results.append(test_results)\n",
    "\n",
    "                # 메모리 정리\n",
    "                clear_memory()\n",
    "\n",
    "# 모든 결과를 하나의 DataFrame으로 결합\n",
    "final_results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 결과 저장\n",
    "final_results_df.to_csv('test_results.csv', index=False)\n",
    "print(\"Test results saved to 'test_results.csv'\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNNs 테스트 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추출 병합 ecfp적용에서\n",
    "추출 ecfp 정용 병합으로 방향을 전환(속도 개선)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bind = 0을 랜덤 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def query_and_save_chunks(train_path, output_prefix, total_records=1589906, chunk_size=100000):\n",
    "    # DuckDB 연결\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    remaining_records = total_records\n",
    "    offset = 0\n",
    "    chunk_id = 13\n",
    "    \n",
    "    while remaining_records > 0:\n",
    "        fetch_size = min(chunk_size, remaining_records)\n",
    "        \n",
    "        # 데이터 쿼리\n",
    "        df = con.execute(f\"\"\"\n",
    "        SELECT * FROM parquet_scan('{train_path}') WHERE binds = 0 ORDER BY RANDOM() LIMIT {fetch_size} OFFSET {offset}\n",
    "        \"\"\").fetchdf()\n",
    "        \n",
    "        if df.empty:\n",
    "            break\n",
    "        \n",
    "        # 청크 파일로 저장\n",
    "        chunk_path = f\"{output_prefix}_chunk_{chunk_id}.parquet\"\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, chunk_path)\n",
    "        print(f\"Data saved to {chunk_path}, fetched: {fetch_size}, remaining: {remaining_records}\")\n",
    "        \n",
    "        # 메모리 해제\n",
    "        del df, table\n",
    "        gc.collect()\n",
    "        \n",
    "        offset += fetch_size\n",
    "        remaining_records -= fetch_size\n",
    "        chunk_id += 1\n",
    "\n",
    "# 파일 경로 및 설정\n",
    "train_path = './train.parquet'\n",
    "output_prefix = './0_queried_data'\n",
    "final_output_path = './final_queried_data.parquet'\n",
    "total_records = 1589906\n",
    "chunk_size = 100000\n",
    "\n",
    "# 데이터 쿼리 및 청크 저장 실행\n",
    "query_and_save_chunks(train_path, output_prefix, total_records, chunk_size)\n",
    "\n",
    "# 청크 파일 병합 실행\n",
    "# total_chunks = (total_records + chunk_size - 1) // chunk_size\n",
    "# merge_chunks(output_prefix, final_output_path, total_chunks)\n",
    "\n",
    "# print(\"Data querying and merging completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추출된 파일들을 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001AAA383B490>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 processed and merged.\n",
      "Chunk 1 processed and merged.\n",
      "Chunk 2 processed and merged.\n",
      "Chunk 3 processed and merged.\n",
      "Chunk 4 processed and merged.\n",
      "Chunk 5 processed and merged.\n",
      "Chunk 6 processed and merged.\n",
      "Chunk 7 processed and merged.\n",
      "Chunk 8 processed and merged.\n",
      "Chunk 9 processed and merged.\n",
      "Chunk 10 processed and merged.\n",
      "Chunk 11 processed and merged.\n",
      "Chunk 12 processed and merged.\n",
      "Chunk 13 processed and merged.\n",
      "Chunk 14 processed and merged.\n",
      "Chunk 15 processed and merged.\n",
      "Final data saved to ./final_queried_data3.parquet\n",
      "Data querying and merging completed.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def merge_chunks(output_prefix, final_output_path, total_chunks):\n",
    "    schema = None\n",
    "    unique_data = pd.DataFrame()\n",
    "    \n",
    "    for chunk_id in range(total_chunks):\n",
    "        chunk_path = f\"{output_prefix}_chunk_{chunk_id}.parquet\"\n",
    "        table = pq.read_table(chunk_path)\n",
    "        df = table.to_pandas()\n",
    "        \n",
    "        # 중복 제거 및 데이터 합치기\n",
    "        unique_data = pd.concat([unique_data, df]).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        # 메모리 해제 및 청크 파일 삭제\n",
    "        del table, df\n",
    "        os.remove(chunk_path)\n",
    "        gc.collect()\n",
    "        print(f\"Chunk {chunk_id} processed and merged.\")\n",
    "    \n",
    "    # 최종 파일로 저장\n",
    "    final_table = pa.Table.from_pandas(unique_data)\n",
    "    pq.write_table(final_table, final_output_path)\n",
    "    print(f\"Final data saved to {final_output_path}\")\n",
    "    \n",
    "    # 메모리 해제\n",
    "    del unique_data, final_table\n",
    "    gc.collect()\n",
    "\n",
    "# 파일 경로 및 설정\n",
    "train_path = './train.parquet'\n",
    "output_prefix = './0_queried_data'\n",
    "final_output_path = './final_queried_data3.parquet'\n",
    "total_records = 1600000#1589906\n",
    "chunk_size = 100000\n",
    "\n",
    "# 청크 파일 병합 실행\n",
    "total_chunks = (total_records + chunk_size - 1) // chunk_size\n",
    "merge_chunks(output_prefix, final_output_path, total_chunks)\n",
    "\n",
    "print(\"Data querying and merging completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추출/병합된 파일을 ecfp로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 49batch [45:51, 56.15s/batch]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECFP generation and saving completed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ECFP 생성 함수\n",
    "def generate_ecfp(smiles, radius=2, bits=1024):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    if molecule is None:\n",
    "        return [0] * bits\n",
    "    return list(AllChem.GetMorganFingerprintAsBitVect(molecule, radius, nBits=bits))\n",
    "\n",
    "def process_batch(df):\n",
    "    # 각 SMILES 열에 대해 ECFP 생성\n",
    "    df['molecule_ecfp'] = df['molecule_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock1_ecfp'] = df['buildingblock1_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock2_ecfp'] = df['buildingblock2_smiles'].apply(generate_ecfp)\n",
    "    df['buildingblock3_ecfp'] = df['buildingblock3_smiles'].apply(generate_ecfp)\n",
    "    \n",
    "    # 필요한 열만 포함된 DataFrame 반환\n",
    "    return df[['id', 'protein_name', 'molecule_ecfp', 'buildingblock1_ecfp', 'buildingblock2_ecfp', 'buildingblock3_ecfp', 'binds']]\n",
    "\n",
    "def preprocess_and_save_ecfp(input_path, output_path, batch_size=32768):\n",
    "    reader = pq.ParquetFile(input_path)\n",
    "    \n",
    "    # 적절한 스키마로 Parquet writer 초기화\n",
    "    schema = pa.schema([\n",
    "        ('id', pa.int32()),\n",
    "        ('molecule_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock1_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock2_ecfp', pa.list_(pa.int32())),\n",
    "        ('buildingblock3_ecfp', pa.list_(pa.int32())),\n",
    "        ('protein_name', pa.string()),\n",
    "        ('binds', pa.int32())  # test할 때는 제외\n",
    "    ])\n",
    "    \n",
    "    with pq.ParquetWriter(output_path, schema) as writer:\n",
    "        total_batches = reader.metadata.num_row_groups\n",
    "        \n",
    "        with tqdm(total=total_batches, desc=\"Processing\", unit=\"batch\", leave=True) as pbar:\n",
    "            for batch in reader.iter_batches(batch_size=batch_size):\n",
    "                df_batch = batch.to_pandas()\n",
    "                \n",
    "                processed_batch = process_batch(df_batch)\n",
    "                \n",
    "                # 처리된 DataFrame을 Arrow Table로 변환하여 파일에 작성\n",
    "                table = pa.Table.from_pandas(processed_batch, schema=schema)\n",
    "                writer.write_table(table)\n",
    "                \n",
    "                # 진행 상황 업데이트\n",
    "                pbar.update(1)\n",
    "\n",
    "                # 주기적으로 가비지 컬렉션 호출\n",
    "                gc.collect()\n",
    "\n",
    "# 파일 경로 설정\n",
    "#input_parquet_path = './merged_shuffled_data.parquet'\n",
    "input_parquet_path = './final_queried_data3.parquet'\n",
    "output_parquet_path = './ecfp_queried_data.parquet'\n",
    "# 테스트 파일 경로\n",
    "# input_parquet_path = './test.parquet'\n",
    "# output_parquet_path = './test_processed_data.parquet'\n",
    "\n",
    "# 데이터 전처리 및 저장 실행\n",
    "preprocess_and_save_ecfp(input_parquet_path, output_parquet_path)\n",
    "\n",
    "print(\"ECFP generation and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "원본 파일과 병합 후 순서를 다 섞기 -> 램이 부족한지 작동하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow as pa\n",
    "# import pandas as pd\n",
    "# import gc\n",
    "\n",
    "# def merge_and_shuffle_parquets(input_path1, input_path2, output_path):\n",
    "#     # 첫 번째 Parquet 파일 읽기\n",
    "#     table1 = pq.read_table(input_path1)\n",
    "#     df1 = table1.to_pandas()\n",
    "    \n",
    "#     # 두 번째 Parquet 파일 읽기\n",
    "#     table2 = pq.read_table(input_path2)\n",
    "#     df2 = table2.to_pandas()\n",
    "    \n",
    "#     # 데이터 프레임 병합\n",
    "#     combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "#     # 데이터 프레임 셔플\n",
    "#     shuffled_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "#     # 결과를 Parquet 파일로 저장\n",
    "#     shuffled_table = pa.Table.from_pandas(shuffled_df)\n",
    "#     pq.write_table(shuffled_table, output_path)\n",
    "    \n",
    "#     print(f\"Data from {input_path1} and {input_path2} merged and shuffled, saved to {output_path}\")\n",
    "    \n",
    "#     # 메모리 해제\n",
    "#     del df1, df2, combined_df, shuffled_df, table1, table2, shuffled_table\n",
    "#     gc.collect()\n",
    "\n",
    "# # 파일 경로 설정\n",
    "# input_path1 = 'processed_merged_queried_data.parquet'\n",
    "# input_path2 = './ecfp_queried_data.parquet'#'./queried_data.parquet'\n",
    "# output_path = 'processed_merged_queried_data.parquet'\n",
    "\n",
    "# # 데이터 병합 및 셔플 실행\n",
    "# merge_and_shuffle_parquets(input_path1, input_path2, output_path)\n",
    "\n",
    "# print(\"Data merging and shuffling completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000026776EFB490>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import gc  # Garbage Collection 모듈 임포트\n",
    "\n",
    "def external_sort_and_merge_with_shuffle(input_path1, input_path2, output_path, batch_size=200000):\n",
    "    # 파일 읽기\n",
    "    reader1 = pq.ParquetFile(input_path1)\n",
    "    reader2 = pq.ParquetFile(input_path2)\n",
    "    \n",
    "    # 최소 힙 초기화\n",
    "    min_heap = []\n",
    "    schema = reader1.schema.to_arrow_schema()\n",
    "    \n",
    "    # 파일에서 배치 읽기 및 힙에 삽입\n",
    "    def process_batches(reader):\n",
    "        for batch in reader.iter_batches(batch_size=batch_size):\n",
    "            dict_data = batch.to_pydict()\n",
    "            # 각 열 리스트를 행으로 조합하여 힙에 삽입\n",
    "            for row in zip(*dict_data.values()):\n",
    "                heapq.heappush(min_heap, row)\n",
    "            gc.collect()  # 배치 처리 후 메모리 정리\n",
    "\n",
    "    process_batches(reader1)\n",
    "    process_batches(reader2)\n",
    "    \n",
    "    # 모든 데이터를 리스트로 변환\n",
    "    all_data = []\n",
    "    while min_heap:\n",
    "        all_data.append(heapq.heappop(min_heap))\n",
    "\n",
    "    # 데이터 셔플\n",
    "    np.random.shuffle(all_data)\n",
    "    \n",
    "    # 셔플된 데이터를 새 파일에 저장\n",
    "    with pq.ParquetWriter(output_path, schema) as writer:\n",
    "        for data in all_data:\n",
    "            # 행 데이터를 Arrow Table로 변환\n",
    "            table = pa.Table.from_pydict(dict(zip(schema.names, [[value] for value in data])), schema=schema)\n",
    "            writer.write_table(table)\n",
    "            gc.collect()  # 데이터 쓰기 후 메모리 정리\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_path1 = 'processed_merged_queried_data.parquet'\n",
    "input_path2 = './ecfp_queried_data.parquet'#'./queried_data.parquet'\n",
    "output_path = 'processed_merged_queried_data1.parquet'\n",
    "\n",
    "# 외부 정렬 실행\n",
    "external_sort_and_merge_with_shuffle(input_path1, input_path2, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 섞였는지 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "\n",
    "df = con.query(f\"\"\"\n",
    "SELECT * FROM parquet_scan('./processed_merged_queried_data.parquet') LIMIT 1000       \n",
    "\"\"\").df()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 컬럼의 데이터가 동일한 중복 항을 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 동일한 모든 컬럼 데이터를 가진 행을 제거하는 쿼리\n",
    "query = \"\"\"\n",
    "WITH UniqueRows AS (\n",
    "    SELECT *,\n",
    "    ROW_NUMBER() OVER (PARTITION BY * ORDER BY *) as rn\n",
    "    FROM parquet_scan('./processed_merged_queried_data.parquet')\n",
    ")\n",
    "SELECT * FROM UniqueRows\n",
    "WHERE rn = 1\n",
    "\"\"\"\n",
    "\n",
    "# 제거된 데이터를 DataFrame으로 로드\n",
    "df = con.execute(query).df()\n",
    "\n",
    "# 결과 DataFrame을 파켓 파일로 저장\n",
    "output_path = './processed_merged_queried_data.parquet'\n",
    "df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(\"Cleaned data has been saved to:\", output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇대 몇인지비율 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 전체 데이터 중에서 binds=1 및 binds=0의 갯수를 계산\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN binds = 1 THEN 1 ELSE 0 END) AS binds_1_count,\n",
    "    SUM(CASE WHEN binds = 0 THEN 1 ELSE 0 END) AS binds_0_count,\n",
    "    COUNT(*) AS total_count\n",
    "FROM parquet_scan('./processed_merged_queried_data.parquet')\n",
    "\"\"\"\n",
    "\n",
    "df = con.execute(query).df()\n",
    "\n",
    "# 전체 행의 수를 사용하여 비율 계산\n",
    "df['ratio_binds_1'] = df['binds_1_count'] / df['total_count']\n",
    "df['ratio_binds_0'] = df['binds_0_count'] / df['total_count']\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score 점수를 기준으로 재구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-8.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install natsort\n",
    "score_df = score_df[['filename', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['submission (0).csv', 'submission (1).csv', 'submission (2).csv', 'submission (3).csv', 'submission (4).csv', 'submission (5).csv', 'submission (6).csv', 'submission (7).csv', 'submission (8).csv', 'submission (9).csv', 'submission (10).csv', 'submission (11).csv', 'submission (12).csv', 'submission (13).csv', 'submission (14).csv', 'submission (15).csv', 'submission (16).csv', 'submission (17).csv', 'submission (18).csv', 'submission (19).csv', 'submission (20).csv', 'submission (21).csv', 'submission (22).csv', 'submission (23).csv', 'submission (24).csv', 'submission (25).csv', 'submission (26).csv', 'submission (27).csv', 'submission (28).csv', 'submission (29).csv', 'submission (30).csv', 'submission (31).csv', 'submission (32).csv', 'submission (33).csv', 'submission (34).csv', 'submission (35).csv', 'submission (36).csv', 'submission (37).csv', 'submission (38).csv', 'submission (39).csv', 'submission (40).csv', 'submission (41).csv', 'submission (42).csv', 'submission (43).csv', 'submission (44).csv', 'submission (45).csv', 'submission (46).csv', 'submission (47).csv', 'submission (48).csv', 'submission (49).csv', 'submission (50).csv', 'submission (51).csv', 'submission (52).csv', 'submission (53).csv', 'submission (54).csv']\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing submission files:   2%|▏         | 1/55 [01:08<1:02:02, 68.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m             y_temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(X_temp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], score)  \u001b[38;5;66;03m# score 값을 반복하여 배열 생성\u001b[39;00m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 최적의 binds 값 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # tqdm 라이브러리 추가\n",
    "from natsort import natsorted  # natsort 라이브러리 추가\n",
    "\n",
    "# 55개의 제출 파일이 있는 디렉토리\n",
    "directory = './submissions'\n",
    "\n",
    "# score 데이터를 읽어오기\n",
    "score_df = pd.read_csv('score.csv')  # score.csv 파일 경로 수정 필요\n",
    "score_dict = dict(zip(score_df['filename'], score_df['score']))  # 파일명과 score를 딕셔너리로 변환\n",
    "\n",
    "# 랜덤 포레스트 모델 초기화\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 데이터를 배치로 처리하여 모델 학습\n",
    "batch_size = 250000  # 한 번에 처리할 데이터의 크기\n",
    "submission_files = natsorted(os.listdir(directory))  # 자연스럽게 파일명을 정렬\n",
    "\n",
    "print(submission_files)\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "for filename in tqdm(submission_files, desc=\"Processing submission files\"):\n",
    "    if filename.endswith(\".csv\"):  # 파일 형식에 맞게 수정 필요\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        score = score_dict[filename]  # 현재 파일의 score 값 가져오기\n",
    "        \n",
    "        # 파일을 배치로 나누어 처리\n",
    "        for chunk in pd.read_csv(file_path, chunksize=batch_size):\n",
    "            X_temp = chunk[['binds']]\n",
    "            y_temp = np.full(X_temp.shape[0], score)  # score 값을 반복하여 배열 생성\n",
    "            \n",
    "            # 모델 학습\n",
    "            model.fit(X_temp, y_temp)\n",
    "\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# 최적의 binds 값 예측\n",
    "print(\"Predicting optimal binds value...\")\n",
    "X_new = pd.DataFrame({'binds': np.linspace(0, 1, 1000)})  # 0에서 1 사이의 1000개의 값을 가진 데이터프레임 생성\n",
    "y_new_pred = model.predict(X_new)  # 모델을 사용하여 각 binds 값에 대한 예측 수행\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.plot(X_new['binds'], y_new_pred, color='red')\n",
    "plt.xlabel('Binds')\n",
    "plt.ylabel('Predicted Score')\n",
    "plt.title('Predicted Score vs Binds')\n",
    "plt.show()\n",
    "\n",
    "# 최적의 binds 값\n",
    "optimal_binds = X_new.iloc[np.argmax(y_new_pred)]['binds']\n",
    "print(f'Optimal binds value: {optimal_binds}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['submission (0).csv', 'submission (1).csv', 'submission (2).csv', 'submission (3).csv', 'submission (4).csv', 'submission (5).csv', 'submission (6).csv', 'submission (7).csv', 'submission (8).csv', 'submission (9).csv', 'submission (10).csv', 'submission (11).csv', 'submission (12).csv', 'submission (13).csv', 'submission (14).csv', 'submission (15).csv', 'submission (16).csv', 'submission (17).csv', 'submission (18).csv', 'submission (19).csv', 'submission (20).csv', 'submission (21).csv', 'submission (22).csv', 'submission (23).csv', 'submission (24).csv', 'submission (25).csv', 'submission (26).csv', 'submission (27).csv', 'submission (28).csv', 'submission (29).csv', 'submission (30).csv', 'submission (31).csv', 'submission (32).csv', 'submission (33).csv', 'submission (34).csv', 'submission (35).csv', 'submission (36).csv', 'submission (37).csv', 'submission (38).csv', 'submission (39).csv', 'submission (40).csv', 'submission (41).csv', 'submission (42).csv', 'submission (43).csv', 'submission (44).csv', 'submission (45).csv', 'submission (46).csv', 'submission (47).csv', 'submission (48).csv', 'submission (49).csv', 'submission (50).csv', 'submission (51).csv', 'submission (52).csv', 'submission (53).csv', 'submission (54).csv']\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing submission files:   2%|▏         | 1/55 [01:46<1:35:53, 106.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m         y_temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(X_temp\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], score)  \u001b[38;5;66;03m# score 값을 반복하여 배열 생성\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 최적의 binds 값 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gksru\\anaconda3\\envs\\belka39\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # tqdm 라이브러리 추가\n",
    "from natsort import natsorted  # natsort 라이브러리 추가\n",
    "\n",
    "# 55개의 제출 파일이 있는 디렉토리\n",
    "directory = './submissions'\n",
    "\n",
    "# score 데이터를 읽어오기\n",
    "score_df = pd.read_csv('score.csv')  # score.csv 파일 경로 수정 필요\n",
    "score_dict = dict(zip(score_df['filename'], score_df['score']))  # 파일명과 score를 딕셔너리로 변환\n",
    "\n",
    "# 랜덤 포레스트 모델 초기화\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 데이터를 처리하여 모델 학습\n",
    "submission_files = natsorted(os.listdir(directory))  # 자연스럽게 파일명을 정렬\n",
    "\n",
    "print(submission_files)\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "for filename in tqdm(submission_files, desc=\"Processing submission files\"):\n",
    "    if filename.endswith(\".csv\"):  # 파일 형식에 맞게 수정 필요\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        score = score_dict[filename]  # 현재 파일의 score 값 가져오기\n",
    "        \n",
    "        # 파일 전체를 읽어서 처리\n",
    "        df = pd.read_csv(file_path)\n",
    "        X_temp = df[['binds']]\n",
    "        y_temp = np.full(X_temp.shape[0], score)  # score 값을 반복하여 배열 생성\n",
    "        \n",
    "        # 모델 학습\n",
    "        model.fit(X_temp, y_temp)\n",
    "\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# 최적의 binds 값 예측\n",
    "print(\"Predicting optimal binds value...\")\n",
    "X_new = pd.DataFrame({'binds': np.linspace(0, 1, 1000)})  # 0에서 1 사이의 1000개의 값을 가진 데이터프레임 생성\n",
    "y_new_pred = model.predict(X_new)  # 모델을 사용하여 각 binds 값에 대한 예측 수행\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.plot(X_new['binds'], y_new_pred, color='red')\n",
    "plt.xlabel('Binds')\n",
    "plt.ylabel('Predicted Score')\n",
    "plt.title('Predicted Score vs Binds')\n",
    "plt.show()\n",
    "\n",
    "# 최적의 binds 값\n",
    "optimal_binds = X_new.iloc[np.argmax(y_new_pred)]['binds']\n",
    "print(f'Optimal binds value: {optimal_binds}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "submission 합치기 -> 최적 검토"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting row-wise prediction...\n",
      "Row-wise prediction completed.\n",
      "          id  optimal_binds\n",
      "0  295246830       0.109665\n",
      "1  295246831       0.121072\n",
      "2  295246832       0.122045\n",
      "3  295246833       0.135941\n",
      "4  295246834       0.164062\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# combined_submissions.csv 파일을 읽어오기\n",
    "combined_df = pd.read_csv('combined_submissions.csv')\n",
    "\n",
    "# score 데이터를 읽어오기\n",
    "score_df = pd.read_csv('score.csv')  # score.csv 파일 경로 수정 필요\n",
    "score_dict = dict(zip(score_df['filename'], score_df['score']))  # 파일명과 score를 딕셔너리로 변환\n",
    "\n",
    "# 컬럼명 생성 (binds_0, binds_1, ..., binds_54)\n",
    "binds_columns = [col for col in combined_df.columns if col.startswith('binds_')]\n",
    "\n",
    "# 각 row의 최적의 binds 값을 예측하기 위한 데이터프레임 생성\n",
    "optimal_binds = []\n",
    "\n",
    "print(\"Starting row-wise prediction...\")\n",
    "\n",
    "for idx, row in combined_df.iterrows():\n",
    "    row_data = []\n",
    "    for col in binds_columns:\n",
    "        score = score_dict[col.replace('binds_', 'submission (') + ').csv']\n",
    "        row_data.append((row[col], score))\n",
    "    \n",
    "    # 각 row의 binds 값과 score를 기반으로 최적의 binds 값을 예측\n",
    "    optimal_binds_value = sum(bind_value * score for bind_value, score in row_data) / sum(score for _, score in row_data)\n",
    "    optimal_binds.append(optimal_binds_value)\n",
    "\n",
    "# 예측된 최적의 binds 값을 combined_df에 추가\n",
    "combined_df['optimal_binds'] = optimal_binds\n",
    "\n",
    "print(\"Row-wise prediction completed.\")\n",
    "print(combined_df[['id', 'optimal_binds']].head())  # 예측 결과의 상위 5개 행 출력\n",
    "\n",
    "save_df = combined_df[['id', 'optimal_binds']]\n",
    "# 예측된 결과를 저장 (선택 사항)\n",
    "combined_df.to_csv('predicted_optimal_binds.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>optimal_binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>0.109665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>0.121072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>0.122045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>0.135941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>0.164062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  optimal_binds\n",
       "0  295246830       0.109665\n",
       "1  295246831       0.121072\n",
       "2  295246832       0.122045\n",
       "3  295246833       0.135941\n",
       "4  295246834       0.164062"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = combined_df[['id', 'optimal_binds']]\n",
    "# 예측된 결과를 저장 (선택 사항)\n",
    "save_df.to_csv('predicted_optimal_binds3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "베이지안 방법론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "\n",
    "# combined_submissions.csv 파일을 읽어오기\n",
    "combined_df = pd.read_csv('combined_submissions.csv')\n",
    "\n",
    "# score 데이터를 읽어오기\n",
    "score_df = pd.read_csv('score.csv')  # score.csv 파일 경로 수정 필요\n",
    "score_dict = dict(zip(score_df['filename'], score_df['score']))  # 파일명과 score를 딕셔너리로 변환\n",
    "\n",
    "# 컬럼명 생성 (binds_0, binds_1, ..., binds_54)\n",
    "binds_columns = [col for col in combined_df.columns if col.startswith('binds_')]\n",
    "\n",
    "# 각 row의 최적의 binds 값을 예측하기 위한 데이터프레임 생성\n",
    "optimal_binds = []\n",
    "\n",
    "print(\"Starting row-wise prediction with Bayesian Optimization...\")\n",
    "\n",
    "for idx, row in tqdm(combined_df.iterrows(), total=combined_df.shape[0]):\n",
    "    def objective_function(**kwargs):\n",
    "        \"\"\"\n",
    "        kwargs는 binds_0, binds_1, ..., binds_54에 해당하는 값으로 이루어져 있습니다.\n",
    "        이를 사용하여 가중 평균을 계산하고 그 값을 반환합니다.\n",
    "        \"\"\"\n",
    "        return np.sum([row[col] * score_dict[col.replace('binds_', 'submission (') + ').csv'] for col in kwargs.keys()])\n",
    "    \n",
    "    # 베이지안 최적화를 위한 파라미터 범위 설정\n",
    "    pbounds = {col: (0, 1) for col in binds_columns}\n",
    "    \n",
    "    # 베이지안 최적화 수행\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function,\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    # 초기 포인트와 최대 탐색 포인트 설정\n",
    "    optimizer.maximize(\n",
    "        init_points=5,\n",
    "        n_iter=10,\n",
    "    )\n",
    "    \n",
    "    # 최적의 binds 값 추출\n",
    "    optimal_binds_value = {k: v for k, v in optimizer.max['params'].items()}\n",
    "    \n",
    "    # 최적의 binds 값을 평균하여 최종 값을 도출\n",
    "    optimal_binds.append(np.mean(list(optimal_binds_value.values())))\n",
    "\n",
    "# 예측된 최적의 binds 값을 combined_df에 추가\n",
    "combined_df['optimal_binds'] = optimal_binds\n",
    "\n",
    "print(\"Row-wise prediction completed.\")\n",
    "print(combined_df[['id', 'optimal_binds']].head())  # 예측 결과의 상위 5개 행 출력\n",
    "\n",
    "# 예측된 결과를 저장 (선택 사항)\n",
    "combined_df.to_csv('predicted_optimal_binds2.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
